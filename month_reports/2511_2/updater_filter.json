{
    "foundation-models": [
        {
            "year": "2025.11",
            "title": "Explainable AI with fine-tuned large language models for sustainable cultural heritage management",
            "team": "Huipin Lu",
            "team website": "",
            "affiliation": "",
            "domain": "Sentiment Analysis for Cultural Heritage",
            "abstract": "The redevelopment of cultural heritage areas, especially in historical urban environments, requires a nuanced understanding of public perceptions to balance preservation with modernization. This study introduces an advanced AI-driven framework for Aspect Sentiment Quadruple Prediction (ASQP) to assess public perceptions of Lijiang Ancient Town, a UNESCO World Heritage site in China. We fine-tuned the large language model Qwen-14B using LoRA-based methods to augment sentiment data, effectively uncovering implicit emotional cues in social media content. The model integrates BERT, multi-layer BiLSTM, self-attention, CNN, and CRF for enhanced entity recognition and sentiment classification. Experimental results show that the enhanced model (Qwen-14B\u2009+\u2009ASQP) improved F1-score by 0.97% (from 75.42% to 76.39%) and Precision by 4.48% (from 76.14% to 80.62%) compared to the baseline. Analyzing data from platforms such as Weibo, Dazhong Dianping, and Xiaohongshu (2018\u20132024), the research uncovers factors influencing public perception, offering insights for heritage site management, urban planning, and the sustainable preservation of cultural heritage.",
            "venue": "Scientific Reports",
            "paperUrl": "https://www.semanticscholar.org/paper/247d160e05a1d020a5900f286fffb48c3606c3fb",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1038/s41598-025-25406-5",
            "reason_for_inclusion": "High quality: Published in Scientific Reports, a reputable Q1 journal."
        },
        {
            "year": "2025.11",
            "title": "Text-based prediction of \u0131mmunohistochemical biomarkers in breast cancer using a generative large language model: a retrospective study",
            "team": "F. D\u00f6nmez",
            "team website": "",
            "affiliation": "",
            "domain": "Oncology Biomarker Prediction from Text",
            "abstract": "",
            "venue": "Health Information Science and Systems",
            "paperUrl": "https://www.semanticscholar.org/paper/172a827bae20d4bc60e407209dccb2ad33bd6139",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1007/s13755-025-00398-8",
            "reason_for_inclusion": "Included to meet target filter limit."
        },
        {
            "year": "2025.11",
            "title": "Efficient finetuning of foundation model combined with few-shot learning improves pattern recognition in histopathology.",
            "team": "Cleo-Aron Weis",
            "team website": "",
            "affiliation": "",
            "domain": "Histopathology Pattern Recognition",
            "abstract": "",
            "venue": "Virchows Archiv : an international journal of pathology",
            "paperUrl": "https://www.semanticscholar.org/paper/a72392197e73d9a07eae7ed05d372d99aacb8019",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1007/s00428-025-04351-8",
            "reason_for_inclusion": "Included to meet target filter limit."
        },
        {
            "year": "2025.11",
            "title": "Large language models driven neural architecture search for universal and lightweight disease diagnosis on histopathology slide images",
            "team": "Chang Xu",
            "team website": "",
            "affiliation": "",
            "domain": "Histopathology Slide Diagnosis Automation",
            "abstract": "Artificial Intelligence has revolutionized healthcare by offering smart services and reducing diagnostic burden, particularly facilitating the identification and segmentation of malignant tissues. However, current task-specific approaches require disease-specific models, while universal foundation models demand costly customization for complex cases, hindering practical deployment in clinical environments. We present Pathology-NAS, a universal and lightweight medical analysis framework that leverages LLMs\u2019 knowledge to refine the architecture space across diverse scenarios, eliminating the need for exhaustive search. Pathology-NAS is pretrained on 1.3 million images across three supernet architectures, providing a robust visual foundation that generalizes across diverse tasks. Across breast cancer and diabetic retinopathy diagnosis tasks, Pathology-NAS achieves 99.98% classification accuracy while reducing FLOPs by 45% compared to leading methods. Our model delivers near-optimal architectures in just 10 iterations, bypassing the exponential search space. Pathology-NAS provides accurate tumor recognition across diverse tissues with computational efficiency, making AI-assisted diagnosis practical even in resource-constrained clinical environments.",
            "venue": "NPJ Digital Medicine",
            "paperUrl": "https://www.semanticscholar.org/paper/e8ff2fc61bdbb4dafc4a47c3a9b0db33519f7e3c",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1038/s41746-025-02042-x",
            "reason_for_inclusion": "High quality: Published in NPJ Digital Medicine, a top-tier journal."
        },
        {
            "year": "2025.11",
            "title": "Evaluating cell AI foundation models in kidney pathology with human-in-the-loop enrichment",
            "team": "Yuankai Huo",
            "team website": "",
            "affiliation": "",
            "domain": "Kidney Pathology Cell Segmentation",
            "abstract": "Large-scale artificial intelligence foundation models have emerged as promising tools for addressing healthcare challenges, including digital pathology. While many have been developed for complex tasks such as disease diagnosis and tissue quantification using extensive and diverse datasets, their readiness for seemingly simpler tasks, such as nuclei segmentation within a single organ (for example, the kidney), remains unclear. This study answers two questions: How good are current cell foundation models? and How can we improve them? We curated a multi-center, multi-disease, and multi-species dataset sampled from 2542 kidney whole slide images. Three state-of-the-art cell foundation models\u2014Cellpose, StarDist, and CellViT\u2014were evaluated. To enhance performance, we developed a human-in-the-loop strategy that distilled multi-model predictions, improving data quality while reducing reliance on pixel-level annotation. Fine-tuning was performed using the enriched datasets, and segmentation performance was quantitatively assessed. Here we show that cell nuclei segmentation in kidney pathology still requires improvement with more organ-targeted foundation models. Among the evaluated models, CellViT achieves the highest baseline performance, with an F1 score of 0.78. Fine-tuning with enriched data improves all three models, with StarDist achieving the highest F1 score of 0.82. The combination of the foundation model-generated pseudo-labels and a subset of pathologist-corrected \u201chard\u201d patches yields consistent performance gains across all models. This study establishes a benchmark for the development and deployment of cell AI foundation models tailored to real-world data. The proposed framework, which leverages foundation models with reduced expert annotation, supports more efficient workflows in clinical pathology. The rise of digital pathology has transformed traditional histology slides into vast collections of high-resolution images, enabling medical research on a much larger scale. However, analysing this data remains challenging. Foundation models\u2014advanced AI systems trained on diverse datasets\u2014offer a promising solution, but their ability to perform simpler yet essential tasks, such as identifying cell nuclei in kidney tissue, is unclear. We evaluated three leading models on a large, curated kidney image dataset and found that cell nuclei segmentation in kidney pathology still requires improvement with more organ-targeted foundation models. To enhance performance, we introduced a \u201chuman-in-the-loop\u201d approach that combines multiple foundation models with expert labeling of only the most difficult cases, improving accuracy, reducing manual labeling, and enabling more efficient pathology workflows. Guo et al. evaluate three cutting-edge cell foundation models on a diverse kidney nuclei dataset and develop a training strategy leveraging multiple cell foundation models to reduce pathologist labeling costs. Findings reveal that current histopathology models need organ-targeted improvements, and the framework consistently boosts performance.",
            "venue": "Communications Medicine",
            "paperUrl": "https://www.semanticscholar.org/paper/4c6905c93cba6bed8e11b29dcb446b11c996c49a",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1038/s43856-025-01205-x",
            "reason_for_inclusion": "High quality: Published in Communications Medicine, a reputable journal."
        },
        {
            "year": "2025.11",
            "title": "mRNABERT: advancing mRNA sequence design with a universal language model and comprehensive dataset",
            "team": "Tingjun Hou",
            "team website": "",
            "affiliation": "",
            "domain": "mRNA Sequence Design with Language Models",
            "abstract": "Designing effective mRNA sequences for therapeutics remains a formidable challenge. Inspired by successes in protein design, language models (LMs) are now being applied to RNA, but progress is often impeded by the lack of comprehensive training data. Existing models are frequently limited to UTR or CDS regions, restricting their application for complete mRNA sequences. We introduce mRNABERT, a robust, all-in-one mRNA designer pre-trained on the largest available mRNA dataset. To enhance performance, we propose a dual tokenization scheme with a cross-modality contrastive learning framework to integrate semantic information from protein sequences. On a comprehensive benchmark, mRNABERT demonstrates state-of-the-art performance, outperforming previous models in the majority of tasks for 5\u2019 UTR and CDS design, RNA-binding protein (RBP) site prediction, and full-length mRNA property prediction. It also surpasses large protein models in several related tasks. In conclusion, mRNABERT\u2019s superior performance across these diverse tasks signifies a substantial leap forward in mRNA research and therapeutic development. Designing complete mRNA sequences for new vaccines and therapies is a complex challenge. Here, the authors develop mRNABERT, a foundational AI model that designs entire mRNA sequences and demonstrates superior performance across comprehensive benchmarks.",
            "venue": "Nature Communications",
            "paperUrl": "https://www.semanticscholar.org/paper/c9cd1f5b57f86be3a656f17a0dc0ed32a4e82825",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1038/s41467-025-65340-8",
            "reason_for_inclusion": "High quality: Published in Nature Communications, a premier journal."
        }
    ],
    "benchmarks": [
        {
            "year": "2025.11",
            "title": "Cardiac Remodeling in Preeclampsia: A Large-Language-Model-Assisted Meta-Analysis and Meta-Regression.",
            "team": "George W Booz",
            "team website": "",
            "affiliation": "",
            "domain": "Cardiovascular Meta\u2010Analysis Automation",
            "abstract": "Preeclampsia is a hypertensive disorder of pregnancy associated with substantial maternal morbidity and long-term cardiovascular risk, but the consistency of echocardiographic remodeling remains unclear. We conducted a mega-meta-analysis of left ventricular function and geometry, enabled by a large language model based suite of tools. A PROSPERO-registered review (CRD420251109103) searched PubMed, Scopus, and Embase without date limits. Synthesa AI screened more than 18,000 abstracts, extracted data, assessed risk of bias, and generated Bayesian analytic code, with all outputs validated by human reviewers. Seventy-five studies including met eligibility criteria. Preeclampsia was associated with a small but statistically significant reduction in ejection fraction (mean difference -0.87%, 95% CrI -1.58 to -0.16) and a clinically meaningful impairment in global longitudinal strain (-3.08%, 95% CrI -4.13 to -2.06). Left ventricular mass index was substantially higher in the preeclampsia group (+13.10 g/m2, 95% CrI 10.06 to 16.21), as was relative wall thickness (+0.062, 95% CrI 0.042 to 0.081), whereas fractional shortening showed no significant difference (-0.60%, 95% CrI -2.15 to +0.86). Moderator analyses revealed that BMI and parity significantly influenced strain, while gestational age at diagnosis accounted for nearly all variance in ventricular mass. This mega-meta-analysis defines a remodeling phenotype of preserved ejection fraction, impaired strain, and hypertrophic adaptation consistent with subclinical systolic dysfunction. Equally, it demonstrates the transformative role of LLM-based tools, showing that evidence syntheses of this magnitude can be automated, scaled, and standardized in ways previously unattainable.",
            "venue": "Journal of cardiovascular pharmacology",
            "paperUrl": "https://www.semanticscholar.org/paper/c7090180b43a50ab27b3146a7704a8bfce998d4b",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1097/FJC.0000000000001774",
            "reason_for_inclusion": "Included to meet target filter limit."
        },
        {
            "year": "2025.11",
            "title": "Shifts in emergency physicians\u2019 attitudes toward large language model-based documentation: a pre- and post-implementation study",
            "team": "Ji Hoon Kim",
            "team website": "",
            "affiliation": "",
            "domain": "Clinical Workflow and AI Acceptance",
            "abstract": "Large language models (LLMs) can assist physicians in writing medical notes more efficiently. This study evaluates whether using an LLM assistant for writing emergency department discharge notes can reduce doctors\u2019 workload and addresses concerns regarding the incorporation of AI in medical practice. Eight emergency doctors with an average experience of 12 years participated in our study. We surveyed them prior to, post 3 days, and post 5 weeks of their LLM usage. The results showed that doctors\u2019 concerns about using LLMs decreased significantly and remained low throughout the study period. Moreover, the LLM usage considerably reduced the perceived workload, with the time required to write each discharge note reduced by one-third of the original time. These findings demonstrate that doctors readily accept and benefit from LLM assistants in their daily practice. Our study provides the first real-world evidence of how doctors\u2019 attitudes toward AI assistants change over time in clinical settings, offering valuable insights into the future implementation of LLM-based documentation tools in healthcare.",
            "venue": "Scientific Reports",
            "paperUrl": "https://www.semanticscholar.org/paper/c592c1d7d1fc5cd44c4806ac9b7d04513a5bb0e0",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1038/s41598-025-24659-4",
            "reason_for_inclusion": "High quality: Published in Scientific Reports, a reputable Q1 journal."
        },
        {
            "year": "2025.11",
            "title": "Investigating the capabilities of large vision language models in dog emotion recognition",
            "team": "Annika Bremhorst",
            "team website": "",
            "affiliation": "",
            "domain": "Animal Emotion Recognition with VLMs",
            "abstract": "Identifying emotional states in animals is a key challenge in behavioural science and a prerequisite for developing reliable welfare assessments, ethical frameworks, and robust human\u2013animal communication models. Recently, large vision-language models (LVLMs) such as GPT-4o, Gemini, and LLaVA have shown promise in general image understanding tasks, and are beginning to be applied for emotion recognition in animals. In this study, we critically evaluated the ability of state-of-the-art LVLMs to classify emotional states in dogs using a zero-shot approach. We assessed model performance on two datasets: (1) the Dog Emotions (DE) dataset, consisting of web-sourced images with layperson-generated emotion labels, and (2) the Labrador Retriever cropped-face (LRc) dataset, which stems from a rigorously controlled experimental study where emotional states were systematically elicited in dogs and defined based on the experimental context in canine emotion research. Our results revealed that while LVLMs showed moderate classification accuracy on DE, performance is likely driven by superficial correlations, such as background context and breed morphology. When evaluated on LRc, where emotional states are experimentally induced and backgrounds are minimal, performance dropped to near-chance levels, indicating limited ability to generalise based on biologically relevant cues. Background manipulation experiments further confirmed that models relied heavily on contextual features. Prompt variation and system-level instructions slightly improved response rates but did not enhance classification accuracy. These findings highlight significant limitations in the current application of LVLMs to non-human species and raise ethical and epistemological concerns regarding potential anthropocentric biases embedded in their training data. We advocate for species-sensitive AI approaches grounded in validated behavioural science, emphasising the need for high-quality, preferably experimentally-based multimodal datasets and more transparent validation. Our study underscores both the potential and the risks of using general-purpose AI to infer internal states in animals and calls for rigorous, interdisciplinary development of animal-centred computational approaches.",
            "venue": "Scientific Reports",
            "paperUrl": "https://www.semanticscholar.org/paper/43750e97d4e532536283c765b8c40944d8cbc1b5",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1038/s41598-025-25199-7",
            "reason_for_inclusion": "High quality: Published in Scientific Reports, a reputable Q1 journal."
        },
        {
            "year": "2025.11",
            "title": "The pitfalls of multiple-choice questions in generative AI and medical education",
            "team": "E. Oermann",
            "team website": "",
            "affiliation": "",
            "domain": "MCQ Format Effects on LLM Performance",
            "abstract": "The performance of Large Language Models (LLMs) on multiple-choice question (MCQ) benchmarks is frequently cited as proof of their medical capabilities. We hypothesized that LLM performance on medical MCQs may in part be illusory and driven by factors beyond medical content knowledge and reasoning capabilities. To assess this, we created a novel benchmark of free-response questions with paired MCQs (FreeMedQA). Using this benchmark, we evaluated three state-of-the-art LLMs (GPT-4o, GPT-3.5, and LLama-3-70B-instruct) and found an average absolute deterioration of 39.43% in performance on free-response questions relative to multiple-choice (p = 1.3 * 10-5) which was greater than the human performance decline of 22.29%. To isolate the role of the MCQ format on performance, we performed a masking study, iteratively masking out parts of the question stem. At 100% masking, the average LLM multiple-choice performance was 6.70% greater than random chance (p = 0.002) with one LLM (GPT-4o) obtaining an accuracy of 37.34%. Notably, for all LLMs the free-response performance was near zero. Our results highlight the shortcomings in medical MCQ benchmarks for overestimating the capabilities of LLMs in medicine, and, broadly, the potential for improving both human and machine assessments using LLM-evaluated free-response questions.",
            "venue": "Scientific Reports",
            "paperUrl": "https://www.semanticscholar.org/paper/af436088324f97a9b2106bc9a45e587aa2c8c473",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1038/s41598-025-26036-7",
            "reason_for_inclusion": "High quality: Published in Scientific Reports, a reputable Q1 journal."
        }
    ],
    "ai-agents": [
        {
            "year": "2025.11",
            "title": "MetaBeeAI: an AI pipeline for full-text systematic reviews in biology",
            "team": "Stephen Roberts",
            "team website": "",
            "affiliation": "University of Oxford; Queen Mary University of London",
            "domain": "Automated Systematic Reviews in Biology",
            "abstract": "",
            "venue": "bioRxiv",
            "paperUrl": "https://www.semanticscholar.org/paper/ea57ddb13f4b20dd70ff9af3a0e021a33316c39a",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1101/2025.11.24.690154",
            "reason_for_inclusion": "Included to meet target filter limit."
        },
        {
            "year": "2025.11",
            "title": "Neuro-symbolic AI for auditable cognitive information extraction from medical reports",
            "team": "A. Afshar-Oromieh",
            "team website": "",
            "affiliation": "",
            "domain": "Neuro\u2010Symbolic Clinical Text Extraction",
            "abstract": "Large language models (LLMs) such as GPT-4 can interpret free text, but unreliable answers, opaque reasoning, and privacy risks limit their use in healthcare. In contrast, rule-based artificial intelligence (AI) provides transparent and reproducible results but struggles with free text. We aimed to combine the strengths of both approaches to test whether such a hybrid system can autonomously and reliably extract clinical data from diagnostic imaging reports. We developed a neuro-symbolic AI that connects GPT-4 with a rule-based expert system through a semantic integration platform. GPT-4 extracted candidate facts from free-text reports, while the expert system verified them against medical rules, producing traceable, deterministic labels. We evaluated the system on 206 consecutive prostate cancer PET/CT scan reports, requiring extraction of 26 clinical parameters per report, generating 5356 data points, and answering three study questions: study inclusion, recurrent cancer identification, and prostate-specific antigen (PSA) level retrieval. Outputs were compared against physician-derived references, and discrepancies were reviewed by a blinded adjudicator. Here we show that neuro-symbolic AI outperforms GPT-4 alone and matches physicians in structuring and analysing reports. GPT-4 alone achieves F1 scores of 0.63 for study inclusion and 0.95 for recurrence detection, with 96.6% correct PSA values. Physicians reach F1 scores of 1.00 and 0.99, with 98.1% PSA accuracy. The neuro-symbolic AI scores twice 1.00 with 100% PSA accuracy and delivers always an auditable chain of reasoning. It intercepts two intentionally introduced reports with residual identifiers, preventing unintended transfer of sensitive data. Unlike standalone LLMs, neuro-symbolic AI can safely automate data extraction for clinical research and may provide a path toward trustworthy AI in healthcare practice. Medical doctors often write reports as free text, which is hard to reuse for research or care. A large language model is software that reads and writes text by imitating large networks of brain cells. This type of artificial intelligence can extract and organize important information from medical reports. But its reasoning is opaque, answers can be wrong, and it raises privacy concerns. Rule-based artificial intelligence is transparent, responds correctly, and is privacy-protecting but struggles with free text. We combined both artificial intelligence types, so each offsets the other\u2019s weaknesses. We tested the system on 206 prostate cancer imaging reports, where it extracted information correctly, showed how it reached its answers, and protected sensitive data. Pairing large language models with rule-based systems could make artificial intelligence safer, more trustworthy, and more useful in healthcare. Prenosil, Weitzel et al. unify semantically the large language model GPT-4 with a rule-based expert system to turn free-text radiology reports into structured, privacy-safe data. In a proof-of-concept on 206 prostate cancer PET/CT reports, the resulting neuro-symbolic artificial intelligence matches physicians and supports clinical trials at scale.",
            "venue": "Communications Medicine",
            "paperUrl": "https://www.semanticscholar.org/paper/166aa151726fda4f8b95dd79df1d8e56276bca77",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1038/s43856-025-01194-x",
            "reason_for_inclusion": "High quality: Published in Communications Medicine, a reputable journal."
        }
    ],
    "reviews": [
        {
            "year": "2025.11",
            "title": "The potential existential threat of large language models to online survey research",
            "team": "S. Westwood",
            "team website": "",
            "affiliation": "",
            "domain": "Survey Methodology and AI Risk",
            "abstract": "Significance Surveys are a primary source of data across the sciences, from medicine to economics. I demonstrate that the assumption that logically coherent responses are from humans is now untenable. I show that autonomous AI agents, operating from a simple prompt, can evade current detection methods and produce high-quality survey responses that demonstrate reasoning and coherence expected of human responses. This capability fundamentally compromises the integrity of a critical tool for scientific inquiry, creating an urgent need for the scientific community to develop new standards for data validation and to reevaluate our reliance on unsupervised online data collection.",
            "venue": "Proceedings of the National Academy of Sciences of the United States of America",
            "paperUrl": "https://www.semanticscholar.org/paper/9fc7f21ff6d8a51d50b6361287f5751cd1a89c79",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1073/pnas.2518075122",
            "reason_for_inclusion": "High quality: Published in Proceedings of the National Academy of Sciences, a premier journal."
        },
        {
            "year": "2025.11",
            "title": "AI and innovation in clinical trials",
            "team": "Alireza Mansouri",
            "team website": "",
            "affiliation": "",
            "domain": "AI Innovations in Clinical Trials Design",
            "abstract": "Clinical trials face persistent challenges in cost, enrollment, and generalizability. This perspective examines how artificial intelligence (AI), large language models (LLMs), adaptive trial designs, and digital twins (DTs) can modernize trial design and execution. We detail AI-driven eligibility optimization, reinforcement learning for real-time adaptation, and in silico DT modeling. Methodological, regulatory, and ethical hurdles are addressed, emphasizing the need for validated, scalable frameworks to enable responsible and widespread integration.",
            "venue": "NPJ Digital Medicine",
            "paperUrl": "https://www.semanticscholar.org/paper/c7c877cb0570876573dd7116ea75cd480cc87383",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1038/s41746-025-02048-5",
            "reason_for_inclusion": "High quality: Published in NPJ Digital Medicine, a top-tier journal."
        }
    ],
    "databases": [
        {
            "year": "2025.11",
            "title": "BV-BRC: a unified bacterial and viral bioinformatics resource with expanded functionality and AI integration.",
            "team": "Rick L. Stevens",
            "team website": "",
            "affiliation": "",
            "domain": "Bacterial and Viral Bioinformatics Resource",
            "abstract": "The Bacterial and Viral Bioinformatics Resource Center (BV-BRC; https://www.bv-brc.org) is a comprehensive resource supporting research on bacterial and viral pathogens. It currently hosts over 14 million publicly available genomes and 33 high-throughput bioinformatic analysis services with numerous visual analytic tools allowing researchers to analyze their private data, generate comparisons with public data, and share data and results with colleagues. In recent years, the BV-BRC has added several new analysis services to support rapid comparative genomics and epidemiological analysis, viral genome assembly and annotation, viral subspecies classification, wastewater analysis, and molecular docking. In addition, several existing services have been updated to incorporate state-of-the-art tools, including assembly, annotation, taxonomic classification, metagenomic read mapping, and RNA-seq analysis. A new tool, called BV-BRC Copilot, provides an AI-powered natural-language interface that combines large language models with retrieval-augmented generation to guide users through data exploration, analysis workflows, and knowledge integration. With expanded outbreak tracking pages, training and educator engagement, and continued development of novel AI-driven analytics, BV-BRC continues to provide a unified resource to meet the evolving needs of the global research community.",
            "venue": "Nucleic acids research",
            "paperUrl": "https://www.semanticscholar.org/paper/693c203c962996b3a53f115787aa2ad849ba4f16",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1093/nar/gkaf1254",
            "reason_for_inclusion": "High quality: Published in Nucleic Acids Research, a top-tier journal."
        }
    ]
}