# Research Paper Report for 2025-11-16 to 2025-11-30

## Overall Summary

Between November 16 and November 30, 2025, a diverse portfolio of 15 papers highlights the maturing intersection of large-scale AI models with domain-specific applications across cultural heritage, healthcare, biology, and methodological evaluation. A core theme is the fine-tuning and adaptation of foundation models to specialized tasks. For example, Lu et al. [1] leverage LoRA-based methods to fine-tune Qwen-14B for Aspect Sentiment Quadruple Prediction, integrating BERT, multi-layer BiLSTM, self-attention, CNN, and CRF modules to yield modest but meaningful F1 and Precision gains for sustainable cultural heritage management. Similarly, Weis et al. [3] and Dönmez [2] explore few-shot and generative LLM approaches to histopathology pattern recognition and oncology biomarker prediction, though the latter’s lack of an abstract limits our insight into model architecture and dataset specifics.

Emerging trends include neural architecture search guided by LLMs ([4] Xu et al.), which achieved 99.98% accuracy on breast cancer and diabetic retinopathy tasks while slashing FLOPs by 45%, and human-in-the-loop enrichment strategies ([5] Huo et al.) that distilled predictions from multiple cell segmentation models to boost F1-scores from 0.78 to 0.82 on kidney nuclei segmentation. Cross-modality contrastive learning is demonstrated by Hou et al. [6] in mRNABERT to unify semantic embeddings from protein sequences and mRNA for end-to-end sequence design, setting a new benchmark on RNA property prediction.

Benchmark and evaluation papers underscore both the power and the pitfalls of AI: Booz et al. [7] present a mega-meta-analysis pipeline where LLM tools screen and extract from 18,000+ abstracts, revealing subtle cardiac remodeling in preeclampsia with Bayesian effect sizes. Kim et al. [8] offer real-world evidence that LLM assistants reduce emergency physicians’ documentation time by one-third, while Bremhorst et al. [9] expose the failure modes of vision-language models in zero-shot dog emotion recognition, and Oermann et al. [10] reveal a 39.4% performance drop when shifting LLMs from multiple-choice to free-response medical questions.

AI-agent systems bring automation to systematic reviews ([11] Roberts et al.) and medical report extraction via neuro-symbolic integration ([12] Afshar-Oromieh et al.), the latter matching physician accuracy with fully auditable reasoning chains. Two review papers caution against AI’s unintended consequences: Westwood [13] warns of LLM-generated survey responses undermining data integrity, and Mansouri [14] surveys AI innovations in trial design, highlighting ethical and regulatory hurdles. Finally, Stevens [15] describes BV-BRC’s AI-augmented bacterial and viral bioinformatics resource, introducing a retrieval-augmented “Copilot” to guide users through 14 million genomes.

Taken together, these works showcase accelerated methodological innovation—LoRA fine-tuning, LLM-driven NAS, human-in-loop distillation, cross-modality learning, retrieval-augmented generation, and neuro-symbolic pipelines—alongside rigorous benchmarking and critical evaluation of AI’s limits. Practical implications span improved heritage planning, resource-efficient clinical tools, scalable meta-analysis, and trustworthy AI in healthcare. Yet, limitations around generalization, data sparsity, anthropocentric bias, and evaluation formats underscore the need for domain-targeted model design, high-quality datasets, transparent methodologies, and robust validation standards.

## Table of Contents

- [Foundation Models](#foundation-models)  
- [Benchmarks](#benchmarks)  
- [AI Agents](#ai-agents)  
- [Reviews](#reviews)  
- [Databases](#databases)  

## Foundation Models

The foundation-models category encompasses six papers ([1]–[6]) that adapt large pretrained models to highly specialized tasks. Lu et al. [1] introduce an Aspect Sentiment Quadruple Prediction (ASQP) framework by fine-tuning Qwen-14B with LoRA methods on Weibo, Dianping, and Xiaohongshu data (2018–2024). Their hybrid architecture—combining BERT embeddings, multi-layer BiLSTM, self-attention, CNN feature extractors, and a CRF layer—yields a relative F1-score improvement of 0.97% and a precision boost of 4.48%, underscoring the utility of ensemble sequence tagging for cultural sentiment analysis.

Dönmez [2] and Weis et al. [3], though lacking abstracts, signal ongoing efforts to apply generative LLMs and few-shot learning to histopathology and oncology biomarker prediction. Xu et al. [4] pioneer Pathology-NAS, a universal neural architecture search guided by LLM-derived priors. Pretrained on 1.3M images across three supernet backbones, Pathology-NAS achieves 99.98% classification accuracy for breast cancer and diabetic retinopathy, while reducing FLOPs by 45% and requiring only ten NAS iterations—demonstrating practical deployment viability in low-resource clinics.

Huo et al. [5] benchmark three cell segmentation foundation models—Cellpose, StarDist, CellViT—on a 2542-slide, multi-center kidney dataset. A human-in-the-loop distillation approach integrates pseudo-labels with pathologist-corrected “hard” patches, raising StarDist’s F1 from 0.78 to 0.82 and establishing a benchmark for organ-targeted AI workflows. Finally, Hou et al. [6] unveil mRNABERT, a universal mRNA sequence designer. Leveraging dual tokenization and cross-modality contrastive learning with protein embeddings, mRNABERT sets state-of-the-art performance on 5′ UTR/CDS design, RBP site prediction, and full-length mRNA property inference, marking a milestone in therapeutic sequence engineering.

These studies collectively demonstrate methodological advances—from LoRA fine-tuning and few-shot learning to LLM-driven NAS and cross-modality contrastive training—applied to domains as varied as cultural heritage, pathology, oncology, and RNA therapeutics. They reveal both breakthroughs in accuracy and efficiency and persistent challenges around data coverage, model generalization, and the need for organ- or task-specific foundation models.

| Index | Title                                                                                                           | Domain                                             | Venue                                               | Team                   | DOI                                  | affiliation | paperUrl |
|-------|-----------------------------------------------------------------------------------------------------------------|----------------------------------------------------|-----------------------------------------------------|------------------------|--------------------------------------|-------------|----------|
| 1     | Explainable AI with fine-tuned large language models for sustainable cultural heritage management               | Sentiment Analysis for Cultural Heritage           | Scientific Reports                                  | Huipin Lu              | 10.1038/s41598-025-25406-5           |             | [Link](https://www.semanticscholar.org/paper/247d160e05a1d020a5900f286fffb48c3606c3fb) |
| 2     | Text-based prediction of İmmunohistochemical biomarkers in breast cancer using a generative large language model | Oncology Biomarker Prediction from Text            | Health Information Science and Systems              | F. Dönmez              | 10.1007/s13755-025-00398-8           |             | [Link](https://www.semanticscholar.org/paper/172a827bae20d4bc60e407209dccb2ad33bd6139) |
| 3     | Efficient finetuning of foundation model combined with few-shot learning improves pattern recognition in histopathology. | Histopathology Pattern Recognition                 | Virchows Archiv : an international journal of pathology | Cleo-Aron Weis         | 10.1007/s00428-025-04351-8           |             | [Link](https://www.semanticscholar.org/paper/a72392197e73d9a07eae7ed05d372d99aacb8019) |
| 4     | Large language models driven neural architecture search for universal and lightweight disease diagnosis on histopathology slide images | Histopathology Slide Diagnosis Automation          | NPJ Digital Medicine                                | Chang Xu               | 10.1038/s41746-025-02042-x           |             | [Link](https://www.semanticscholar.org/paper/e8ff2fc61bdbb4dafc4a47c3a9b0db33519f7e3c) |
| 5     | Evaluating cell AI foundation models in kidney pathology with human-in-the-loop enrichment                      | Kidney Pathology Cell Segmentation                 | Communications Medicine                             | Yuankai Huo            | 10.1038/s43856-025-01205-x           |             | [Link](https://www.semanticscholar.org/paper/4c6905c93cba6bed8e11b29dcb446b11c996c49a) |
| 6     | mRNABERT: advancing mRNA sequence design with a universal language model and comprehensive dataset              | mRNA Sequence Design with Language Models          | Nature Communications                               | Tingjun Hou            | 10.1038/s41467-025-65340-8           |             | [Link](https://www.semanticscholar.org/paper/c9cd1f5b57f86be3a656f17a0dc0ed32a4e82825) |

## Benchmarks

In the benchmarks category, four papers ([7]–[10]) interrogate both the potential and the pitfalls of AI-driven evaluation and meta-analysis. Booz et al. [7] enact a PROSPERO-registered mega-meta-analysis of preeclampsia cardiac remodeling using a large-language-model suite to screen 18,000+ abstracts and generate Bayesian analytic code. Their automated pipeline, validated by human reviewers, reveals subtle but significant reductions in ejection fraction (–0.87%, 95% CrI –1.58 to –0.16) and global longitudinal strain (–3.08%, 95% CrI –4.13 to –2.06) alongside hypertrophic adaptations, demonstrating that LLM-assisted meta-analysis can scale standardized evidence synthesis.

Kim et al. [8] conduct a pre- and post-implementation study of an LLM assistant in emergency medicine documentation. By measuring perceived workload and time-to-note, they find a sustained one-third reduction in discharge note writing time and a durable drop in AI-concern scores among eight physicians, signaling strong acceptance and operational impact.

Bremhorst et al. [9] critically assess state-of-the-art vision-language models (GPT-4o, Gemini, LLaVA) on dog emotion recognition. Their zero-shot evaluation across the Dog Emotions dataset and the experimental LRc dataset exposes moderate performance driven by background cues and near-chance accuracy under controlled conditions, underscoring anthropocentric biases and generalization limits. Prompt tuning and system-level instructions improve response rates but not classification accuracy, highlighting dataset and methodological gaps.

Oermann et al. [10] probe the illusion of medical reasoning in LLMs by comparing multiple-choice and free-response performance on a novel FreeMedQA benchmark. A 39.43% average decline for LLMs when shifting to free-response (p=1.3×10⁻⁵) versus a 22.29% human decline, plus a masking study showing residual MCQ advantage, calls into question reliance on MCQ benchmarks for evaluating medical AI.

These works illustrate methodologies such as LLM-driven screening and Bayesian meta-regression [7], real-world workload metrics [8], zero-shot vision-language probing [9], and masking-based format analyses [10]. Collectively, they emphasize the need for robust, context-aware benchmarks and reveal both transformative scale and critical vulnerabilities in current AI evaluation practices.

| Index | Title                                                                                                                         | Domain                                            | Venue                               | Team            | DOI                                  | affiliation | paperUrl |
|-------|-------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------|-------------------------------------|-----------------|--------------------------------------|-------------|----------|
| 7     | Cardiac Remodeling in Preeclampsia: A Large-Language-Model-Assisted Meta-Analysis and Meta-Regression.                         | Cardiovascular Meta-Analysis Automation           | Journal of cardiovascular pharmacology | George W Booz   | 10.1097/FJC.0000000000001774         |             | [Link](https://www.semanticscholar.org/paper/c7090180b43a50ab27b3146a7704a8bfce998d4b) |
| 8     | Shifts in emergency physicians’ attitudes toward large language model-based documentation: a pre- and post-implementation study | Clinical Workflow and AI Acceptance               | Scientific Reports                  | Ji Hoon Kim     | 10.1038/s41598-025-24659-4           |             | [Link](https://www.semanticscholar.org/paper/c592c1d7d1fc5cd44c4806ac9b7d04513a5bb0e0) |
| 9     | Investigating the capabilities of large vision language models in dog emotion recognition                                    | Animal Emotion Recognition with VLMs              | Scientific Reports                  | Annika Bremhorst | 10.1038/s41598-025-25199-7           |             | [Link](https://www.semanticscholar.org/paper/43750e97d4e532536283c765b8c40944d8cbc1b5) |
| 10    | The pitfalls of multiple-choice questions in generative AI and medical education                                              | MCQ Format Effects on LLM Performance             | Scientific Reports                  | E. Oermann      | 10.1038/s41598-025-26036-7           |             | [Link](https://www.semanticscholar.org/paper/af436088324f97a9b2106bc9a45e587aa2c8c473) |

## AI Agents

The AI-agents section includes two papers ([11], [12]) that demonstrate how autonomous systems can transform routine research workflows. Roberts et al. [11] present MetaBeeAI, an end-to-end pipeline for full-text systematic reviews in biology. Though lacking an abstract, the title and venue (bioRxiv) suggest an automated suite for literature retrieval, eligibility screening, data extraction, and synthesis—potentially integrating entity recognition, citation analysis, and summary generation to accelerate review timelines.

Afshar-Oromieh et al. [12] detail a neuro-symbolic framework that interlinks GPT-4’s free-text interpretation with a rule-based expert system. Processing 206 prostate cancer PET/CT reports, the hybrid system extracts 26 parameters per report and achieves F1 scores of 1.00 on study inclusion and recurrence detection, plus 100% PSA accuracy—outperforming GPT-4 alone (F1s of 0.63 and 0.95, 96.6% PSA) and matching physician benchmarks. The pipeline provides an auditable reasoning trail and intercepts residual identifiers, addressing both transparency and privacy. This marriage of retrieval-augmented LLM outputs with deterministic rules charts a path for trustworthy clinical AI.

Together, these papers illustrate technical depth in combining machine learning with symbolic reasoning [12] and likely employing advanced NLP tools, data schemas, and workflow orchestration in systematic review automation [11]. They highlight practical implications for scalable, reliable data curation in biology and healthcare.

| Index | Title                                                                                                    | Domain                                      | Venue               | Team                   | DOI                             | affiliation                                                           | paperUrl |
|-------|----------------------------------------------------------------------------------------------------------|---------------------------------------------|---------------------|------------------------|---------------------------------|-----------------------------------------------------------------------|----------|
| 11    | MetaBeeAI: an AI pipeline for full-text systematic reviews in biology                                     | Automated Systematic Reviews in Biology     | bioRxiv             | Stephen Roberts       | 10.1101/2025.11.24.690154       | University of Oxford; Queen Mary University of London                 | [Link](https://www.semanticscholar.org/paper/ea57ddb13f4b20dd70ff9af3a0e021a33316c39a) |
| 12    | Neuro-symbolic AI for auditable cognitive information extraction from medical reports                     | Neuro-Symbolic Clinical Text Extraction     | Communications Medicine | A. Afshar-Oromieh      | 10.1038/s43856-025-01194-x       |                                                                       | [Link](https://www.semanticscholar.org/paper/166aa151726fda4f8b95dd79df1d8e56276bca77) |

## Reviews

This section contains two perspective pieces ([13], [14]) examining broader AI-methodological risks and innovations. Westwood [13] issues a stark warning about LLM-generated survey responses undermining online research integrity. By demonstrating that simple AI prompts can evade detection and produce coherent, high-quality answers, the paper calls for new validation standards to preserve survey reliability across disciplines.

Mansouri [14] surveys AI’s potential to modernize clinical trials via LLM-driven eligibility optimization, reinforcement learning for adaptive designs, and in silico digital twins. The review addresses methodological, regulatory, and ethical challenges, advocating validated, scalable frameworks for responsible integration of AI into trial design and execution.

These reviews underscore the interdisciplinary implications of AI beyond pure performance metrics, urging the research community to balance innovation with robustness, ethics, and standardization.

| Index | Title                                                                   | Domain                                | Venue                                                    | Team               | DOI                             | affiliation | paperUrl |
|-------|-------------------------------------------------------------------------|---------------------------------------|----------------------------------------------------------|--------------------|---------------------------------|-------------|----------|
| 13    | The potential existential threat of large language models to online survey research | Survey Methodology and AI Risk       | Proceedings of the National Academy of Sciences of the United States of America | S. Westwood        | 10.1073/pnas.2518075122        |             | [Link](https://www.semanticscholar.org/paper/9fc7f21ff6d8a51d50b6361287f5751cd1a89c79) |
| 14    | AI and innovation in clinical trials                                    | AI Innovations in Clinical Trials Design | NPJ Digital Medicine                                     | Alireza Mansouri   | 10.1038/s41746-025-02048-5     |             | [Link](https://www.semanticscholar.org/paper/c7c877cb0570876573dd7116ea75cd480cc87383) |

## Databases

Stevens [15] reports on BV-BRC, a unified bacterial and viral bioinformatics resource hosting over 14 million genomes and 33 analysis services. Key innovations include outbreak tracking, wastewater analysis, molecular docking, and AI-powered “BV-BRC Copilot,” which employs retrieval-augmented generation to guide users through analysis workflows. By integrating LLMs with state-of-the-art bioinformatic tools for assembly, annotation, and metagenomic mapping, BV-BRC exemplifies how large databases can harness AI to democratize pathogen research, support rapid comparative genomics, and foster collaboration.

| Index | Title                                                                                                        | Domain                                        | Venue                    | Team               | DOI                              | affiliation | paperUrl |
|-------|--------------------------------------------------------------------------------------------------------------|-----------------------------------------------|--------------------------|--------------------|----------------------------------|-------------|----------|
| 15    | BV-BRC: a unified bacterial and viral bioinformatics resource with expanded functionality and AI integration. | Bacterial and Viral Bioinformatics Resource   | Nucleic acids research   | Rick L. Stevens    | 10.1093/nar/gkaf1254             |             | [Link](https://www.semanticscholar.org/paper/693c203c962996b3a53f115787aa2ad849ba4f16) |

