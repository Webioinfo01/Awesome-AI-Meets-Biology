# Research Paper Report for 2025-07-16 to 2025-07-31

## Overall Summary

Between mid- and late July 2025, a diverse set of 15 studies highlights rapid advances in AI-driven research tools, foundation models, benchmarking efforts, methodological reviews, and database curation. A prominent theme is the emergence of agentic AI systems that autonomously orchestrate complex scientific workflows, bridging domain expertise and large language models (LLMs). In the “AI Agents” cluster, tools such as the DANDI chat assistant [1] and SciToolAgent’s knowledge‐graph–driven orchestration [7] exemplify technical innovations in pipeline automation, leveraging retrieval‐augmented generation and custom tool libraries. GeneAgent’s self-verification strategy [5] and the Virtual Lab’s nanobody design [6] further illustrate how LLMs can integrate external databases and iterative design loops to reduce hallucinations and accelerate biomolecular discovery. These agentic systems collectively demonstrate methodological depth, from notebook generation pipelines [1] to domain‐specific prompt engineering and statistical evaluation metrics like Cohen’s κ in bias assessment [3].

Foundation-model research [8,9] reveals complementary trends: Doctor Sun integrates vision‐language alignment and bilingual medical datasets (SunMed-VL) through a two-stage training process, while SAM-Med3D’s fully learnable 3D promptable segmentation model is trained on a massive SA-Med3D-140K dataset spanning 22K images and 143K masks. Both works advance multimodal and general-purpose medical AI, yet face limitations in medical concept coverage and zero-shot performance trade-offs.

Benchmark studies [10–12] underscore rigorous evaluation frameworks tailored to domain-specific tasks. The ophthalmology QA benchmark evaluates six LLMs on MedMCQA with accuracy, macro F1, ROUGE-L, BERTScore, BARTScore, AlignScore, and METEOR metrics, revealing that o1 leads in accuracy but trails GPT-4 in generation quality [10]. The HIVMedQA benchmark highlights the complexity of clinical reasoning, noting that larger model size and fine-tuning do not always yield superior real-world performance [12]. A PGx recommendation replication study [11] spotlights the need for standardized benchmarks in pharmacogenomics.

Methodological reviews [13,14] caution against unexamined deployment: medical ethics puzzles reveal LLM biases rooted in training data patterns [13], while a mixed‐methods typology of physician–chatbot interactions finds no clear input strategy that boosts clinical reasoning scores, emphasizing the need for targeted user training [14].

Finally, the VQ4PEDB database [15] offers a large-scale, multi‐center V/Q scintigraphy repository with DICOM images, clinical reports, and deep learning annotations, providing a critical resource for pulmonary embolism AI model development.

Collectively, these works demonstrate interdisciplinary connections across neurophysiology, genomics, chemistry, materials science, and clinical medicine. They underscore emerging trends in self-verifying LLM agents, knowledge-graph orchestration, and large multimodal foundation models, while highlighting persistent challenges in domain specificity, hallucination mitigation, and real-world evaluation.

## Table of Contents

- [ai-agents](#ai-agents)  
- [foundation-models](#foundation-models)  
- [benchmarks](#benchmarks)  
- [reviews](#reviews)  
- [databases](#databases)  

## ai-agents

Large language models are increasingly embedded within autonomous agents to streamline scientific workflows, data analysis, and experimental design. Studies [1], [5], and [7] present architecturally distinct agentic frameworks. Dichter et al. [1] integrate an LLM‐powered chat assistant with an automated notebook generator that inspects NWB‐formatted neurophysiology datasets (12 test cases) using scripts for structural analysis, visualization routines, and a secondary LLM to produce instructional Python notebooks—evaluated qualitatively by domain experts. GeneAgent [5] employs a paired LLM–database self‐verification loop: upon generating functional descriptions for 1,106 gene sets, the agent queries domain repositories to confirm factual accuracy, outperforming GPT-4 on both benchmark and seven novel melanoma‐derived gene sets. SciToolAgent [7] advances orchestration by leveraging a scientific tool knowledge graph for retrieval‐augmented generation, allowing graph‐driven selection among hundreds of tools in biology, chemistry, and materials science. It incorporates safety checks and is benchmarked on curated workflows (protein engineering, MOF screening) to demonstrate higher success rates than existing approaches.

Pilot studies [3] and [2] explore LLMs in systematic review workflows: Potrebny et al. [3] engineer a ChatGPT prompt on trial methods text and measure interrater agreement (Cohen’s κ with exact 95% CI) against human Risk of Bias assessments across 75 RCTs, marking the first LLM‐human κ comparison. Polymenakos’s Synthesa AI [2] screens abstracts and titles across nine systematic reviews, though technical details remain unpublished.

Yang et al. [4] and Zou [6] hint at domain applications without abstract details: an LLM‐powered primer design agent for amplicon sequencing and an AI agentic platform for designing SARS-CoV-2 nanobodies, respectively—both highlighting the broad applicability of agentic LLMs in wet-lab–oriented tasks.

Comparative analysis reveals that while [1] and [7] emphasize tool integration and user guidance, [5] focuses on factual correctness via self-verification. Limitations include dependence on curated tool graphs, domain‐specific scripting overhead, and the need for expert prompt tuning. Future directions point toward unified frameworks combining self-verification, knowledge graphs, and end-to-end workflow automation.

| Index | Title | Domain | Venue | Team | DOI | affiliation | paperUrl |
|---|---|---|---|---|---|---|---|
| 1 | Facilitating analysis of open neurophysiology data on the DANDI Archive using large language model tools | Neurophysiology data exploration and reanalysis | bioRxiv | Benjamin Dichter | 10.1101/2025.07.17.663965 | Flatiron Institute | [Link](https://www.semanticscholar.org/paper/57d843f6a604da36480c76aa4da44928b77adb43) |
| 2 | Validation of Synthesa AI, a Large Language Model-Based Screening Tool for Systematic Reviews: Results from Nine Studies | LLM-based screening tool for systematic reviews |  | Kyiakos Polymenakos | 10.1101/2025.07.16.25331632 |  | [Link](https://www.semanticscholar.org/paper/eb38a1e2b9598ebfed6f7a6c08dfe300dea412f3) |
| 3 | Using a large language model (ChatGPT) to assess risk of bias in randomized controlled trials of medical interventions: protocol for a pilot study of interrater agreement with human reviewers | LLM support for risk of bias assessment in clinical trials | BMC Medical Research Methodology | Thomas Potrebny | 10.1186/s12874-025-02631-0 |  | [Link](https://www.semanticscholar.org/paper/8adad8c65d9226dc4bb3b3ebadc8b37131bbc2e3) |
| 4 | Accelerating primer design for amplicon sequencing using large language model-powered agents. | LLM-powered primer design for amplicon sequencing | Nature biomedical engineering | Mengcheng Yang | 10.1038/s41551-025-01455-z |  | [Link](https://www.semanticscholar.org/paper/284b42743a6b4316b0e0ab0ad69a01c76a8487f7) |
| 5 | GeneAgent: self-verification language agent for gene-set analysis using domain databases | LLM agent for gene-set functional analysis with verification | Nature Methods | Zhiyong Lu | 10.1038/s41592-025-02748-6 |  | [Link](https://www.semanticscholar.org/paper/5f781329fb77a7711404361597490a4f2cd18c6a) |
| 6 | The Virtual Lab of AI agents designs new SARS-CoV-2 nanobodies | AI agentic design of SARS-CoV-2 nanobodies | Nature | James Y. Zou | 10.1038/s41586-025-09442-9 |  | [Link](https://www.semanticscholar.org/paper/d24e37aafcf48c76aca30430670bad9a61cd0fca) |
| 7 | SciToolAgent: a knowledge-graph-driven scientific agent for multitool integration | Knowledge-graph-driven LLM agent for scientific tool orchestration | Nature Computational Science | Huajun Chen | 10.1038/s43588-025-00849-y |  | [Link](https://www.semanticscholar.org/paper/804b5775524552b27d5ffd6a3ab82d0f5c5f1011) |

## foundation-models

Foundation models in biomedicine are extending to multimodal and general‐purpose tasks. Zhang et al.’s Doctor Sun [8] combines a pre‐trained vision encoder with a medical LLM, using a two-stage training regimen (feature alignment followed by instruction fine-tuning) on diverse medical text and image datasets. The release of the SunMed-VL bilingual medical multimodal dataset (covering pathology and radiology) facilitates comprehensive evaluation of the model’s ability to encode and integrate text–image relationships. He et al.’s SAM-Med3D [9] introduces a fully learnable 3D promptable segmentation VFM trained on the SA-Med3D-140K dataset comprising 22K volumes and 143K masks from 78 public and private sources. A two-stage training procedure—pretraining on large-scale data and prompt tuning on 3D point annotations—allows SAM-Med3D to generalize zero-shot to unseen anatomical structures across 16 test datasets. Performance metrics include Dice coefficient, intersection‐over‐union (IoU), and zero-shot transfer accuracy, indicating robust generalization. Comparative analysis shows Doctor Sun’s strength in multimodal instruction following and bilingual capability, whereas SAM-Med3D focuses on universal segmentation across volumetric modalities. Both works push the boundary of foundation models in resource‐intensive medical domains but face challenges in large-scale pretraining costs, domain‐specific evaluation benchmarks, and ensuring safety in clinical deployments. Future research may integrate multimodal alignment techniques from Doctor Sun with volumetric prompt engineering from SAM-Med3D to build unified medical AI platforms.

| Index | Title | Domain | Venue | Team | DOI | affiliation | paperUrl |
|---|---|---|---|---|---|---|---|
| 8 | Doctor Sun: A Bilingual Multimodal Large Language Model for Biomedical AI | Biomedical multimodal large language model development | ArXiv | Zhongheng Zhang | 10.48550/arXiv.2508.08270 |  | [Link](https://www.semanticscholar.org/paper/0823d41ac69e564813c75057b160d7fba5aa6646) |
| 9 | SAM-Med3D: A Vision Foundation Model for General-Purpose Segmentation on Volumetric Medical Images | Vision foundation model for general-purpose 3D medical segmentation | IEEE Transactions on Neural Networks and Learning Systems | Junjun He | 10.1109/TNNLS.2025.3586694 |  | [Link](https://www.semanticscholar.org/paper/08d4d1b9ff0154293916b66d43cae18182c6e3b7) |

## benchmarks

Benchmarking studies probe LLM capabilities in specialized medical tasks. Tham et al. [10] evaluate OpenAI o1 against six other LLMs on 6,990 MedMCQA ophthalmology questions, measuring accuracy (0.877 for o1), macro F1, ROUGE-L, BERTScore, BARTScore, AlignScore, and METEOR. Expert Likert‐scale assessments of usefulness and organization further contextualize quantitative metrics, revealing that while o1 leads in accuracy, GPT-4 variants excel in text‐generation fidelity. Gobbs’s PGx recommendations replication benchmark [11] assesses LLMs’ ability to reproduce guideline‐based pharmacogenomic dosing, although procedural details remain minimal. Duroux et al.’s HIVMedQA [12] introduces a curated question set with an LLM‐as-judge framework to evaluate comprehension, reasoning, recall, bias, and potential harm. Results show proprietary Gemini 2.5 Pro outperforms open models, and that complexity and cognitive bias remain major obstacles. Comparative analysis indicates that domain‐specific benchmarks ([12]) demand more nuanced evaluation metrics than general question‐answering tasks ([10]), underscoring the need for standardized, open‐source benchmark suites. Limitations include lack of transparency in proprietary model comparisons and uneven evaluation criteria across studies. Future directions involve harmonizing metrics and expanding benchmarks to cover longitudinal patient scenarios and safety‐critical contexts.

| Index | Title | Domain | Venue | Team | DOI | affiliation | paperUrl |
|---|---|---|---|---|---|---|---|
| 10 | Ophthalmological Question Answering and Reasoning Using OpenAI o1 vs Other Large Language Models. | Benchmark LLMs on ophthalmological question answering | JAMA ophthalmology | Y. Tham | 10.1001/jamaophthalmol.2025.2413 |  | [Link](https://www.semanticscholar.org/paper/b6fab798086d638085a6cc589376999fae0614ce) |
| 11 | Benchmarking large language models for replication of guideline-based PGx recommendations | Benchmarking LLMs for PGx recommendations replication | The Pharmacogenomics Journal | Allan Gobbs | 10.1038/s41397-025-00383-0 |  | [Link](https://www.semanticscholar.org/paper/033444fd24632d1c33c0db64273af4d0f31253e5) |
| 12 | HIVMedQA: Benchmarking large language models for HIV medical decision support | Benchmarking LLMs for HIV medical question answering | ArXiv | Diane Duroux | 10.48550/arXiv.2507.18143 |  | [Link](https://www.semanticscholar.org/paper/14cc521817abf23142822f880c9569498ccf129a) |

## reviews

Critical evaluations of LLM reasoning and user interaction highlight inherent risks and user strategies. Klang et al. [13] use lateral thinking puzzles and medical ethics scenarios to expose cognitive biases in ChatGPT-o1, demonstrating that pattern correlations in training data can lead to ethically unsound reasoning. Holdsworth’s mixed-methods typology [14] combines semi-structured interviews, chat‐log analysis, and linear mixed-effects modeling to categorize physician input into four types: copy-paster, selective copy-paster, summarizer, and searcher. Despite varied strategies, no input type significantly improved clinical reasoning scores, suggesting that LLM efficacy is also shaped by user training and prompt quality. Together, these reviews illustrate methodological depth—from qualitative coding frameworks to quantitative performance modeling—and underscore the necessity of combining ethical oversight with structured clinician education to ensure safe integration of chatbots in healthcare.

| Index | Title | Domain | Venue | Team | DOI | affiliation | paperUrl |
|---|---|---|---|---|---|---|---|
| 13 | Pitfalls of large language models in medical ethics reasoning | Medical ethics reasoning pitfalls of LLMs | NPJ Digital Medicine | Eyal Klang | 10.1038/s41746-025-01792-y |  | [Link](https://www.semanticscholar.org/paper/30d8956ed5be789a59d9d6c8812c386b03cda723) |
| 14 | A typology of physician input approaches to using AI chatbots for clinical decision-making: a mixed methods study | Typology of physician input strategies to LLM chatbots | medRxiv | Laura M Holdsworth | 10.1101/2025.07.23.25332002 | Stanford University School of Medicine | [Link](https://www.semanticscholar.org/paper/3d551b3bc13e128b59acef3dd5101a2ad42f572b) |

## databases

Efficient AI development requires high-quality annotated datasets. Klein et al. [15] present VQ4PEDB, a multi-center V/Q scintigraphy registry with 3,122 DICOM studies, 3,511 segmented perfusion defects, and associated clinical reports (CTPA, DVT ultrasounds, lab tests). The pipeline uses the V7 Darwin platform for crowdsourced annotation and Deep Lake for SQL-based storage, with manual and algorithmic quality assurance. Detailed metadata on scanners (Siemens, Philips, GE), ventilation agents, and imaging protocols provides a robust foundation for pulmonary embolism AI models. Challenges include data de-identification, annotation consistency, and the licensing of private cases. VQ4PEDB stands out for its depth across imaging modalities and clinical annotations, offering future avenues for federated learning and multi-modal AI evaluation.

| Index | Title | Domain | Venue | Team | DOI | affiliation | paperUrl |
|---|---|---|---|---|---|---|---|
| 15 | On the construction of a large-scale database of AI-assisted annotating lung ventilation-perfusion scintigraphy for pulmonary embolism (VQ4PEDB) | V/Q scintigraphy database for pulmonary embolism AI | Frontiers in Nuclear Medicine | Ran Klein | 10.3389/fnume.2025.1632112 |  | [Link](https://www.semanticscholar.org/paper/5ee2937fd924a03fbcfdfa00e33893a7b154a961) |