{
    "ai-agents": [
        {
            "year": "2025.10",
            "title": "Photoresponsive HOF-Based Platforms for Large Language Model-Assisted Multimodal Diagnosis of Metabolic Diseases.",
            "team": "B. Yan",
            "team website": "",
            "affiliation": "",
            "domain": "Photoresponsive sensor integrated with multimodal LLM for metabolic disease diagnosis",
            "abstract": "Artificial intelligence (AI), particularly large language models (LLMs) such as chat generative pretrained transformer (ChatGPT), is revolutionizing various fields. Here, we present an AI-enhanced olfactory diagnosis platform that integrates a photoresponsive hydrogen-bonded organic framework (HOF)-based sensor with the multimodal GPT-4o model. Eu-FDA@HOF prepared by introducing europium ions (Eu3+) and 2,5-furandicarboxylic acid (FDA) via a host-guest coassembly strategy possesses ultralong room-temperature phosphorescence (461 ms) and characteristic fluorescence emission. The photoresponsive sensor achieves selective and rapid detection of phenylpyruvic acid (PPA) and creatinine (Cr), odor molecules of phenylketonuria and renal dysfunction, respectively, with low detection limits (1.53 \u03bcM and 1.18 \u03bcM). Body odor, as a reflection of metabolic status, offers a potential foundation for olfactory diagnosis. Leveraging PL responses, GPT-4o, guided via a human-in-the-loop strategy, performs accurate concentration prediction and generates clinically interpretable diagnoses. This work bridges photoluminescent sensing and AI-driven medical interpretation, demonstrating a versatile and accessible platform for intelligent olfactory diagnostics.",
            "venue": "Analytical chemistry",
            "paperUrl": "https://www.semanticscholar.org/paper/d17aa5bf14869c5fec4b3563af21910c197c0aeb",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1021/acs.analchem.5c04807",
            "reason_for_inclusion": "High quality: Published in Analytical Chemistry, a reputable Q1 journal."
        },
        {
            "year": "2025.10",
            "title": "Validation of Synthesa AI, a Large Language Model-Based Screening Tool for Systematic Reviews: Results from Nine Pharmacologic Studies",
            "team": "Kyiakos Polymenakos",
            "team website": "",
            "affiliation": "",
            "domain": "LLM-based abstract screening for pharmacologic systematic reviews",
            "abstract": "Systematic review screening underpins the evidence base for pharmacology and drug development but remains burdensome, error-prone, and resource-intensive. Synthesa AI, a large language model (LLM)-based abstract screening tool, was developed to streamline this process by providing a transparent and prompt-driven framework for abstract screening. In this validation study, Synthesa AI was tested across 17 benchmark meta-analyses on nine therapeutic domains relevant to pharmacology and clinical pharmacotherapy. The tool screened 270,626 abstracts retrieved from PubMed and Scopus. Synthesa AI successfully identified all 163 benchmark-included studies, achieving a sensitivity of 100% (95% CI: 97.7%\u2013100.0%) and a specificity of 99.4% (95% CI: 99.37%\u201399.42%). Importantly, it reduced reviewer workload by 91.7%, with only 1,797 abstracts requiring manual review. Beyond replication, the tool identified 32 additional relevant studies that had been missed in the original reviews, representing a 19.6% increase in evidence yield. These findings highlight the potential of Synthesa AI to enhance pharmacological evidence synthesis by improving the reproducibility and comprehensiveness of systematic reviews used to evaluate drug efficacy, safety, and therapeutic positioning. Synthesa AI represents a transformative solution for living systematic reviews and large-scale evidence integration, offering a rigorous and efficient alternative to traditional human-led screening in pharmacology research.",
            "venue": "Journal of Cardiovascular Pharmacology",
            "paperUrl": "https://www.semanticscholar.org/paper/2a75b97593d1b841466707f8a8582932eecfc0fe",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1097/fjc.0000000000001768",
            "reason_for_inclusion": "Included to meet target filter limit."
        },
        {
            "year": "2025.10",
            "title": "Mining Solid-State Electrolytes from Metal-Organic Framework Databases through Large Language Models and Representation Clustering.",
            "team": "Chunpeng Yang",
            "team website": "",
            "affiliation": "",
            "domain": "LLM and clustering approach for mining MOF solid-state electrolytes",
            "abstract": "Metal-organic frameworks (MOFs) are attracting increasing attention as solid-state electrolytes (SSEs) due to their three-dimensional porous diffusion paths for Li+ migration. However, their development is hindered by their inherent complexity and the absence of design guidelines. Large language models (LLMs) and machine learning, as emerging artificial intelligence (AI) technologies, can significantly accelerate the development of MOF SSEs by analyzing data and identifying potential materials. Herein, we use LLMs and representation clustering to intelligently mine MOF SSEs from 11,393 candidate MOF materials, subsequently verified by physicochemical characterizations and electrochemical demonstration. Specifically, we adopt an interactive iteration text mining framework based on LLMs to extract information on MOF SSEs, constructing a specialized data set for structural and electrochemical properties of MOF SSEs, with high precision and recall. Each property is projected into a representation space, and representation clustering is performed on samples to mine promising MOF SSEs from a candidate MOF data set. As a typical result, NOTT-400 is successfully identified through the clustering analysis, exhibiting high Li+ conductivity (2.23 \u00d7 10-4 S cm-1) and a wide electrochemical stability window (0-4.79 V), confirming both the material feasibility and the reliability of the entire AI-driven approach. The AI-assisted mining of novel MOF SSEs, along with their design principles, creates a new paradigm for accelerating materials discovery.",
            "venue": "Journal of the American Chemical Society",
            "paperUrl": "https://www.semanticscholar.org/paper/a9ce5fb4b09b869e475f27ea4f89cc47a23e3e2b",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1021/jacs.5c12212",
            "reason_for_inclusion": "High quality: Published in Journal of the American Chemical Society, a top-tier journal."
        },
        {
            "year": "2025.10",
            "title": "QSP\u2010Copilot: An AI\u2010Augmented Platform for Accelerating Quantitative Systems Pharmacology Model Development",
            "team": "A. Farnoud",
            "team website": "",
            "affiliation": "",
            "domain": "QSP\u2010Copilot: AI-augmented platform for QSP model development",
            "abstract": "ABSTRACT Quantitative Systems Pharmacology (QSP) is a powerful approach to provide decision\u2010making support throughout the drug development process. QSP comes with many challenges in model development, validation, and applications. Traditional QSP workflows are limited by slow knowledge integration, labor\u2010intensive model construction, inconsistent validation practices, and restricted scalability. In this work, we introduce QSP\u2010Copilot, the first end\u2010to\u2010end AI\u2010augmented solution designed to improve QSP modeling workflows by integrating a multi\u2010agent system utilizing large language models (LLMs). QSP\u2010Copilot provides modular support from project scoping and model structuring to model evaluation and reporting. Through the automation of routine tasks, QSP\u2010Copilot reduces model development time by approximately 40% and improves methodological transparency through systematic documentation of literature sources and modeling assumptions. We demonstrate QSP\u2010Copilot's application for two rare diseases of blood coagulation and Gaucher disease. In the blood coagulation case, automated extraction from ten peer\u2010reviewed articles yielded 179 biological entity interaction pairs; out of these, only 105 unique mechanisms were retained after standardization. For Gaucher disease, screening nine articles produced 151 pairs, which were consolidated into 68 distinct biological interactions following the same post\u2010processing workflow. The extraction precision for blood coagulation and Gaucher disease is 99.1% and 100.0%, respectively. QSP\u2010Copilot extractions can be incorporated into effect diagrams with minimal expert filtering, significantly reducing the manual curation burden. The integration of AI\u2010augmented workflows like QSP\u2010Copilot represents a pivotal shift toward enhanced scalability and impact for QSP across the drug development pipelines, especially in disease areas where biological knowledge is sparse, such as rare diseases.",
            "venue": "CPT: Pharmacometrics & Systems Pharmacology",
            "paperUrl": "https://www.semanticscholar.org/paper/c65e1ee2971a29babd541a601d3777b20dca25ac",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1002/psp4.70127",
            "reason_for_inclusion": "Included to meet target filter limit."
        },
        {
            "year": "2025.10",
            "title": "Construction of an artificial-intelligence agent for the discovery of next-generation white-LED phosphors.",
            "team": "Yi\u2010Yang Sun",
            "team website": "",
            "affiliation": "",
            "domain": "AI agent with retrieval-augmented knowledge base for phosphor materials discovery",
            "abstract": "Large language models have been extensively employed for scientific research from different aspects, yet their performance is often limited by gaps in highly specialized knowledge. To bridge this divide, in this perspective we take phosphor materials for white LED applications as a model system and construct a domain-specific knowledge base that couples retrieval-augmented generation with a numerical-querying model context protocol. By automatically extracting and structuring data from more than 5400 publications-including chemical compositions, crystallographic parameters, excitation-emission wavelengths, and synthesis conditions-we construct an artificial-intelligence agent that delivers both broad semantic search and exact parameter lookup, each answer accompanied by verifiable references. This hybrid approach mitigates hallucinations, and improves recall and precision in expert-level question-answering. Finally, we outline how linking this curated corpus to lightweight machine-learning models and even automated experimental synthesis facilities can close the loop from target specification to experimental validation, offering a blueprint for accelerated materials discovery.",
            "venue": "Physical chemistry chemical physics : PCCP",
            "paperUrl": "https://www.semanticscholar.org/paper/05f3a7aa54c4fc44a92d2d00927a5efd524e5ece",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1039/d5cp03582a",
            "reason_for_inclusion": "High quality: Published in Physical Chemistry Chemical Physics, a reputable Q1 journal."
        }
    ],
    "benchmarks": [
        {
            "year": "2025.10",
            "title": "Comparative evaluation of large language model\u2013based chatbots in a septic arthritis scenario: ChatGPT, Claude, and Perplexity",
            "team": "\u00d6zlem Bayrak",
            "team website": "",
            "affiliation": "",
            "domain": "Comparative evaluation of LLM chatbots in septic arthritis scenarios",
            "abstract": "Objective: This study aimed to comparatively evaluate the clinical knowledge generation performance of 3 widely used large language model (LLM)-based chatbots (ChatGPT, Claude, and Perplexity) in the context of septic arthritis.\n\nMethods: This cross-sectional comparative study was based on 24 scenario-based clinical questions developed in accordance with the SANJO guideline (Management of Septic Arthritis in Native Joints) of the European Bone and Joint Infection Society.Responses generated by ChatGPT (OpenAI GPT-4), Claude 2 (Anthropic), and Perplexity AI were independently assessed by 2 senior experts: 1 in orthopedic surgery and the other in infectious diseases. Each response was evaluated across 6 domains: scientific accuracy, content depth, termino logical consistency, clinical applicability, brevity, and reference support, using a 5-point Likert scale.\n\nResults: All 3 LLM-based chatbots achieved perfect scores in accuracy and terminological consistency (P = 1.000), and no significant difference was observed in clinical applicability (P = .912). Perplexity scored significantly lower in content depth compared to both ChatGPT (P = .001) and Claude (P = .041), whereas ChatGPT and Claude did not differ significantly (P = .807). ChatGPT produced significantly more unnecessary elaboration than Claude (P = .009) and Perplexity (P < .001), while Claude and Perplexity were comparable (P = .115). For reference support, Perplexity scored significantly higher than both ChatGPT (P < .001) and Claude (P < .001), with no difference between the latter 2 (P = 1.000). Overall, Perplexity achieved the highest total score (P < .001), followed by ChatGPT and Claude. Interrater agree\nment was substantial (\u03ba = 0.72).\n\nConclusion: The LLM-based chat platforms demonstrated overall high performance, but their strengths differed across evaluation \ndomains. While ChatGPT and Claude provided more comprehensive and detailed responses, Perplexity offered stronger reference support. These findings suggest that context-specific selection of LLMs is essential, as the optimal choice may vary depending on whether detailed explanation or robust referencing is prioritized.\n\nCite this article as: Bayrak HC, Karag\u00f6z B, Bayrak \u00d6. Comparative evaluation of large language model\u2013based chatbots in a septic arthritis scenario: ChatGPT, Claude, and Perplexity. Acta Orthop Traumatol Turc., Published online XX X, 2025. doi:10.5152/j.aott.2025.25428.",
            "venue": "Acta Orthopaedica et Traumatologica Turcica",
            "paperUrl": "https://www.semanticscholar.org/paper/9b089d091cbba4c292c025d4903001a08ab326c0",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.5152/j.aott.2025.25428",
            "reason_for_inclusion": "Included to meet target filter limit."
        },
        {
            "year": "2025.10",
            "title": "Evolution of AI in anatomy education study based on comparison of current large language models against historical ChatGPT performance",
            "team": "Volodymyr Mavrych",
            "team website": "",
            "affiliation": "",
            "domain": "Comparative performance of current versus historical LLMs in anatomy education",
            "abstract": "The integration of Large Language Models (LLMs) in medical education has gained significant attention, particularly in their ability to handle complex medical knowledge assessments. However, a comprehensive evaluation of their performance in anatomical education remains limited. To evaluate the performance accuracy of current LLMs compared to previous versions in answering anatomical multiple-choice questions and assessing their reliability across different anatomical topics. We analyzed the performance of four LLMs (GPT-4o, Claude, Copilot, and Gemini) on 325 USMLE-style MCQs covering seven anatomical topics. Each model attempted the questions three times. Results were compared with the previous year\u2019s GPT-3.5 performance and random guessing. Statistical analysis included chi-square tests for performance differences. Current LLMs achieved an average accuracy of 76.8\u2009\u00b1\u200912.2%, significantly higher than GPT-3.5 (44.4\u2009\u00b1\u20098.5%) and random responses (19.4\u2009\u00b1\u20095.9%). GPT-4o demonstrated the highest accuracy (92.9\u2009\u00b1\u20092.5%), followed by Claude (76.7\u2009\u00b1\u20095.7%), Copilot (73.9\u2009\u00b1\u200911.9%), and Gemini (63.7\u2009\u00b1\u20096.5%). Performance varied significantly across anatomical topics, with Head & Neck (79.5%) and Abdomen (78.7%) showing the highest accuracy rates, while Upper Limb questions showed the lowest performance (72.9%). Only 29.5% of questions were answered correctly by all LLMs, and 2.5% were never answered correctly. Statistical analysis confirmed significant differences between models and across topics (\u03c72 = 182.11\u2013518.32, p\u2009<\u20090.001). Current LLMs show markedly improved performance in anatomical knowledge assessment compared to previous versions, with GPT-4o demonstrating superior accuracy and consistency. However, performance variations across anatomical topics and between models suggest the need for careful consideration in educational applications. These tools show promise as supplementary resources in medical education while highlighting the continued necessity for human expertise.",
            "venue": "Scientific Reports",
            "paperUrl": "https://www.semanticscholar.org/paper/c7971a60f6f5d72b3ddc29767eb9905d0a01dbe5",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1038/s41598-025-22437-w",
            "reason_for_inclusion": "High quality: Published in Scientific Reports, a reputable Q1 journal."
        },
        {
            "year": "2025.10",
            "title": "Compliance and factuality of large language models for clinical research document generation.",
            "team": "Jimeng Sun",
            "team website": "",
            "affiliation": "",
            "domain": "Evaluation of compliance and factuality in clinical research document generation by LLMs",
            "abstract": "",
            "venue": "Journal of the American Medical Informatics Association : JAMIA",
            "paperUrl": "https://www.semanticscholar.org/paper/a4c43e72d1311ea886405e9bdc942f49d5357046",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1093/jamia/ocaf174",
            "reason_for_inclusion": "High quality: Published in Journal of the American Medical Informatics Association, a leading journal in medical informatics."
        },
        {
            "year": "2025.10",
            "title": "Benchmarking large language models for personalized, biomarker-based health intervention recommendations",
            "team": "Georg Fuellen",
            "team website": "",
            "affiliation": "",
            "domain": "Benchmarking LLMs for personalized biomarker-based health recommendations",
            "abstract": "The use of large language models (LLMs) in clinical diagnostics and intervention planning is expanding, yet their utility for personalized recommendations for longevity interventions remains opaque. We extended the BioChatter framework to benchmark LLMs\u2019 ability to generate personalized longevity intervention recommendations based on biomarker profiles while adhering to key medical validation requirements. Using 25 individual profiles across three different age groups, we generated 1000 diverse test cases covering interventions such as caloric restriction, fasting and supplements. Evaluating 56000 model responses via an LLM-as-a-Judge system with clinician validated ground truths, we found that proprietary models outperformed open-source models especially in comprehensiveness. However, even with Retrieval-Augmented Generation (RAG), all models exhibited limitations in addressing key medical validation requirements, prompt stability, and handling age-related biases. Our findings highlight limited suitability of LLMs for unsupervised longevity intervention recommendations. Our open-source framework offers a foundation for advancing AI benchmarking in various medical contexts.",
            "venue": "NPJ Digital Medicine",
            "paperUrl": "https://www.semanticscholar.org/paper/56739c6a235efbc01afbb0fb198538219fefb181",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1038/s41746-025-01996-2",
            "reason_for_inclusion": "High quality: Published in NPJ Digital Medicine, a Nature partner journal."
        },
        {
            "year": "2025.10",
            "title": "Prompt-dependent performance of multimodal AI model in oral diagnosis: a comprehensive analysis of accuracy, narrative quality, calibration, and latency versus human experts",
            "team": "Asmaa Abou-Bakr",
            "team website": "",
            "affiliation": "",
            "domain": "Prompt strategy effects on multimodal AI oral diagnosis performance",
            "abstract": "Prompt design is a critical yet underexplored factor influencing the diagnostic performance of large language models (LLMs). Gemini Pro 2.5 shows promise in multimodal reasoning, but no prior study has systematically compared prompt structures in oral datasets against expert benchmarks. This study aimed to evaluate the diagnostic performance of a multimodal LLM (Gemini Pro 2.5) under different prompting strategies compared with oral medicine experts using prospective, histopathology-verified clinical vignettes. In a prospective, paired diagnostic accuracy study, Gemini pro 2.5 (a multimodal LLM) was evaluated under three prompting strategies: Direct (P-1), Chain-of-Thought (P-2), and Self-Reflection (P-3) on 300 oral lesion cases with histopathologic confirmation. Each prompt was applied to identical inputs and compared against diagnoses from board-certified oral medicine specialists. Accuracy, rubric-based narrative quality, probability calibration, and computational efficiency were assessed under STARD-AI guidelines. Human experts achieved the highest Top-1 accuracy (61%), but Chain-of-Thought prompting (P-2) led AI performance in Top-3 accuracy (82%) and produced the highest explanation quality (mean rubric score 8.49/10). No AI prompt matched human performance in low-difficulty cases. P-2 also showed the best calibration (Brier score 0.238) compared to P-1 and P-3. Resource-wise, Direct prompting was fastest, but longer outputs modestly improved Top-3 recall. Mixed-effects modeling confirmed that AI performance varied significantly by prompt structure, highlighting context-specific trade-offs. Prompt structure significantly affects the diagnostic performance and interpretability of AI-generated differentials in oral lesion diagnosis. While expert clinicians remain superior in straightforward cases, structured prompting, particularly Chain-of-Thought, may enhance AI reliability in complex diagnostic scenarios. These findings support the integration of prompt engineering into AI-assisted diagnostic tools to augment clinical decision-making in oral medicine.",
            "venue": "Scientific Reports",
            "paperUrl": "https://www.semanticscholar.org/paper/3ee4da3d68df74aaef3fc69df14dea76d32223a8",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1038/s41598-025-22979-z",
            "reason_for_inclusion": "High quality: Published in Scientific Reports, a reputable Q1 journal."
        }
    ],
    "reviews": [
        {
            "year": "2025.10",
            "title": "Large language model chatbots for patient education in kidney stones: a scoping review",
            "team": "N. Bhojani",
            "team website": "",
            "affiliation": "",
            "domain": "Scoping review of LLM chatbots for patient education in kidney stones",
            "abstract": "",
            "venue": "World Journal of Urology",
            "paperUrl": "https://www.semanticscholar.org/paper/b6078700f6315eff5ce13121caa8e06d3d2e0f88",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1007/s00345-025-06019-z",
            "reason_for_inclusion": "Included to meet target filter limit."
        },
        {
            "year": "2025.10",
            "title": "Improving Large Language Model Applications in the Medical and Nursing Domains With Retrieval-Augmented Generation: Scoping Review",
            "team": "Ying Wu",
            "team website": "",
            "affiliation": "",
            "domain": "Scoping review of retrieval-augmented generation in medical and nursing domains",
            "abstract": "Background Retrieval-augmented generation (RAG) is increasingly used to improve large language models in the medical and nursing domains. However, a comprehensive understanding of its specific architecture and applications in medical and nursing reasoning remains limited. Objective We aimed to summarize the current state, existing limitations, and future development directions of RAG in the medical and nursing domains. Methods The PubMed, Web of Science, IEEE Xplore, and arXiv databases were searched for relevant articles using queries that combined terms related to RAG, medical, and nursing domains, covering the period from November 1, 2022, to May 31, 2025. This review was conducted following the PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses Extension for Scoping Reviews) guidelines. Results A total of 917 articles were retrieved, of which 67 met the inclusion criteria. Most studies focused on the medical domain (63/67, 94%), while only a few addressed nursing applications (4/67, 6%). The RAG frameworks included in this review were categorized into 5 functional types: text-based RAG (36/67, 54%), knowledge graph\u2013enhanced RAG (17/67, 25%), agentic RAG (6/67, 9%), multimodal RAG (2/67, 3%), and plug-and-play RAG (6/67, 9%). On the basis of the Simon decision-making process theory, we divided the RAG workflow into 4 stages: intent recognition, knowledge retrieval, knowledge integration, and generation. Only 26 studies included explicit reasoning support, and few were aligned with real-world clinical workflows. Only 12 studies attempted to address ethical considerations related to RAG. Conclusions We identified 4 key shifts in recent RAG development: shifting from surface-level matching toward contextualized intent recognition, from vague semantics toward logic-driven dynamic retrieval, from passive toward active knowledge retrieval, and from simple aggregation toward coherent context construction. However, most RAG systems in the medical and nursing domains have not yet introduced reasoning methods, and those that have are still predominantly reliant on data\u2011driven associations without causal modeling. This highlights the need to integrate causal mechanisms for more effective and domain-relevant reasoning in health care. Trial Registration OSF Registries 10.17605/OSF.IO/WBSV5; https://osf.io/wbsv5",
            "venue": "Journal of Medical Internet Research",
            "paperUrl": "https://www.semanticscholar.org/paper/7215c7382de7800c038787235fa4305e2e6af4ad",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.2196/80557",
            "reason_for_inclusion": "Included to meet target filter limit."
        }
    ],
    "foundation-models": [
        {
            "year": "2025.10",
            "title": "Scaling Large Language Models for Next-Generation Single-Cell Analysis",
            "team": "David van Dijk",
            "team website": "",
            "affiliation": "Yale University",
            "domain": "Scaled LLM pretraining for next-generation single-cell analysis",
            "abstract": "Single-cell RNA sequencing has transformed our understanding of cellular diversity, yet current single-cell foundation models (scFMs) remain limited in their scalability, flexibility across diverse tasks, and ability to natively integrate textual information. In this work, we build upon the Cell2Sentence (C2S) framework, which represents scRNA-seq profiles as textual \u201ccell sentences,\u201d to train Large Language Models (LLMs) on a corpus comprising over one billion tokens of transcriptomic data, biological text, and metadata. Scaling the model to 27 billion parameters yields consistent improvements in predictive and generative capabilities and supports advanced downstream tasks that require synthesis of information across multi-cellular contexts. Targeted fine-tuning with modern reinforcement learning techniques produces strong performance in perturbation response prediction, natural language interpretation, and complex biological reasoning. This predictive strength directly enabled a dual-context virtual screen that uncovered a striking context split for the kinase inhibitor silmitasertib (CX-4945), suggesting its potential as a synergistic, interferon-conditional amplifier of antigen presentation. Experimental validation in human cell models unseen during training confirmed this hypothesis, demonstrating that C2S-Scale can generate biologically grounded, testable discoveries of context-conditioned biology. C2S-Scale unifies transcriptomic and textual data at unprecedented scales, surpassing both specialized single-cell models and general-purpose LLMs to provide a platform for next-generation single-cell analysis and the development of \u201cvirtual cells.\u201d",
            "venue": "bioRxiv",
            "paperUrl": "https://www.semanticscholar.org/paper/fdb0acc2938c6d058d1bce9414dad649c869c25a",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1101/2025.04.14.648850",
            "reason_for_inclusion": "High relevance: Directly matches user's research interests."
        },
        {
            "year": "2025.10",
            "title": "A visual language model enabling intelligent nanomaterial scanning electron micrograph annotation.",
            "team": "Hong Wang",
            "team website": "",
            "affiliation": "",
            "domain": "Domain-adapted vision-language model for nanomaterial SEM annotation",
            "abstract": "Artificial intelligence (AI) has significantly advanced the research and development of materials science through data-driven approaches. However, the large number of labeled datasets required for AI to operate is difficult to obtain in current times due to the time-consuming and laborious nature of manual labeling. The morphology of nanomaterials is crucial for the study of their properties, and scanning electron microscopy is among the key techniques for morphology characterization. For nanomaterials, the structural complexity makes the annotation of Scanning Electron Microscopy (SEM) images extremely challenging, with very few labeled images available. Therefore, it is urgent to develop an automatic pattern recognition technology for SEM images of nanomaterials without relying on labeled data. In this paper, we develop the Scanning Electron Microscopy Vision-Language Model (SEM-VLM), which is a domain-specific adaptation of the Vision-Language Model (VLM) for nanomaterials science. The model is trained via contrastive learning on SEM image-text pairs extracted from the literature. SEM-VLM demonstrates superior cross-modal retrieval performance over the general-domain model Contrastive Language-Image Pretraining (CLIP) and random baselines with Recall@10 and Recall@50 metrics, and keyword searches show its robust capability to retrieve relevant images. SEM-VLM also achieves high accuracy in zero-shot classification through ensemble vision-language alignment, outperforming CLIP. In few-shot settings, SEM-VLM with 2.1% training labels exhibits superior performance compared with the fully supervised model (EMCNet: Graph-Nets for Electron Micrograph Classification). Activation mapping analysis reveals precise localization of critical nanoscale features (particles, holes, and probe tips), providing more interpretable results than conventional approaches while maintaining operational reliability. This multimodal framework reduces labeled dataset dependency by orders of magnitude and enables automated high-precision classification.",
            "venue": "Nanoscale",
            "paperUrl": "https://www.semanticscholar.org/paper/60ff987179dd2874300a3180bc8a37995b20035a",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1039/d5nr03027g",
            "reason_for_inclusion": "High quality: Published in Nanoscale, a reputable Q1 journal."
        }
    ],
    "databases": [
        {
            "year": "2025.10",
            "title": "AI-Powered Workflow for Constructing Organic Materials Databases from the Literature: Integrating Large Language Models",
            "team": "Yue Huang",
            "team website": "",
            "affiliation": "",
            "domain": "Automated materials science database construction via LLMs",
            "abstract": "We developed an end-to-end workflow to automate the construction of materials science databases from published literature, addressing a traditionally manual, time-intensive, and labor-intensive process. The work systematically evaluates and compares different machine learning (ML) methods to optimize each task. For identifying relevant publications, we tested various ML techniques and concluded that a combination of large language model (LLM)-based embeddings, clustering, and direct LLM queries is most effective. In the subsequent data extraction phase, we employed OpenAI\u2019s GPT-4 to extract materials and their properties, achieving accuracy comparable to manually curated data sets. Additionally, we integrated AI/ML methods to automatically generate SMILES from chemical structure images, expanding the workflow\u2019s applicability to organic materials. To validate the workflow, we applied it to studying organic donor materials in organic photovoltaic devices and benchmarked its performance against a manually curated data set derived from 503 papers. The results demonstrate the workflow\u2019s efficiency and accuracy. Finally, based on our findings, we provide recommendations for selecting the best ML methods for each task and propose further improvements for the future tool development. This workflow represents a major advancement in accelerating the development of materials science databases and enables data science applications in a broader range of research topics that were historically infeasible due to the lack of available data sets.",
            "venue": "ACS Omega",
            "paperUrl": "https://www.semanticscholar.org/paper/4f2dc57404b117f11dae7e704c9a360440a02d27",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1021/acsomega.5c03612",
            "reason_for_inclusion": "High quality: Published in ACS Omega, a reputable Q1 journal."
        }
    ]
}