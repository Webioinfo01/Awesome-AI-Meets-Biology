# Research Paper Report for 2025-09-16 to 2025-09-30

## Overall Summary

Between September 16 and September 30 of 2025, a diverse set of 15 papers [1–15] were published, spanning AI agents, benchmarking studies, methodological reviews, foundation models, and database releases. A unifying theme is the adaptation and specialization of large foundation models (e.g., vision-language models, large language models) to domain-specific tasks, from neuronal cell-type classification in brain-computer interfaces (BCIs) [1] and ultra-low bitrate image compression [2] to single-cell transcriptomics [3] and genomics question answering using small language models [4]. Across these “AI Agents” papers, innovations include few-shot repurposing of vision-language models (VLMs) for in vivo neuronal subtype inference, alternating rate-distortion optimization to extract multimodal (visual/textual) representations for compression, contrastive learning for LLM-derived gene embeddings, and agentic frameworks that orchestrate small models with external genomic databases.

The benchmarking category [5–9] highlights persistent gaps between general-purpose LLMs and specialized AI systems. On medical imaging tasks, ECG-dedicated AI (ECG Buddy) outperformed ChatGPT-4o and Gemini in myocardial infarction detection by over 30% in accuracy and AUC [5]. Similarly, two studies show that while GPT-4 variants excel in consistency checks of AI RCT reporting (CONSORT-AI) [6] and improve clinical language tasks via DPO fine-tuning over supervised fine-tuning [7], they remain inferior to domain-specific guidelines and frameworks in critical guidance tasks [8]. A multi-agent reinforcement learning setup for radiology report generation and evaluation [9] illustrates how modular architectures (ten specialized agents) can integrate LLMs/LVMs and expert feedback to yield trustworthy clinical outputs.

Methodological reviews [10,11] address sustainability and economics, applying Delphi–Conjoint Analysis to rank supply-chain factors (environmental impact, resource optimization, ethics) for LLM deployments [10], and dissecting the trade-offs between generative AI accuracy and computational cost in large healthcare systems [11]. These studies underscore responsible AI concerns, advocating for optimized hybrid model portfolios.

Foundation-model papers [12–14] push technical boundaries: LEADS trains on over 633K systematic review samples to outperform four cutting-edge LLMs in literature mining tasks, achieving 0.81 recall and 26.9% time savings in expert study selection [12]. UA-VLFM combines MAE pretraining, LoRA adaptation, and Dirichlet contrastive learning to yield F1 scores up to 0.9839 in vitreoretinal lymphoma diagnosis on OCT images [13]. Meanwhile, a transformer-based ALBERT inference on a 14 nm analog AI chip demonstrates near iso-accuracy (within 1.8% of floating-point) across GLUE tasks, with hardware-aware fine-tuning and drift-compensation techniques [14].

Finally, the release of LMOD+ [15], a comprehensive multimodal ophthalmology dataset with 32,633 instances and annotations across 12 diseases and 5 modalities, fills a crucial gap for generative MLLM evaluation. Zero-shot disease screening reached only ~58% accuracy, indicating room for improvement.

Emerging trends across categories include the pivot from generic to specialized models, creative few-shot/fine-tuning and contrastive learning methodologies, modular agentic frameworks, and the convergence of model performance with resource, sustainability, and cost considerations. Breakthroughs such as hardware-level analog inference [14] and high-accuracy VLM-based neuronal inference [1] are tempered by limitations: hallucinations in LLM explanations [5], performance bottlenecks in ultra-low bitrate compression [2], reliance on high-quality domain data [12], and incomplete autoguidance in clinical settings [8]. Collectively, these works chart a path toward trustworthy, efficient, and domain-adapted AI systems.

## Table of Contents
- [AI Agents](#ai-agents)  
- [Benchmarks](#benchmarks)  
- [Reviews](#reviews)  
- [Foundation Models](#foundation-models)  
- [Databases](#databases)  

## AI Agents

The AI Agents category (papers [1–4]) demonstrates how repurposing and orchestrating language and vision models can address specialized biological and biomedical tasks. Jia Liu’s BCI-Agent [1] pioneers using pretrained vision-language models as few-shot learners to classify neuronal subtypes from extracellular electrophysiology, achieving training-free inference, automated molecular atlas validation, and stable neuron tracking. This VLM-based framework embeds molecular identities into low-dimensional manifolds, revealing dynamic cell-type-specific trajectories in rodent motor learning and enabling subtype inference in human Neuropixels recordings via atlas integration.  

Chuanmin Jia’s MKIC [2] integrates natural visual and human language knowledge through an Alternating Rate-Distortion Optimization that extracts global semantic text embeddings and local visual feature maps. These multimodal codes feed into a generative foundation model for ultra-low bitrate image compression, outperforming existing codecs in reconstruction quality under sparse bitstreams. Technical innovations include decoupled knowledge storage and joint multimodal representation within a single generator.

Feng Tian et al. [3] tackle low-sample scRNA-seq by mapping highly expressed genes to NCBI descriptions, embedding via text-embedding-ada-002, BioBERT, and SciBERT, and applying contrastive learning to fine-tune cell embeddings. On retinal ganglion cells vulnerable to glaucoma, this approach boosts subtype classification accuracy and uncovers vulnerability pathways, illustrating how LLM-derived embeddings can augment biological analysis under scarce data.

Daniel Trejo-Baños’ Nano Bio-Agents (NBA) [4] explores small (<10B-parameter) language models with task decomposition and tool orchestration to interface with NCBI and AlphaGenome. On the GeneTuring benchmark, NBA agents reach 98% accuracy, matching or exceeding larger models while drastically reducing compute demands. This agentic framework addresses hallucination and cost challenges, democratizing genomics QA.

| Index | Title                                                                                               | Domain                                          | Venue                                     | Team                        | DOI                         | affiliation                                                                                                                                                                      | paperUrl                                                                                                  |
|-------|-----------------------------------------------------------------------------------------------------|-------------------------------------------------|-------------------------------------------|-----------------------------|-----------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------|
| 1     | An AI Agent for cell-type specific brain computer interfaces                                          | BCI AI agent for cell-type classification       | bioRxiv                                  | Jia Liu                     | 10.1101/2025.09.11.675660    | John A. Paulson School of Engineering and Applied Sciences, Harvard University, Boston, MA, USA                                                                                 | [Link](https://www.semanticscholar.org/paper/d90383ebe5de7fe2ea516b899a1656d13656b2fc)                    |
| 2     | Exploring Multimodal Knowledge for Image Compression via Large Foundation Models                     | LLM-guided multimodal image compression         | IEEE Transactions on Image Processing    | Chuanmin Jia                | 10.1109/TIP.2025.3607616     |                                                                                                                                                                                  | [Link](https://www.semanticscholar.org/paper/a4035c0100232b2438b309aa3b03cad8f3d25571)                    |
| 3     | Contrastive Learning Enhances Language Model Based Cell Embeddings for Low-Sample Single Cell Transcriptomics | LLM-derived cell embeddings for scRNA-seq       | ArXiv                                     | Feng Tian                   | 10.48550/arXiv.2509.23543    |                                                                                                                                                                                  | [Link](https://www.semanticscholar.org/paper/bb0e3f2e7eccb8027b4fa4e5dbc4c97f85b6c2d7)                    |
| 4     | Nano Bio-Agents (NBA): Small Language Model Agents for Genomics                                      | Small language model agents for genomics (NBA)  | ArXiv                                     | Daniel Trejo-Baños         | 10.48550/arXiv.2509.19566    |                                                                                                                                                                                  | [Link](https://www.semanticscholar.org/paper/b3dc8722e83573c7a6143e2b590b5791a4cf475e)                    |

## Benchmarks

The Benchmarks category (papers [5–9]) critically evaluates general-purpose LLMs and compares them against specialized tools or fine-tuning strategies in healthcare and clinical contexts. Keehyuck Lee et al. [5] compare ChatGPT-4o, Gemini 2.5 Pro, and ECG Buddy on a 12-lead ECG dataset (928 images), reporting ECG Buddy’s accuracy of 96.98% and AUC 98.8%, significantly outperforming LLMs (ChatGPT 65.95% accuracy, AUC 57.34%) via DeLong tests (P<.001). A qualitative error analysis further highlights the high hallucination rate in LLM explanations, underscoring the necessity of domain-specific AI.

Zhaoxiang Bian [6] employs six LLM variants to assess 41 AI-intervention RCT abstracts against CONSORT-AI criteria, measuring Overall Consistency Score (OCS), recall, and inter-rater reliability. GPT-4 variants (gpt-4-00125-preview) achieve OCS ~86.5%, outperforming gpt-3.5-turbo-0125 at 61.9%, but identify gaps in detailing inclusion/exclusion criteria (Item 2 average OCS 48.8%), indicating prompt refinement needs.

Jonathan H. Chen [7] compares Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) on Llama3-8B and Mistral-7B across text classification, clinical reasoning, summarization, and triage. SFT boosts rule-based classification F1 to 0.98–0.97; DPO on top improves reasoning (from 22% to 36–40%) and triage F1 (provider: 0.74, urgency: up to 0.91) but incurs 2–3× more compute, guiding informaticists on method selection.

Bekir Karagöz [8] compares ChatGPT-4o vs DeepSeek AI on 14 AAOS-based clavicle fracture questions, using DISCERN, PEMAT, CLEAR, Flesch-Kincaid, and accuracy metrics. Both models yield high accuracy (ChatGPT 0.93, DeepSeek 0.89), but neither attains actionable PEMAT scores and occasional hallucinations persist. DeepSeek’s longer responses and marginally higher CLEAR score (18 vs 16) did not translate into superior overall quality.

Essam A. Rashed [9] introduces a multi-agent reinforcement learning framework for radiology report generation, combining ten specialized agents (image analysis, feature extraction, generation, review, evaluation) with LLM/LVM modules. Using ChatGPT-4o and public radiology datasets, the framework aligns evaluation with LLM lifecycles (pretraining, finetuning, alignment, deployment), offering fine-grained assessment of detection/segmentation accuracy and consensus-level clinical relevance.

| Index | Title                                                                                                             | Domain                                                          | Venue                                               | Team                       | DOI                          | affiliation | paperUrl                                                                                               |
|-------|-------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------|-----------------------------------------------------|----------------------------|------------------------------|-------------|--------------------------------------------------------------------------------------------------------|
| 5     | Comparative Diagnostic Performance of a Multimodal Large Language Model Versus a Dedicated Electrocardiogram AI in Detecting Myocardial Infarction From Electrocardiogram Images: Comparative Study | ECG image–based MI diagnosis comparison                        | JMIR AI                                            | Keehyuck Lee               | 10.2196/75910               |             | [Link](https://www.semanticscholar.org/paper/9e8ad3169a37fe49b3bbf6bd6319aa6c47e6665f)                 |
| 6     | Using Large Language Models to Assess the Consistency of Randomized Controlled Trials on AI Interventions With CONSORT-AI: Cross-Sectional Survey              | LLM consistency assessment of AI RCTs                         | Journal of Medical Internet Research               | Zhaoxiang Bian             | 10.2196/72412               |             | [Link](https://www.semanticscholar.org/paper/fa0d96aa69d0f420172727ffe5402530f1f57d33)                |
| 7     | Fine-Tuning Methods for Large Language Models in Clinical Medicine by Supervised Fine-Tuning and Direct Preference Optimization: Comparative Evaluation             | SFT vs DPO fine-tuning in clinical LLMs                        | Journal of Medical Internet Research               | Jonathan H. Chen           | 10.2196/76048               |             | [Link](https://www.semanticscholar.org/paper/8ba1a7337bd576ea0473c5aa0cb39fbf19310a0f)                 |
| 8     | Can large language models follow guidelines? A comparative study of ChatGPT-4o and DeepSeek AI in clavicle fracture management based on AAOS recommendations         | CPG adherence by ChatGPT-4o vs DeepSeek AI                     | BMC Medical Informatics and Decision Making        | Bekir Karagöz             | 10.1186/s12911-025-03202-5   |             | [Link](https://www.semanticscholar.org/paper/77ac819d1287c6bb41c1d988f5528a09e40ccce7)                |
| 9     | Medical AI Consensus: A Multi-Agent Framework for Radiology Report Generation and Evaluation                       | Multi-agent framework for radiology report gen                 | ArXiv                                              | Essam A. Rashed           | 10.48550/arXiv.2509.17353   |             | [Link](https://www.semanticscholar.org/paper/8cdb6320735984619991bab8b1f54502ecc96aca)                |

## Reviews

The Reviews section (papers [10–11]) examines overarching system-level and economic factors in AI deployments. Ashutosh Kumar Singh et al. [10] combine Delphi and Conjoint Analysis to identify and prioritize four critical sustainability factors in LLM supply chains: Environmental Impact, Computational Efficiency & Resource Optimization, Data Quality & Ethical Considerations, and Social Responsibility & Governance. Through four Delphi rounds with AI experts and subsequent Conjoint Analysis, they reveal that respondents undervalue environmental impact (low score), placing highest importance on Data Quality and Ethical considerations—a finding that underscores the need for broader environmental awareness and actionable governance policies.  

Jody Platt’s study [11] analyzes generative AI costs within large healthcare revenue cycles, comparing local vs commercial LLMs on medical free-text classification tasks. The work highlights that while foundation models like ChatGPT offer high accuracy, alternative models can match or exceed performance at lower computational costs. The paper quantifies cost-accuracy trade-offs and advocates for hybrid system portfolios to optimize healthcare expenditures and ensure reliability—pointing to the necessity of rigorous cost assessments before large-scale AI adoption in clinical operations.

| Index | Title                                                                                                                     | Domain                                         | Venue                     | Team                       | DOI                            | affiliation | paperUrl                                                                                          |
|-------|---------------------------------------------------------------------------------------------------------------------------|------------------------------------------------|---------------------------|----------------------------|--------------------------------|-------------|---------------------------------------------------------------------------------------------------|
| 10    | Sustainability in large language model supply chains-insights and recommendations using analysis of utility for affecting factors | Sustainability in LLM supply chains            | Scientific Reports       | Ashutosh Kumar Singh       | 10.1038/s41598-025-17937-8     |             | [Link](https://www.semanticscholar.org/paper/0d300f8d40b41d929137aef09e2eed654cb018cc)             |
| 11    | Generative AI costs in large healthcare systems, an example in revenue cycle                                              | Generative AI costs in healthcare systems      | NPJ Digital Medicine     | Jody Platt                 | 10.1038/s41746-025-01971-x     |             | [Link](https://www.semanticscholar.org/paper/301702a963ee3c7cd1614d38fe432a4e7c1c5c8d)             |

## Foundation Models

Foundation Models (papers [12–14]) present specialized large-scale architectures and hardware demonstrations. Jimeng Sun et al. [12] introduce LEADS, a literature-mining foundation model trained on 633,759 samples from systematic reviews, clinical trials, and registries. LEADS outperforms four state-of-the-art LLMs across six tasks (study search, screening, data extraction), achieving recall 0.81 vs 0.78 (without LEADS) and 26.9% time savings in data extraction. The model leverages high-quality domain data and human-in-the-loop evaluation across 16 clinicians, pointing toward improved expert productivity in evidence synthesis.

Yuanyuan Peng’s UA-VLFM [13] fuses contrastive vision-language pretraining with uncertainty estimation for diagnosing vitreoretinal lymphoma on OCT images. By integrating MAE pretraining knowledge, LoRA adaptation, and Dirichlet-based uncertainty-aware contrastive learning, UA-VLFM achieves an average F1 of 0.9684 (vs 0.8186–0.9427) and 0.9839 with thresholding on 5,563 test images; VRL class F1 improves from 0.9217 to 0.9544. The approach yields reliable predictions with uncertainty scores—critical for clinical deployment.

Geoffrey W. Burr et al. [14] demonstrate analog hardware inference of a transformer-based ALBERT model on a 14 nm phase-change memory chip. The 7.1M-weight, 12-layer model maps to 28.3M devices, achieving near-iso-accuracy on GLUE tasks (average 1.8% drop against floating-point). Hardware-aware fine-tuning yields a 4.4% accuracy boost, and a drift compensation recalibration reduces conductance drift loss from ~5% over 30 days to <1%. This first analog-chip demonstration of a substantial Transformer model heralds energy-efficient AI accelerators.

| Index | Title                                                                                                                   | Domain                                                        | Venue                                                | Team                       | DOI                            | affiliation                                                  | paperUrl                                                                                              |
|-------|-------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------|------------------------------------------------------|----------------------------|--------------------------------|--------------------------------------------------------------|-------------------------------------------------------------------------------------------------------|
| 12    | A foundation model for human-AI collaboration in medical literature mining                                              | Foundation model for literature mining (LEADS)                | Nature Communications                               | Jimeng Sun                | 10.1038/s41467-025-62058-5      |                                                              | [Link](https://www.semanticscholar.org/paper/369f76d5f2e4cd2028863acec5b4f15fecdc9f63)                 |
| 13    | UA-VLFM: An Uncertainty-aware Vision-Language Foundation Model for Auxiliary Diagnosis of Vitreoretinal Iymphoma.      | Uncertainty-aware vision-language model for VRL               | IEEE Journal of Biomedical and Health Informatics     | Yuanyuan Peng             | 10.1109/JBHI.2025.3611985        |                                                              | [Link](https://www.semanticscholar.org/paper/832128fa44868362e97bb29386c5c1425f428e78)                 |
| 14    | Demonstration of transformer-based ALBERT model on a 14nm analog AI inference chip                                      | Analog AI inference of transformer model                      | Nature Communications                               | Geoffrey W. Burr          | 10.1038/s41467-025-63794-4       |                                                              | [Link](https://www.semanticscholar.org/paper/2fd36ab5d27c28324c14047f0ee852e475af152f)                 |

## Databases

The Databases category (paper [15]) fills a critical gap for multimodal medical AI evaluation. Qingyu Chen et al. [15] introduce LMOD+, a comprehensive ophthalmology dataset and benchmark with 32,633 instances, multi-granular annotations across 12 conditions (e.g., diabetic retinopathy, glaucoma) and five imaging modalities (color fundus, OCT, etc.). Tasks include anatomical recognition, disease screening (binary), multi-class diagnosis, severity staging (international standards), and demographic attribute prediction for bias analysis. In systematic evaluations of 24 state-of-the-art multimodal LLMs, zero-shot disease screening peaked at ~58% accuracy, while staging remained suboptimal—highlighting challenges in understanding fine-grained pathology. By publicly releasing the dataset, curation pipeline, and leaderboard, LMOD+ aims to foster robust MLLM development, transparency, and global access to specialized ophthalmic benchmarks.

| Index | Title                                                                                                                               | Domain                                      | Venue | Team           | DOI                         | affiliation | paperUrl                                                                                               |
|-------|-------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------|-------|----------------|-----------------------------|-------------|--------------------------------------------------------------------------------------------------------|
| 15    | LMOD+: A Comprehensive Multimodal Dataset and Benchmark for Developing and Evaluating Multimodal Large Language Models in Ophthalmology | Ophthalmology multimodal dataset LMOD+      | ArXiv | Qingyu Chen    | 10.48550/arXiv.2509.25620   |             | [Link](https://www.semanticscholar.org/paper/04ee89fee63089a928a24f26caddc7b27c2bd22f)                 |

