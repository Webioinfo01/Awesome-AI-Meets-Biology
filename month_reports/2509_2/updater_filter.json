{
    "ai-agents": [
        {
            "year": "2025.09",
            "title": "An AI Agent for cell-type specific brain computer interfaces",
            "team": "Jia Liu",
            "team website": "",
            "affiliation": "John A. Paulson School of Engineering and Applied Sciences, Harvard University, Boston, MA, USA",
            "domain": "BCI AI agent for cell-type classification",
            "abstract": "Decoding how specific neuronal subtypes contribute to brain function requires linking extracellular electrophysiological features to underlying molecular identities, yet reliable in vivo electrophysiological signal classification remains a major challenge for neuroscience and clinical brain-computer interfaces (BCI). Here, we show that pretrained, general-purpose vision-language models (VLMs) can be repurposed as few-shot learners to classify neuronal cell types directly from electrophysiological features, without task-specific fine-tuning. Validated against optogenetically tagged datasets, this approach enables robust and generalizable subtype inference with minimal supervision. Building on this capability, we developed the BCI AI Agent (BCI-Agent), an autonomous AI framework that integrates vision-based cell-type inference, stable neuron tracking, and automated molecular atlas validation with real-time literature synthesis. BCI-Agent addresses three critical challenges for in vivo electrophysiology: (1) accurate, training-free cell-type classification; (2) automated cross-validation of predictions using molecular atlas references and peer-reviewed literature; and (3) embedding molecular identities within stable, low-dimensional neural manifolds for dynamic decoding. In rodent motor-learning tasks, BCI-Agent revealed stable, cell-type-specific neural trajectories across time that uncover previously inaccessible dimensions of neural computation. Additionally, when applied to human Neuropixels recordings\u2013where direct ground-truth labeling is inherently unavailable\u2013BCI-Agent inferred neuronal subtypes and validated them through integration with human single-cell atlases and literature. By enabling scalable, cell-type-specific inference of in vivo electrophysiology, BCI-Agent provides a new approach for dissecting the contributions of distinct neuronal populations to brain function and dysfunction.",
            "venue": "bioRxiv",
            "paperUrl": "https://www.semanticscholar.org/paper/d90383ebe5de7fe2ea516b899a1656d13656b2fc",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1101/2025.09.11.675660",
            "reason_for_inclusion": "High quality: Preprint from top-tier institution (Harvard University)."
        },
        {
            "year": "2025.09",
            "title": "Nano Bio-Agents (NBA): Small Language Model Agents for Genomics",
            "team": "Daniel Trejo-Ba\u00f1os",
            "team website": "",
            "affiliation": "",
            "domain": "Small language model agents for genomics (NBA)",
            "abstract": "We investigate the application of Small Language Models (<10 billion parameters) for genomics question answering via agentic framework to address hallucination issues and computational cost challenges. The Nano Bio-Agent (NBA) framework we implemented incorporates task decomposition, tool orchestration, and API access into well-established systems such as NCBI and AlphaGenome. Results show that SLMs combined with such agentic framework can achieve comparable and in many cases superior performance versus existing approaches utilising larger models, with our best model-agent combination achieving 98% accuracy on the GeneTuring benchmark. Notably, small 3-10B parameter models consistently achieve 85-97% accuracy while requiring much lower computational resources than conventional approaches. This demonstrates promising potential for efficiency gains, cost savings, and democratization of ML-powered genomics tools while retaining highly robust and accurate performance.",
            "venue": "ArXiv",
            "paperUrl": "https://www.semanticscholar.org/paper/b3dc8722e83573c7a6143e2b590b5791a4cf475e",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.48550/arXiv.2509.19566",
            "reason_for_inclusion": "Included to meet target filter limit."
        }
    ],
    "benchmarks": [
        {
            "year": "2025.09",
            "title": "Comparative Diagnostic Performance of a Multimodal Large Language Model Versus a Dedicated Electrocardiogram AI in Detecting Myocardial Infarction From Electrocardiogram Images: Comparative Study",
            "team": "Keehyuck Lee",
            "team website": "",
            "affiliation": "",
            "domain": "ECG image\u2013based MI diagnosis comparison",
            "abstract": "Abstract Background Accurate and timely electrocardiogram (ECG) interpretation is critical for diagnosing myocardial infarction (MI) in emergency settings. Recent advances in multimodal large language models (LLMs), such as ChatGPT (OpenAI) and Gemini (Google DeepMind), have shown promise in clinical interpretation for medical imaging. However, whether these models analyze waveform patterns or simply rely on text cues remains unclear, underscoring the need for direct comparisons with dedicated ECG artificial intelligence (AI) tools. Objective This study aimed to evaluate the diagnostic performance of ChatGPT and Gemini, a general-purpose LLM, in detecting MI from ECG images and to compare its performance with that of ECG Buddy (ARPI Inc), a dedicated AI-driven ECG analysis tool. Methods This retrospective study evaluated and compared AI models for classifying MI using a publicly available 12-lead ECG dataset from Pakistan, categorizing cases into MI-positive (239 images) and MI-negative (689 images). ChatGPT (GPT-4o, version November 20, 2024) and Gemini (Gemini 2.5 pro) were queried with 5 MI confidence options, whereas ECG Buddy for Microsoft Windows analyzed the images based on ST-elevation MI, acute coronary syndrome, and myocardial injury biomarkers. Results Among 928 ECG recordings (239/928, 25.8% MI-positive), ChatGPT achieved an accuracy of 65.95% (95% CI 62.80\u201069.00), area under the curve (AUC) of 57.34% (95% CI 53.44\u201061.24), sensitivity of 36.40% (95% CI 30.30\u201042.85), and specificity of 76.2% (95% CI 72.84\u201079.33). With Gemini 2.5 Pro, accuracy dropped to 29.63% (95% CI 26.71\u201032.69), AUC to 51.63% (95% CI 50.22\u201053.04), and sensitivity rose to 97.07% (95% CI 94.06\u201098.81), but specificity fell sharply to 6.24% (95% CI 4.55\u20108.31). However, ECG Buddy reached an accuracy of 96.98% (95% CI 95.67\u201097.99), AUC of 98.8% (95% CI 98.3\u201099.43), sensitivity of 96.65% (95% CI 93.51\u201098.54), and specificity of 97.10% (95% CI 95.55\u201098.22). DeLong test confirmed that ECG Buddy significantly outperformed ChatGPT (all P<.001). In a qualitative error analysis of LLMs\u2019 diagnostic explanations, GPT-4o produced fully accurate explanations in only 5% of cases (2/40), was partially accurate in 38% (15/40), and completely inaccurate in 58% (23/40). By contrast, Gemini 2.5 Pro yielded fully accurate explanations in 32% of cases (12/37), was partially accurate in 14% (5/37), and completely inaccurate in 54% (20/37). Conclusions LLMs, such as ChatGPT and Gemini, underperform relative to specialized tools such as ECG Buddy in ECG image\u2013based MI diagnosis. Further training may improve LLMs; however, domain-specific AI remains essential for clinical accuracy. The high performance of ECG Buddy underscores the importance of specialized models for achieving reliable and robust diagnostic outcomes.",
            "venue": "JMIR AI",
            "paperUrl": "https://www.semanticscholar.org/paper/9e8ad3169a37fe49b3bbf6bd6319aa6c47e6665f",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.2196/75910",
            "reason_for_inclusion": "High quality: Published in JMIR AI, a reputable journal."
        },
        {
            "year": "2025.09",
            "title": "Using Large Language Models to Assess the Consistency of Randomized Controlled Trials on AI Interventions With CONSORT-AI: Cross-Sectional Survey",
            "team": "Zhaoxiang Bian",
            "team website": "",
            "affiliation": "",
            "domain": "LLM consistency assessment of AI RCTs",
            "abstract": "Abstract Background Chatbots based on large language models (LLMs) have shown promise in evaluating the consistency of research. Previously, researchers used LLM to assess if randomized controlled trial (RCT) abstracts adhered to the CONSORT-Abstract guidelines. However, the consistency of artificial intelligence (AI) interventional RCTs aligning with the CONSORT-AI (Consolidated Standards of Reporting Trials-Artificial Intelligence) standards by LLMs remains unclear. Objective The aim of this study is to identify the consistency of RCTs on AI interventions with CONSORT-AI using chatbots based on LLMs. Methods This cross-sectional study employed 6 LLM models to assess the consistency of RCTs on AI interventions. The sample selection is based on articles published in JAMA Network Open, which included a total of 41 RCTs. All queries were submitted to LLMs through an application programming interface with a temperature setting of 0 to ensure deterministic responses. One researcher posed the questions to each model, while another independently verified the responses for validity before recording the results. The Overall Consistency Score (OCS), recall, inter-rater reliability, and consistency of contents were analyzed. Results We found gpt-4\u20100125-preview has the best average OCS on the basis of the results obtained by JAMA Network Open authors and by us (86.5%, 95% CI 82.5%\u201090.5% and 81.6%, 95% CI 77.6%\u201085.6%, respectively), followed by gpt-4\u20101106-preview (80.3%, 95% CI 76.3%\u201084.3% and 78.0%, 95% CI 74.0%\u201082.0%, respectively). The model with the worst average OCS is gpt-3.5-turbo-0125 on the basis of the results obtained by JAMA Network Open authors and by us (61.9%, 95% CI 57.9%\u201065.9% and 63.0%, 95% CI 59.0%\u201067.0%, respectively). Among the 11 unique items of CONSORT-AI, Item 2 (\u201cState the inclusion and exclusion criteria at the level of the input data\u201d) received the poorest overall evaluation across the 6 models, with an average OCS of 48.8%. For other items, those with an average OCS greater than 80% across the 6 models included Items 1, 5, 8, and 9. Conclusions GPT-4 variants demonstrate strong performance in assessing the consistency of RCTs with CONSORT-AI. Nonetheless, refining the prompts could enhance the precision and consistency of the outcomes. While AI tools like GPT-4 variants are valuable, they are not yet fully autonomous in addressing complex and nuanced tasks such as adherence to CONSORT-AI standards. Therefore, integrating AI with higher levels of human supervision and expertise will be crucial to ensuring more reliable and efficient evaluations, ultimately advancing the quality of medical research.",
            "venue": "Journal of Medical Internet Research",
            "paperUrl": "https://www.semanticscholar.org/paper/fa0d96aa69d0f420172727ffe5402530f1f57d33",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.2196/72412",
            "reason_for_inclusion": "High quality: Published in Journal of Medical Internet Research, a reputable journal."
        },
        {
            "year": "2025.09",
            "title": "Fine-Tuning Methods for Large Language Models in Clinical Medicine by Supervised Fine-Tuning and Direct Preference Optimization: Comparative Evaluation",
            "team": "Jonathan H. Chen",
            "team website": "",
            "affiliation": "",
            "domain": "SFT vs DPO fine-tuning in clinical LLMs",
            "abstract": "Abstract Background Large language model (LLM) fine-tuning is the process of adjusting out-of-the-box model weights using a dataset of interest. Fine-tuning can be a powerful technique to improve model performance in fields like medicine, where LLMs may have poor out-of-the-box performance. The 2 common fine-tuning techniques are supervised fine-tuning (SFT) and direct preference optimization (DPO); however, little guidance is available for when to apply either method within clinical medicine or health care operations. Objective This study aims to investigate the benefits of fine-tuning with SFT and DPO across a range of core natural language tasks in medicine to better inform clinical informaticists when either technique should be deployed. Methods We use Llama3 8B (Meta) and Mistral 7B v2 (Mistral AI) to compare the performance of SFT alone and DPO across 4 common natural language tasks in medicine. The tasks we evaluate include text classification, clinical reasoning, text summarization, and clinical triage. Results Our results found clinical reasoning accuracy increased from 7% to 22% with base Llama3 and Mistral2, respectively, to 28% and 33% with SFT, and then 36% and 40% with DPO (P=.003 and P=.004, respectively). Summarization quality, graded on a 5-point Likert scale, was 4.11 with base Llama3 and 3.93 with base Mistral2. Performance increased to 4.21 and 3.98 with SFT and then 4.34 and 4.08 with DPO (P<.001). F1-scores for provider triage were 0.55 for Llama3 and 0.49 for Mistral2, which increased to 0.58 and 0.52 with SFT and 0.74 and 0.66 with DPO (P<.001). F1-scores for urgency triage were 0.81 for Llama3 and 0.88 for Mistral2, which decreased with SFT to 0.79 and 0.87, and then experienced mixed results with DPO, achieving 0.91 and 0.85, respectively (P<.001 and P>.99, respectively). Finally, F1-scores for text classification were 0.63 for Llama3 and 0.73 for Mistral2, which increased to 0.98 and 0.97 with SFT, and then essentially did not change with DPO to 0.95 and 0.97, respectively (P=.55 and P>.99, respectively). DPO fine-tuning required approximately 2 to 3 times more compute resources than SFT alone. Conclusions SFT alone is sufficient for simple tasks such as rule-based text classification, while DPO after SFT improves performance on the more complex tasks of triage, clinical reasoning, and summarization. We postulate that SFT alone is sufficient for simple tasks because SFT strengthens simple word-association reasoning, whereas DPO enables deeper comprehension because it is trained with both positive and negative examples, enabling the model to recognize more complex patterns. Ultimately, our results help inform clinical informaticists when to deploy either fine-tuning method and encourage commercial LLM providers to offer DPO fine-tuning for commonly used proprietary LLMs in medicine.",
            "venue": "Journal of Medical Internet Research",
            "paperUrl": "https://www.semanticscholar.org/paper/8ba1a7337bd576ea0473c5aa0cb39fbf19310a0f",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.2196/76048",
            "reason_for_inclusion": "High quality: Published in Journal of Medical Internet Research, a reputable journal."
        },
        {
            "year": "2025.09",
            "title": "Can large language models follow guidelines? A comparative study of ChatGPT-4o and DeepSeek AI in clavicle fracture management based on AAOS recommendations",
            "team": "Bekir Karag\u00f6z",
            "team website": "",
            "affiliation": "",
            "domain": "CPG adherence by ChatGPT-4o vs DeepSeek AI",
            "abstract": "Artificial intelligence (AI)-based large language models (LLM) are increasingly used in healthcare education. However, the accuracy, readability, and reliability of their medical outputs remain a concern. This study aimed to compare the quality of responses generated by ChatGPT-4o and DeepSeek AI regarding the diagnosis and treatment of clavicle fractures, based on the 2022 AAOS Clinical Practice Guidelines (CPG). Fourteen clinical questions were formulated based on the AAOS CPG for clavicle fractures. Each question was independently submitted to ChatGPT-4o and DeepSeek AI. Responses were evaluated using standardized scoring tools, including DISCERN, PEMAT-P, CLEAR, Flesch-Kincaid Grade Level, Flesch Reading Ease, and Gunning-Fog Index. Two orthopedic surgeons independently rated the responses, and inter-rater scores were averaged. Statistical comparison between the two AI models was conducted using the Mann\u2013Whitney U test. DeepSeek AI generated responses with a significantly higher word count (median: 572, IQR: 258.25 vs. 438.5, IQR: 229; p\u2009=\u20090.016), CLEAR score (median: 18, IQR: 0.75 vs. 16, IQR: 0.75; p\u2009<\u20090.001). No statistically significant differences were found in PEMAT understandability (median: 77.7 vs. 77.7; p\u2009=\u20090.519), PEMAT actionability (median: 0 vs. 0; p\u2009=\u20091.000), or PEMAT total score (median: 57.2 vs. 58; p\u2009>\u20090.05). Similarly, no statistically significant differences were observed in DISCERN (52.1 vs. 51.6; p\u2009>\u20090.05), readability indices, binary accuracy (ChatGPT: 0.93, DeepSeek: 0.89; p\u2009>\u20090.05), or weighted accuracy (ChatGPT: 0.83, DeepSeek: 0.79; p\u2009>\u20090.05). Both models demonstrated generally high accuracy levels. Both ChatGPT-4o and DeepSeek AI generated coherent and clinically relevant responses to guideline-based questions on clavicle fracture management. However, neither model achieved meaningful scores in PEMAT actionability, and occasional inaccuracies and hallucinations were observed. While DeepSeek produced longer responses, verbosity did not correspond to superior quality. These findings suggest that LLMs may serve as supplementary tools for medical education and reference, but they cannot replace evidence-based clinical judgment, underscoring the need for supervised integration and ongoing validation. Finally, the number of prompts analyzed (14) was limited, reflecting the scope of a single guideline; this sample size restricts statistical power and generalizability, and larger multi-guideline datasets will be needed in future studies.",
            "venue": "BMC Medical Informatics and Decision Making",
            "paperUrl": "https://www.semanticscholar.org/paper/77ac819d1287c6bb41c1d988f5528a09e40ccce7",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1186/s12911-025-03202-5",
            "reason_for_inclusion": "High quality: Published in BMC Medical Informatics and Decision Making, a reputable journal."
        },
        {
            "year": "2025.09",
            "title": "Medical AI Consensus: A Multi-Agent Framework for Radiology Report Generation and Evaluation",
            "team": "Essam A. Rashed",
            "team website": "",
            "affiliation": "",
            "domain": "Multi-agent framework for radiology report gen",
            "abstract": "Automating radiology report generation poses a dual challenge: building clinically reliable systems and designing rigorous evaluation protocols. We introduce a multi-agent reinforcement learning framework that serves as both a benchmark and evaluation environment for multimodal clinical reasoning in the radiology ecosystem. The proposed framework integrates large language models (LLMs) and large vision models (LVMs) within a modular architecture composed of ten specialized agents responsible for image analysis, feature extraction, report generation, review, and evaluation. This design enables fine-grained assessment at both the agent level (e.g., detection and segmentation accuracy) and the consensus level (e.g., report quality and clinical relevance). We demonstrate an implementation using chatGPT-4o on public radiology datasets, where LLMs act as evaluators alongside medical radiologist feedback. By aligning evaluation protocols with the LLM development lifecycle, including pretraining, finetuning, alignment, and deployment, the proposed benchmark establishes a path toward trustworthy deviance-based radiology report generation.",
            "venue": "ArXiv",
            "paperUrl": "https://www.semanticscholar.org/paper/8cdb6320735984619991bab8b1f54502ecc96aca",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.48550/arXiv.2509.17353",
            "reason_for_inclusion": "Included to meet target filter limit."
        }
    ],
    "reviews": [
        {
            "year": "2025.09",
            "title": "Sustainability in large language model supply chains-insights and recommendations using analysis of utility for affecting factors",
            "team": "Ashutosh Kumar Singh",
            "team website": "",
            "affiliation": "",
            "domain": "Sustainability in LLM supply chains",
            "abstract": "The increasing adoption of Large Language Models (LLMs) has intensified concerns regarding the sustainability of their supply chains, particularly concerning energy consumption, resource utilization, and carbon emissions. To address these concerns, this study proposes a two-step approach. First, a Delphi method is employed to systematically identify the critical factors affecting the sustainability of LLM supply chains. Expert consensus through four rounds of feedback highlights key factors such as Environmental Impact, Computational Efficiency & Resource Optimization, Data Quality & Ethical Considerations, and Social Responsibility & Governance. In the second step, the identified factor\u2019s relative importance was calculated using Conjoint Analysis, a statistical technique used to determine how respondents value different factors of a supply chain of LLMs. This prioritization helps formulate strategies to make LLM\u2019s supply chain sustainable. The low score for environmental impact suggests a lack of awareness about the sustainability of LLMs\u2019 supply chain. The study finds Data Quality and Ethical considerations to be the most important considerations for the respondents. Thus, it provides a framework for implementing sustainable practices in LLMs\u2019 supply chains in resource-constrained settings. The results demonstrate the effectiveness of this combined Delphi-Conjoint Analysis approach, providing actionable insights for AI organizations aiming to enhance the sustainability of their LLM operations.",
            "venue": "Scientific Reports",
            "paperUrl": "https://www.semanticscholar.org/paper/0d300f8d40b41d929137aef09e2eed654cb018cc",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1038/s41598-025-17937-8",
            "reason_for_inclusion": "High quality: Published in Scientific Reports, a reputable journal."
        },
        {
            "year": "2025.09",
            "title": "Generative AI costs in large healthcare systems, an example in revenue cycle",
            "team": "Jody Platt",
            "team website": "",
            "affiliation": "",
            "domain": "Generative AI costs in healthcare systems",
            "abstract": "Application of large language models in healthcare continues to expand, specifically for medical free-text classification tasks. While foundation models like those from ChatGPT show potential, alternative models demonstrate superior accuracy and lower costs. This study underscores significant challenges, including computational costs and model reliability. Amidst rising healthcare expenditures and AI\u2019s perceived potential to reduce costs, a combination of local and commercial models might offer balanced solutions for healthcare systems.",
            "venue": "NPJ Digital Medicine",
            "paperUrl": "https://www.semanticscholar.org/paper/301702a963ee3c7cd1614d38fe432a4e7c1c5c8d",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1038/s41746-025-01971-x",
            "reason_for_inclusion": "High quality: Published in NPJ Digital Medicine, a top-tier journal."
        }
    ],
    "foundation-models": [
        {
            "year": "2025.09",
            "title": "A foundation model for human-AI collaboration in medical literature mining",
            "team": "Jimeng Sun",
            "team website": "",
            "affiliation": "",
            "domain": "Foundation model for literature mining (LEADS)",
            "abstract": "Applying artificial intelligence (AI) for systematic literature review holds great potential for enhancing evidence-based medicine, yet has been limited by insufficient training and evaluation. Here, we present LEADS, an AI foundation model trained on 633,759 samples curated from 21,335 systematic reviews, 453,625 clinical trial publications, and 27,015 clinical trial registries. In experiments, LEADS demonstrates consistent improvements over four cutting-edge large language models (LLMs) on six literature mining tasks, e.g., study search, screening, and data extraction. We conduct a user study with 16 clinicians and researchers from 14 institutions to assess the utility of LEADS integrated into the expert workflow. In study selection, experts using LEADS achieve 0.81 recall vs. 0.78 without, saving 20.8% time. For data extraction, accuracy reached 0.85 vs. 0.80, with 26.9% time savings. These findings encourage future work on leveraging high-quality domain data to build specialized LLMs that outperform generic models and enhance expert productivity in literature mining. Literature mining, such as systematic review and meta-analysis, is crucial for discovering, integrating, and interpreting emerging research. This study presents a specialized large language model for literature that outperforms six general LLMs and helps clinicians in study selection and data extraction tasks.",
            "venue": "Nature Communications",
            "paperUrl": "https://www.semanticscholar.org/paper/369f76d5f2e4cd2028863acec5b4f15fecdc9f63",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1038/s41467-025-62058-5",
            "reason_for_inclusion": "High quality: Published in Nature Communications, a top-tier journal."
        },
        {
            "year": "2025.09",
            "title": "UA-VLFM: An Uncertainty-aware Vision-Language Foundation Model for Auxiliary Diagnosis of Vitreoretinal Iymphoma.",
            "team": "Yuanyuan Peng",
            "team website": "",
            "affiliation": "",
            "domain": "Uncertainty\u2010aware vision\u2010language model for VRL",
            "abstract": "Vitreoretinal lymphoma (VRL) is a rare malignant ocular tumor, and its early diagnosis is crucial for patient prognosis. However, due to its insidious and diverse clinical manifestations, it is often misdiagnosed as other ophthalmic diseases, leading to blindness or even fatal outcomes. In this study, an uncertainty-aware visionlanguage foundational model (UA-VLFM) based on contrastive learning and uncertainty estimation is developed to achieve automatic classification of VRL and other 5 retinal diseases. First, we integrate MAE-based pretraining knowledge on large-scale optical coherence tomography (OCT) images and efficient Low-rank adaption (LoRA) optimization strategy to enhance the representation ability and optimization efficiency of the model. Moreover, an uncertainty-aware contrastive learning method based on Dirichlet distribution within the contrastive vision-language pretraining framework is proposed to further align vision and language feature in the high-dimensional embedding space and obtain prediction results with corresponding uncertainty scores, thereby enhancing the reliability of VRL diagnosis. In the test dataset with 5,563 OCT images, UA-VLFM achieves a higher average F1 score of 0.9684 than other state-of-the-art algorithms (0.8186-0.9427) and improves to 0.9839 with the threshold strategy. Notably, the proposed UA-VLFM achieves an F1 score of 0.9217 and 0.9544 before and after thresholding on VRL, the most challenging category, significantly outperforming other methods (0.5089-0.9366 and 0.6639-0.9133). Our UA-VLFM provides a trustworthy method for aiding in the diagnosis of VRL on retinal OCT images. The code has been released on Github: https://github.com/wang-wen-wen/UA-VLFM.",
            "venue": "IEEE journal of biomedical and health informatics",
            "paperUrl": "https://www.semanticscholar.org/paper/832128fa44868362e97bb29386c5c1425f428e78",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1109/JBHI.2025.3611985",
            "reason_for_inclusion": "High quality: Published in IEEE Journal of Biomedical and Health Informatics, a top-tier journal."
        },
        {
            "year": "2025.09",
            "title": "Demonstration of transformer-based ALBERT model on a 14nm analog AI inference chip",
            "team": "Geoffrey W. Burr",
            "team website": "",
            "affiliation": "",
            "domain": "Analog AI inference of transformer model",
            "abstract": "ALite Bidirectional Encoder Representations from Transformers model is demonstrated on an analog inference chip fabricated at 14nm node with phase change memory. The 7.1 million unique analog weights shared across 12 layers are mapped to a single chip, accurately programmed into the conductance of 28.3 million devices, for this first analog hardware demonstration of a meaningfully large Transformer model. The implemented model achieved near iso-accuracy on the General Language Understanding Evaluation benchmark of seven tasks, despite the presence of weight-programming errors, hardware imperfections, readout noise, and error propagation. The average hardware accuracy was only 1.8% below that of the floating-point reference, with several tasks at full iso-accuracy. Careful fine-tuning of model weights using hardware-aware techniques contributes an average hardware accuracy improvement of 4.4%. Accuracy loss due to conductance drift \u2013 measured to be roughly 5% over 30 days \u2013 was reduced to less than 1% with a recalibration-based \u201cdrift compensation\u201d technique. The authors report the implementation of a Transformer-based model on the same architecture used in Large Language Models in a 14nm analog AI accelerator with 35 million Phase Change Memory devices, which achieves near iso-accuracy despite hardware imperfections and noise.",
            "venue": "Nature Communications",
            "paperUrl": "https://www.semanticscholar.org/paper/2fd36ab5d27c28324c14047f0ee852e475af152f",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1038/s41467-025-63794-4",
            "reason_for_inclusion": "High quality: Published in Nature Communications, a top-tier journal."
        }
    ],
    "databases": [
        {
            "year": "2025.09",
            "title": "LMOD+: A Comprehensive Multimodal Dataset and Benchmark for Developing and Evaluating Multimodal Large Language Models in Ophthalmology",
            "team": "Qingyu Chen",
            "team website": "",
            "affiliation": "",
            "domain": "Ophthalmology multimodal dataset LMOD+",
            "abstract": "Vision-threatening eye diseases pose a major global health burden, with timely diagnosis limited by workforce shortages and restricted access to specialized care. While multimodal large language models (MLLMs) show promise for medical image interpretation, advancing MLLMs for ophthalmology is hindered by the lack of comprehensive benchmark datasets suitable for evaluating generative models. We present a large-scale multimodal ophthalmology benchmark comprising 32,633 instances with multi-granular annotations across 12 common ophthalmic conditions and 5 imaging modalities. The dataset integrates imaging, anatomical structures, demographics, and free-text annotations, supporting anatomical structure recognition, disease screening, disease staging, and demographic prediction for bias evaluation. This work extends our preliminary LMOD benchmark with three major enhancements: (1) nearly 50% dataset expansion with substantial enlargement of color fundus photography; (2) broadened task coverage including binary disease diagnosis, multi-class diagnosis, severity classification with international grading standards, and demographic prediction; and (3) systematic evaluation of 24 state-of-the-art MLLMs. Our evaluations reveal both promise and limitations. Top-performing models achieved ~58% accuracy in disease screening under zero-shot settings, and performance remained suboptimal for challenging tasks like disease staging. We will publicly release the dataset, curation pipeline, and leaderboard to potentially advance ophthalmic AI applications and reduce the global burden of vision-threatening diseases.",
            "venue": "ArXiv",
            "paperUrl": "https://www.semanticscholar.org/paper/04ee89fee63089a928a24f26caddc7b27c2bd22f",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.48550/arXiv.2509.25620",
            "reason_for_inclusion": "Included to meet target filter limit."
        }
    ]
}
