{
    "databases": [
        {
            "year": "2025.08",
            "title": "Large language model powered knowledge graph construction for mental health exploration",
            "team": "Hongtu Zhu",
            "team website": "",
            "affiliation": "",
            "domain": "Knowledge graph construction for mental health",
            "abstract": "Mental health is a major global concern, yet findings remain fragmented across studies and databases, hindering integrative understanding and clinical translation. To address this gap, we present the Mental Disorders Knowledge Graph (MDKG)\u2014a large-scale, contextualized knowledge graph built using large language models to unify evidence from biomedical literature and curated databases. MDKG comprises over 10 million relations, including nearly 1 million novel associations absent from existing resources. By structurally encoding contextual features such as conditionality, demographic factors, and co-occurring clinical attributes, the graph enables more nuanced interpretation and rapid expert validation, reducing evaluation time by up to 70%. Applied to predictive modeling in the UK Biobank, MDKG-enhanced representations yielded significant gains in predictive performance across multiple mental disorders. As a scalable and semantically enriched resource, MDKG offers a powerful foundation for accelerating psychiatric research and enabling interpretable, data-driven clinical insights. Understanding the pathophysiological pathways of mental disorders and identifying reliable biomarkers remain challenging. This study introduces a large-scale knowledge graph tailored to mental disorders to improve knowledge discovery, disease prediction, and clinical validation",
            "venue": "Nature Communications",
            "paperUrl": "https://www.semanticscholar.org/paper/cf72cf90a34bbb8715ca2626d4ac6e901afb3457",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1038/s41467-025-62781-z",
            "reason_for_inclusion": "High quality: Published in Nature Communications, a top-tier journal."
        },
        {
            "year": "2025.08",
            "title": "Finding the dark matter: Large language model\u2010based enzyme kinetic data extractor and its validation",
            "team": "Zhongyue J. Yang",
            "team website": "",
            "affiliation": "",
            "domain": "LLM-based enzyme kinetic data extraction",
            "abstract": "Despite the vast number of enzymatic kinetic measurements reported across decades of biochemical literature, the majority of relational enzyme kinetic data\u2014linking amino acid sequence, substrate identity, kinetic parameters, and assay conditions\u2014remains uncollected and inaccessible in structured form. This constitutes a significant portion of the \u201cdark matter\u201d of enzymology. Unlocking these hidden data through automated extraction offers an opportunity to expand enzyme dataset diversity and size, critical for building accurate, generalizable models that drive predictive enzyme engineering. To address this limitation, we built EnzyExtract, a large language model\u2010powered pipeline that automates the extraction, verification, and structuring of enzyme kinetics data from scientific literature. By processing 137,892 full\u2010text publications (PDF/XML), EnzyExtract collected more than 218,095 enzyme\u2013substrate\u2013kinetics entries, including 218,095 kcat and 167,794 Km values. These entries are mapped to enzymes spanning 3569 unique four\u2010digit EC numbers, with a total of 84,464 entries assigned at least a first\u2010digit EC number. EnzyExtract identified 89,544 unique kinetic entries (kcat and Km combined) absent from BRENDA, significantly expanding the known enzymology dataset. The newly curated dataset was compiled into a database named EnzyExtractDB. EnzyExtract demonstrates high accuracy when benchmarked against manually curated datasets and strong consistency with BRENDA\u2010derived data. To create model\u2010ready datasets, enzyme and substrate sequences were aligned to UniProt and PubChem, yielding 92,286 high\u2010confidence, sequence\u2010mapped kinetic entries. To assess the practical utility of our dataset, we retrained several state\u2010of\u2010the\u2010art kcat predictors (including MESI, DLKcat, and TurNuP) using EnzyExtractDB. Across held\u2010out test sets, all models demonstrate improved predictive performance in terms of RMSE, MAE, and R2, highlighting the value of high\u2010quality, large\u2010scale, literature\u2010derived EnzyExtractDB for enhancing predictive modeling of enzyme kinetics. The EnzyExtract source code and the database are openly available at https://github.com/ChemBioHTP/EnzyExtract, and an interactive demo can be accessed via Google Colab at https://colab.research.google.com/drive/1MwKSEZzLPNOseksRshbzkkFoO_cgJhva.",
            "venue": "Protein Science : A Publication of the Protein Society",
            "paperUrl": "https://www.semanticscholar.org/paper/839511d2d7a43d41be05e10fe8448b0ca37112a2",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1002/pro.70251",
            "reason_for_inclusion": "Included to meet target filter limit."
        },
        {
            "year": "2025.08",
            "title": "miRKatAI: An Integrated Database and Multi-agent AI system for microRNA Research",
            "team": "Katarzyna Goljanek-Whysall",
            "team website": "",
            "affiliation": "",
            "domain": "Database and AI system for microRNA research",
            "abstract": "MicroRNAs (miRs) are robust regulators of gene expression, implicated in most biological processes. microRNAs predominantly downregulate the expression of genes post-transcriptionally and each miR is predicted to target several hundred genes. The accurate identification and annotation of miR-mRNA target interactions is central to understanding miRs function and their therapeutic potential. However, computational target prediction is challenging due to imperfect complementarity of miRs with their targets and the growing volume and heterogeneity of experimental data present challenges in accessing, integrating, and analysing miR-target interaction information across biological contexts. This creates a need for integrated resources and intelligent query tools. We present the miRKat Suite, comprising miRKatDB, a comprehensive, curated database of predicted and validated miR-target interactions and associated annotations, and miRKatAI, a multi-agent system powered by large language models (LLMs) and LangGraph. miRKatDB integrates data from multiple publicly available sources, providing a comprehensive foundation for miR studies, including miR target genes and changes in levels of tissue expression previously reported. miRKatAI offers a natural language interface for complex querying of miRKatDB, facilitates grounded information retrieval from established sources in the field, and supports basic data visualisation. The miRKat Suite aims to accelerate miR research by streamlining data access, enhancing exploratory analysis, and supporting hypothesis generation.",
            "venue": "ArXiv",
            "paperUrl": "https://www.semanticscholar.org/paper/eecf17eee7339225389f95c092fbf8fbda0f9779",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.48550/arXiv.2508.08331",
            "reason_for_inclusion": "Included to meet target filter limit."
        }
    ],
    "ai-agents": [
        {
            "year": "2025.08",
            "title": "Reverse Physician-AI Relationship: Full-process Clinical Diagnosis Driven by a Large Language Model",
            "team": "Xueqi Cheng",
            "team website": "",
            "affiliation": "",
            "domain": "LLM-driven full-process clinical diagnosis",
            "abstract": "Full-process clinical diagnosis in the real world encompasses the entire diagnostic workflow that begins with only an ambiguous chief complaint. While artificial intelligence (AI), particularly large language models (LLMs), is transforming clinical diagnosis, its role remains largely as an assistant to physicians. This AI-assisted working pattern makes AI can only answer specific medical questions at certain parts within the diagnostic process, but lack the ability to drive the entire diagnostic process starting from an ambiguous complaint, which still relies heavily on human physicians. This gap limits AI's ability to fully reduce physicians'workload and enhance diagnostic efficiency. To address this, we propose a paradigm shift that reverses the relationship between physicians and AI: repositioning AI as the primary director, with physicians serving as its assistants. So we present DxDirector-7B, an LLM endowed with advanced deep thinking capabilities, enabling it to drive the full-process diagnosis with minimal physician involvement. Furthermore, DxDirector-7B establishes a robust accountability framework for misdiagnoses, delineating responsibility between AI and human physicians. In evaluations across rare, complex, and real-world cases under full-process diagnosis setting, DxDirector-7B not only achieves significant superior diagnostic accuracy but also substantially reduces physician workload than state-of-the-art medical LLMs as well as general-purpose LLMs. Fine-grained analyses across multiple clinical departments and tasks validate its efficacy, with expert evaluations indicating its potential to serve as a viable substitute for medical specialists. These findings mark a new era where AI, traditionally a physicians'assistant, now drives the entire diagnostic process to drastically reduce physicians'workload, indicating an efficient and accurate diagnostic solution.",
            "venue": "ArXiv",
            "paperUrl": "https://www.semanticscholar.org/paper/d67afa3cb98b6e5a15966ce06f1aff61cf2ad270",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.48550/arXiv.2508.10492",
            "reason_for_inclusion": "Included to meet target filter limit."
        },
        {
            "year": "2025.08",
            "title": "ChatRadio-Valuer: A Chat Large Language Model for Generalizable Radiology Impression Generation on Multi-institution and Multi-system Data.",
            "team": "Tuo Zhang",
            "team website": "",
            "affiliation": "",
            "domain": "LLM for radiology impression generation",
            "abstract": "",
            "venue": "IEEE transactions on bio-medical engineering",
            "paperUrl": "https://www.semanticscholar.org/paper/ecab6c8b4052a09127340a7fcfcc0b1550435b63",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1109/TBME.2025.3597325",
            "reason_for_inclusion": "Included to meet target filter limit."
        },
        {
            "year": "2025.08",
            "title": "A large language model digital patient system enhances ophthalmology history taking skills",
            "team": "Haotian Lin",
            "team website": "",
            "affiliation": "",
            "domain": "LLM-based digital patient for ophthalmology training",
            "abstract": "Clinical trainees face limited opportunities to practice medical history-taking skills due to scarce case diversity and access to real patients. To address this, we developed a large language model-based digital patient (LLMDP) system that transforms de\u2011identified electronic health records into voice\u2011enabled virtual patients capable of free\u2011text dialog and adaptive feedback, based on our previously established open-source retrieval-augmented framework. In a single\u2011center randomized controlled trial (ClinicalTrials.gov: NCT06229379; N\u2009=\u200984), students trained with LLMDP achieved a 10.50-point increase in medical history-taking assessment scores (95% CI: 4.66\u201316.33, p\u2009<\u20090.001) compared to those using traditional methods. LLMDP-trained students also demonstrated greater empathy. Participants reported high satisfaction with LLMDP, emphasizing its role in reducing training costs and boosting confidence for real patient interactions. These findings provide evidence that LLM-driven digital patients enhance medical history-taking skills and offer a scalable, low-risk pathway for integrating generative AI into ophthalmology education.",
            "venue": "NPJ Digital Medicine",
            "paperUrl": "https://www.semanticscholar.org/paper/0688a4ed8971d339ae2f294a299ce0e9b836fb62",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1038/s41746-025-01841-6",
            "reason_for_inclusion": "High quality: Published in NPJ Digital Medicine, a top-tier journal."
        },
        {
            "year": "2025.08",
            "title": "Evaluating the Capability of Large Language Model Chatbots for Generating Plain Language Summaries in Radiology",
            "team": "Swaha Panda",
            "team website": "",
            "affiliation": "",
            "domain": "LLM-generated plain language summaries in radiology",
            "abstract": "Plain language summary (PLS) are essential for making scientific research accessible to a broader audience. With the increasing capabilities of large language models (LLMs), there is the potential to automate the generation of PLS from complex scientific abstracts. This study assessed the performance of six LLM chatbots: ChatGPT, Claude, Copilot, Gemini, Meta AI, and Perplexity, in generating PLS from radiology research abstracts.",
            "venue": "iRADIOLOGY",
            "paperUrl": "https://www.semanticscholar.org/paper/d1cb625c241b63052cea1da4fbd6b25d5f798790",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1002/ird3.70030",
            "reason_for_inclusion": "Included to meet target filter limit."
        },
        {
            "year": "2025.08",
            "title": "Integrating clinical reasoning into large language model-based diagnosis through etiology-aware attention steering",
            "team": "Jingsong Li",
            "team website": "",
            "affiliation": "",
            "domain": "Etiology-aware attention steering in LLM diagnosis",
            "abstract": "Objective: Large Language Models (LLMs) demonstrate significant capabilities in medical text understanding and generation. However, their diagnostic reliability in complex clinical scenarios remains limited. This study aims to enhance LLMs'diagnostic accuracy and clinical reasoning ability. Method: We propose an Etiology-Aware Attention Steering Framework to integrate structured clinical reasoning into LLM-based diagnosis. Specifically, we first construct Clinical Reasoning Scaffolding (CRS) based on authoritative clinical guidelines for three representative acute abdominal emergencies: acute appendicitis, acute pancreatitis, and acute cholecystitis. Next, we develop the Etiology-Aware Head Identification algorithm to pinpoint attention heads crucial for the model's etiology reasoning. To ensure reliable clinical reasoning alignment, we introduce the Reasoning-Guided Parameter-Efficient Fine-tuning that embeds etiological reasoning cues into input representations and steers the selected Etiology-Aware Heads toward critical information through a Reasoning-Guided Loss function. Result: On the Consistent Diagnosis Cohort, our framework improves average diagnostic accuracy by 15.65% and boosts the average Reasoning Focus Score by 31.6% over baselines. External validation on the Discrepant Diagnosis Cohort further confirms its effectiveness in enhancing diagnostic accuracy. Further assessments via Reasoning Attention Frequency indicate that our models exhibit enhanced reliability when faced with real-world complex scenarios. Conclusion: This study presents a practical and effective approach to enhance clinical reasoning in LLM-based diagnosis. By aligning model attention with structured CRS, the proposed framework offers a promising paradigm for building more interpretable and reliable AI diagnostic systems in complex clinical settings.",
            "venue": "ArXiv",
            "paperUrl": "https://www.semanticscholar.org/paper/fb69b0591a8acf0d3f36b47d3df36b1ef481a394",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.48550/arXiv.2508.00285",
            "reason_for_inclusion": "Included to meet target filter limit."
        },
        {
            "year": "2025.08",
            "title": "Impact of an Evidence-Based Large Language Model (LLM) Diagnostic Decision Support System: A Randomised Controlled Trial.",
            "team": "Junsang Yoo",
            "team website": "",
            "affiliation": "",
            "domain": "Evidence-based LLM diagnostic decision support",
            "abstract": "",
            "venue": "Studies in health technology and informatics",
            "paperUrl": "https://www.semanticscholar.org/paper/b1f35310cb1190f264ffdb07c7ec2fee0335cce4",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.3233/SHTI250844",
            "reason_for_inclusion": "Included to meet target filter limit."
        },
        {
            "year": "2025.08",
            "title": "Leveraging large language models for the deidentification and temporal normalization of sensitive health information in electronic health records",
            "team": "Jitendra Jonnagaddala",
            "team website": "",
            "affiliation": "",
            "domain": "LLM-based deidentification and temporal normalization in EHR",
            "abstract": "Secondary use of electronic health record notes enhances clinical outcomes and personalized medicine, but risks sensitive health information (SHI) exposure. Inconsistent time formats hinder interpretation, necessitating deidentification and temporal normalization. The SREDH/AI CUP 2023 competition explored large language models (LLMs) for these tasks using 3,244 pathology reports with surrogated SHIs and normalized dates. The competition drew 291 teams; the top teams achieved macro-F1 scores >0.8. Results were presented at the IW-DMRN workshop in 2024. Notably, 77.2% used LLMs, highlighting their growing role in healthcare. This study compares competition results with in-context learning and fine-tuned LLMs. Findings show that fine-tuning, especially with lower-rank adaptation, boosts performance but plateaus or degrades in models over 6\u2009B parameters due to overfitting. Our findings highlight the value of data augmentation, training strategies, and hybrid approaches. Effective LLM-based deidentification requires balancing performance with legal and ethical demands, ensuring privacy and interpretability in regulated healthcare settings.",
            "venue": "NPJ Digital Medicine",
            "paperUrl": "https://www.semanticscholar.org/paper/5bd7da9ef638f309da5490cc7a2a4dac8fa9c07c",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1038/s41746-025-01921-7",
            "reason_for_inclusion": "High quality: Published in NPJ Digital Medicine, a top-tier journal."
        }
    ],
    "benchmarks": [
        {
            "year": "2025.08",
            "title": "Leveraging image-based AI for dietary assessment: evaluating a large language model with real-world meal photos from the ZOE PREDICT cohorts",
            "team": "S. Berry",
            "team website": "",
            "affiliation": "",
            "domain": "AI-powered dietary assessment from meal images",
            "abstract": "Traditional dietary assessment methods, such as food diaries, are limited by high participant burden and misreporting(1). Digital tools, such as wearable cameras, offer an alternative reducing reliance on self-logging, but concerns remain around accuracy, researcher burden, and privacy(2-3). AI-powered photologging, using vision-based systems for food identification and nutrient analysis via smartphone meal capture, provides a low-burden solution that goes beyond tracking intake. By analysing meal composition and variety, it enables assessment of diet quality and diversity, shifting the focus beyond nutrient quantity to a comprehensive understanding of dietary patterns. The aim of this study was to evaluate the accuracy of an AI-powered photologging system in identifying food groups and estimating nutrient content from meal photographs, compared to traditional weighed food logs. A total of 10,000 meals from 2,124 individuals were analysed from the PREDICT 1 (NCT03479866) and PREDICT 2 (NCT03983733) cohorts (UK and US). Meal photographs were processed using large language model (LLM)-based image analysis and compared against weighed food logs. For ingredient retrieval, we compared food groups (n=77) of retrieved and suggested ingredients. Model performance was evaluated using precision (correctly retrieved groups), recall (proportion of true groups retrieved), and F1 score (harmonic mean of precision and recall). Metrics are scored on a scale (0 to 1), with higher values indicating better performance. Accuracy of nutrient estimates were compared using the Wilcoxon Signed-Rank Test. Top performing food groups (excluding alcohol) by F1 score (95% CI) included \u201ctea and coffee\u201d (0.88 [0.86, 0.90]), \u201cvegetables\u201d (0.87 [0.85, 0.89]), and \u201cbaked beans\u201d (0.84 [0.61, 1.00]). Lower performance was seen for \u201cplant milk\u201d (0.13 [0.07, 0.18]), \u201csoup\u201d (0.07 [0.00, 0.20]), and \u201chot chocolate\u201d (0.07 [0.00, 0.19]). In the UK and US cohorts respectively, precision by meal (95% CI) was 0.72 (0.74, 0.73), 0.65 (0.64, 0.66); recall was 0.75 (0.74, 0.76), 0.73 (0.72, 0.74); and F1 score was 0.71 (0.71, 0.75), 0.66 (0.65, 0.67). Small differences (95% CI, effect size r = 0.047 to 0.172) were observed for all macronutrients per meal: calories +139 kcal (95% CI: 134, 144), protein +6.11g (5.83, 6.39), carbohydrates +15.6g (14.9, 16.3), fat +7.89g (7.48, 8.33), fibre +2.30g (2.19, 2.42), sodium +0.23g (0.22, 0.25), and sugar +6.83g (6.49, 7.17). Preliminary results demonstrate that smartphone-based photologging with LLMs can accurately identify ingredients and estimate nutrients. This provides a valuable solution to dietary assessment, in a real-world setting, in the era of smartphone apps. A key limitation is the sole use of a proprietary dataset and internal food groups; future work will benchmark performance on public datasets for broader validation.",
            "venue": "Proceedings of the Nutrition Society",
            "paperUrl": "https://www.semanticscholar.org/paper/45ca1748e55b6a73096c33cce5d18d9c3781241a",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1017/S0029665125101572",
            "reason_for_inclusion": "Included to meet target filter limit."
        },
        {
            "year": "2025.08",
            "title": "Can a Large Language Model Judge a Child's Statement?: A Comparative Analysis of ChatGPT and Human Experts in Credibility Assessment.",
            "team": "Zeki Karata\u015f",
            "team website": "",
            "affiliation": "",
            "domain": "LLM reliability in credibility assessment of child statements",
            "abstract": "PURPOSE\nThis study investigates the inter-rater reliability between human experts (a forensic psychologist and a social worker) and a large language model (LLM) in the assessment of child sexual abuse statements. The research aims to explore the potential, limitations, and consistency of this class of AI as an evaluation tool within the framework of Criteria-Based Content Analysis (CBCA), a widely used method for assessing statement credibility.\n\n\nMATERIALS AND METHODS\nSixty-five anonymized transcripts of forensic interviews with child sexual abuse victims (N\u2009=\u200965) were independently evaluated by three raters: a forensic psychologist, a social worker, and a large language model (ChatGPT, GPT-4o Plus). Each statement was coded using the 19-item CBCA framework. Inter-rater reliability was analyzed using Intraclass Correlation Coefficient (ICC), Cohen's Kappa (\u03ba), and other agreement statistics to compare the judgments between the human-human pairing and the human-AI pairings.\n\n\nRESULTS\nA high degree of inter-rater reliability was found between the two human experts, with the majority of criteria showing \"good\" to \"excellent\" agreement (15 of 19 criteria with ICC\u2009>\u2009.75). In stark contrast, a dramatic and significant decrease in reliability was observed when the AI model's evaluations were compared with those of the human experts. The AI demonstrated systematic disagreement on criteria requiring nuanced, contextual judgment, with reliability coefficients frequently falling into \"poor\" or negative ranges (e.g. ICC\u2009=\u2009-.106 for \"Logical structure\"), indicating its evaluation logic fundamentally differs from expert reasoning.\n\n\nDISCUSSION\nThe findings reveal a profound gap between the nuanced, contextual reasoning of human experts and the pattern-recognition capabilities of the LLM tested. The study concludes that this type of AI, in its current, prompt-engineered form, cannot reliably replicate expert judgment in the complex task of credibility assessment. While not a viable autonomous evaluator, it may hold potential as a \"cognitive assistant\" to support expert workflows. The assessment of child testimony credibility remains a task that deeply requires professional judgment and appears far beyond the current capabilities of such generative AI models.",
            "venue": "Journal of evidence-based social work",
            "paperUrl": "https://www.semanticscholar.org/paper/3deaeaf3edefbe465c46927eccb65af9f8193937",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1080/26408066.2025.2547211",
            "reason_for_inclusion": "Included to meet target filter limit."
        },
        {
            "year": "2025.08",
            "title": "Human Expertise and Large Language Model Embeddings in the Content Validity Assessment of Personality Tests.",
            "team": "Davide Marocco",
            "team website": "",
            "affiliation": "",
            "domain": "LLM embeddings for psychometric content validity",
            "abstract": "In this article, we explore the application of Large Language Models (LLMs) in assessing the content validity of psychometric instruments, focusing on the Big Five Questionnaire (BFQ) and Big Five Inventory (BFI). Content validity, a cornerstone of test construction, ensures that psychological measures adequately cover their intended constructs. Using both human expert evaluations and advanced LLMs, we compared the accuracy of semantic item-construct alignment. Graduate psychology students employed the Content Validity Ratio to rate test items, forming the human baseline. In parallel, state-of-the-art LLMs, including multilingual and fine-tuned models, analyzed item embeddings to predict construct mappings. The results reveal distinct strengths and limitations of human and AI approaches. Human validators excelled in aligning the behaviorally rich BFQ items, while LLMs performed better with the linguistically concise BFI items. Training strategies significantly influenced LLM performance, with models tailored for lexical relationships outperforming general-purpose LLMs. Here we highlight the complementary potential of hybrid validation systems that integrate human expertise and AI precision. The findings underscore the transformative role of LLMs in psychological assessment, paving the way for scalable, objective, and robust test development methodologies.",
            "venue": "Educational and psychological measurement",
            "paperUrl": "https://www.semanticscholar.org/paper/820a8fbfdc7dae76f13041ce27e2070342d0b10f",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1177/00131644251355485",
            "reason_for_inclusion": "Included to meet target filter limit."
        },
        {
            "year": "2025.08",
            "title": "High-level visual representations in the human brain are aligned with large language models",
            "team": "Ian Charest",
            "team website": "",
            "affiliation": "",
            "domain": "Alignment of brain visual representations with LLM embeddings",
            "abstract": "The human brain extracts complex information from visual inputs, including objects, their spatial and semantic interrelations, and their interactions with the environment. However, a quantitative approach for studying this information remains elusive. Here we test whether the contextual information encoded in large language models (LLMs) is beneficial for modelling the complex visual information extracted by the brain from natural scenes. We show that LLM embeddings of scene captions successfully characterize brain activity evoked by viewing the natural scenes. This mapping captures selectivities of different brain areas and is sufficiently robust that accurate scene captions can be reconstructed from brain activity. Using carefully controlled model comparisons, we then proceed to show that the accuracy with which LLM representations match brain representations derives from the ability of LLMs to integrate complex information contained in scene captions beyond that conveyed by individual words. Finally, we train deep neural network models to transform image inputs into LLM representations. Remarkably, these networks learn representations that are better aligned with brain representations than a large number of state-of-the-art alternative models, despite being trained on orders-of-magnitude less data. Overall, our results suggest that LLM embeddings of scene captions provide a representational format that accounts for complex information extracted by the brain from visual inputs. Doerig, Kietzmann and colleagues show that the brain\u2019s response to visual scenes can be modelled using language-based AI representations. By linking brain activity to caption-based embeddings from large language models, the study reveals a way to quantify complex visual understanding.",
            "venue": "Nature Machine Intelligence",
            "paperUrl": "https://www.semanticscholar.org/paper/bc5d6c636c36f5ddc9af61da414511d43373b8e0",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1038/s42256-025-01072-0",
            "reason_for_inclusion": "High quality: Published in Nature Machine Intelligence, a top-tier journal."
        }
    ],
    "foundation-models": [
        {
            "year": "2025.08",
            "title": "Vision-language foundation model for 3D medical imaging",
            "team": "Harrison X. Bai",
            "team website": "",
            "affiliation": "",
            "domain": "Vision-language foundation model for 3D medical imaging",
            "abstract": "",
            "venue": "npj Artificial Intelligence",
            "paperUrl": "https://www.semanticscholar.org/paper/669f6dcb66625d549e88f697fdf9a38cd3bcce3f",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1038/s44387-025-00015-9",
            "reason_for_inclusion": "High quality: Published in npj Artificial Intelligence, a top-tier journal."
        }
    ]
}