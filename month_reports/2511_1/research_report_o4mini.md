# Research Paper Report for 2025-11-01 to 2025-11-15

## Overall Summary

Between November 1 and November 15, 2025, an array of studies explored the intersection of large language models (LLMs), machine learning (ML), and biomedical research across prediction, data extraction, multimodal representation, multi-agent systems, benchmarking, and comprehensive reviews. A key theme is the integration of LLMs with structured electronic health records (EHR) and image data to drive clinical predictions and insights. DeLLiriuM [1] reframes structured ICU data as unstructured text, achieving an AUROC of 82.4 and AUPRC of 11.8 on 77,543 patients across 194 hospitals, demonstrating how text-based LLMs can capture rich clinical context. In parallel, foundational ML methods such as Lasso and random forests combined with LLM-simulated expert reasoning identified 18 critical postoperative AKI risk factors [4], blending algorithmic rigor with domain knowledge. In computational pathology, the TITAN model [3] leverages self-supervised visual feature learning on 335,645 whole-slide images and multimodal alignment with synthetic captions, outperforming ROI-based and slide-level models in linear probing, few-shot, and zero-shot tasks without fine-tuning.  

Systematic review workflows also benefited from LLMs: a study-within-reviews (SWAR) design using Claude versions 2.1–3.5 cut extraction time by 41 minutes per study while boosting accuracy to 91% versus 89% for human-only methods [2]. Moving beyond single-agent paradigms, multi-agent frameworks have been proposed for biomedical hypothesis generation [5], Alzheimer’s therapeutics prioritization [6], and end-to-end bioinformatics workflows via retrieval-augmented small LMs [7].  

Robust benchmarking studies assessed LLM performance in domain-specific tasks: GPT-o3-mini as an automated judge for clinical summaries achieved an ICC of 0.818 against PDSQI [8]; ChatGPT 4.0 outperformed Gemini and Copilot in dermato-oncological guidance [9]; and dietary recommendations for CKD stages revealed significant differences among GPT-4, Gemini, and Copilot, highlighting gaps in clinical guideline adherence [10]. A large-scale pretraining evaluation on 22.2 million single-cell transcriptomes found performance plateauing well before current dataset scales [11].  

Complementary surveys and reviews synthesized these advances, covering LLM agent architectures and applications in multi-omics and health management [12], LLM usage in biology and chemistry [13], inference optimization strategies including quantization and pruning [14], and the challenges of deploying foundation and generative AI models in radiology [15].  

Collectively, these works illustrate emerging trends in modality translation, multi-agent collaboration, scalable evaluation, and the pressing need to balance technical innovation with interpretability, data privacy, and real-world validation. Methodologically, they blend self-supervision, statistical feature selection, expert-in-the-loop validation, and rigorous benchmarking, underscoring practical implications for clinical decision support, systematic review efficiency, drug discovery, and biomedical imaging. However, limitations persist in subjective error adjudication [2], potential model hallucinations [7,12], domain shift and regulatory constraints [15], and diminishing returns on dataset scaling [11]. Future directions will likely focus on enhancing model interpretability, integrating multimodal data at scale, and establishing standardized evaluation frameworks to safely deploy LLM-based tools in clinical and bioinformatics settings.

## Table of Contents
- [Foundation Models](#foundation-models)  
- [AI Agents](#ai-agents)  
- [Benchmarks](#benchmarks)  
- [Reviews](#reviews)  

## Foundation Models

The Foundation Models category [1–4] highlights innovations in leveraging LLMs and self-supervised learning for clinical prediction, structured data extraction, and multimodal pathology representations. DeLLiriuM [1] transforms 24-hour structured EHR from 104,303 ICU admissions into unstructured text inputs, enabling an LLM to predict delirium development with AUROC 82.4 (CI 81.8–83.0) and AUPRC 11.8 (CI 11.3–12.4) across 77,543 patients. This textification approach captures contextual features often lost in tabular representations and generalizes across three major databases. In systematic review workflows, a Study Within Reviews (SWAR) design [2] compares Claude versions 2.1, 3.0 Opus, and 3.5 Sonnet for data extraction in six ongoing reviews, yielding 91.0% accuracy versus 89.0% for human-only methods, and reducing extraction time by a median of 41 minutes per study. Despite high PPVs (>99%), the method relies on subjective error classification and time tracking presents challenges.

In computational pathology, TITAN [3] employs a Transformer-based Image and Text Alignment Network pretrained on 335,645 whole-slide images via visual self-supervised learning and vision-language alignment using 423,122 AI-generated synthetic captions. Without fine-tuning, TITAN achieves state-of-the-art performance in linear probing, few-shot and zero-shot classification, rare cancer retrieval, cross-modal retrieval, and report generation, demonstrating the power of large, unlabeled image corpora. However, reliance on synthetic data may introduce distributional biases in rare disease scenarios.

Finally, an integrative ML-LLM framework [4] analyzes 4,565 cardiac surgery patients from MIMIC-IV, using Lasso and random forests to select 113 risk factors, then simulating cardiology and nephrology expertise via an LLM. Validated through 10-fold cross-validation (accuracy, sensitivity, specificity, AUC), the method pinpoints 18 key postoperative AKI predictors (e.g., creatinine, BUN, FiO2), enhancing clinical interpretability. While effective, this hybrid workflow requires alignment of LLM reasoning with domain experts and may be sensitive to LLM update cycles. Collectively, these works demonstrate advanced methodologies—text conversion of EHR [1], SWAR designs [2], self-supervised multimodal pretraining [3], and LLM-simulated expert feature selection [4]—and underscore the importance of rigorous external validation and attention to data bias and model interpretability.

| Index | Title                                                                                                                                           | Domain                                                       | Venue                         | Team              | DOI                                    | affiliation         | paperUrl                       |
|-------|-------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------|-------------------------------|-------------------|----------------------------------------|---------------------|--------------------------------|
| 1     | A large language model for delirium prediction in the intensive care unit using structured electronic health records                             | Delirium prediction in ICU via LLM                           | Scientific Reports            | Parisa Rashidi    | 10.1038/s41598-025-22634-7             |                     | [Link](https://www.semanticscholar.org/paper/51695af35e27cdbe56a2abefe56294c60e19e38a) |
| 2     | Artificial Intelligence-Assisted Data Extraction With a Large Language Model: A Study Within Reviews.                                            | AI-assisted data extraction for systematic reviews           | Annals of internal medicine   | Leila C Kahwati   | 10.7326/ANNALS-25-00739               |                     | [Link](https://www.semanticscholar.org/paper/dc21f74eef0a822e4822d8530f7b31577018aca3) |
| 3     | A multimodal whole-slide foundation model for pathology                                                                                         | Multimodal pathology foundation model                       | Nature Medicine               | Faisal Mahmood    | 10.1038/s41591-025-03982-3             |                     | [Link](https://www.semanticscholar.org/paper/27ef41325e809dc35657540d14bcbb79cadad20f) |
| 4     | Integration of machine learning and large language models for screening and identifying key risk factors of acute kidney injury after cardiac surgery | AKI risk factor identification via ML & LLM                 | Frontiers in Medicine         | Tao Liu           | 10.3389/fmed.2025.1618222             |                     | [Link](https://www.semanticscholar.org/paper/950c512ad8fdd7345407c7c8ca0a6e0baa15e1e9) |

## AI Agents

The AI Agents section [5–7] examines multi-agent frameworks and LLM-based pipelines to accelerate hypothesis generation, drug repurposing, and bioinformatics workflows. Kim et al. [5] propose a multi-agent LLM architecture for biomedical hypothesis generation in drug combination discovery. Though details are limited, the framework likely involves agent roles specializing in literature mining, compound interaction reasoning, and hypothesis ranking, offering a scalable alternative to monolithic models yet requiring orchestration strategies to avoid agent drift and ensure consistency.

Cui et al. [6] bridge computational predictions and experimental validation for Alzheimer’s therapeutics by comparing three computational repurposing methods—TxGNN, CompGCN, and regularized logistic regression—and unifying their strengths into a candidate list of 90 drugs. An LLM-based agent then synthesizes evidence from biomedical abstracts, mimicking expert manual curation. This pipeline was validated against real-world patient cohorts, trial registries, and pharmacological reviews, demonstrating enhanced efficiency, transparency, and generalizability. However, the study acknowledges potential biases in literature data and the need for prospective in vivo trials.

Malladi et al. [7] introduce BioAgents, a multi-agent system built on small LMs fine-tuned with proprietary bioinformatics data and augmented via retrieval-augmented generation (RAG). Deployed locally for genomics tasks, BioAgents matches human expert performance in conceptual queries and code generation planning, but its current code-generation capabilities and reliance on local compute resources represent areas for future enhancement. Compared to [5], BioAgents emphasizes workflow automation over pure hypothesis generation, while [6] integrates LLMs into structured evidence synthesis. Collectively, these works showcase modular agent designs, domain-specific fine-tuning, and retrieval augmentation as key innovations in biomedical multi-agent LLM systems, albeit with ongoing challenges in agent coordination, hallucination mitigation, and large-scale deployment.

| Index | Title                                                                                                                                                  | Domain                                               | Venue             | Team            | DOI                             | affiliation | paperUrl                       |
|-------|--------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------|-------------------|-----------------|---------------------------------|-------------|--------------------------------|
| 5     | Multi Agent Large Language Models for Biomedical Hypothesis Generation in Drug Combination Discovery                                                  | Multi-agent LLM for drug combination hypothesis      | iScience          | Yejin Kim       | 10.1016/j.isci.2025.113984       |             | [Link](https://www.semanticscholar.org/paper/bc2a9fef179f39c7ede24789d91d01b291167fb6) |
| 6     | Bridging the Computational-Experimental Gap: Leveraging Large Language Model to Prioritize Alzheimer’s Therapeutics Based on Comparison of Learning Models | Alzheimer’s therapeutics prioritization with LLM     | Research Square   | Cui Tao         | 10.21203/rs.3.rs-7811754/v1      |             | [Link](https://www.semanticscholar.org/paper/684a9cd95428486f5796feb5ec659182cc57e847) |
| 7     | BioAgents: Bridging the gap in bioinformatics analysis with multi-agent systems                                                                        | Multi-agent bioinformatics workflow system           | Scientific Reports | V. Malladi      | 10.1038/s41598-025-25919-z       |             | [Link](https://www.semanticscholar.org/paper/3def2209d0bbd2ca5acbbb484d24ab5311fd7659) |

## Benchmarks

The Benchmarks category [8–11] rigorously evaluates LLM capabilities in clinical summarization, dermato-oncological guidance, dietary recommendation, and foundational model scaling. Afshar et al. [8] introduce an LLM-as-a-Judge framework using GPT-o3-mini to assess multi-document EHR summaries against the PDSQI standard. They report an intraclass correlation coefficient (ICC) of 0.818 (CI 0.772–0.854) and zero median score difference from human evaluators, with evaluation times of 22 seconds, highlighting automation potential in quality assessment.

Hædersdal et al. [9] compare three LLMs—ChatGPT 4.0, Gemini, Copilot—across 24 basal cell carcinoma management questions. Expert dermatologists rated accuracy, conciseness, and comprehensiveness. ChatGPT 4.0 led in factual accuracy (87.5%), comprehensiveness (62.5%), and concision (54.2%), with moderate semantic similarity (x̄≈0.60–0.70) but low lexical overlap (x̄≈0.07–0.10). This study underscores the need for expert panels in nuanced clinical domains and points to limitations of purely quantitative metrics.

Chan et al. [10] benchmark GPT-4, Gemini, and Copilot on stage-specific CKD dietary plans using simulated patient profiles. Physician evaluation via a 5-point Likert scale and statistical tests (Kruskal–Wallis, Dunn’s post hoc) revealed Gemini and GPT-4 outperform Copilot in personalization (p=0.0001, p=0.0002), with minor discrepancies in sodium, potassium, and phosphorus estimates versus manual calculations. This work highlights both promise and gaps in nutritional guideline adherence.

Finally, Crawford et al. [11] assess pretraining dataset size and diversity on single-cell foundation model performance. Pretraining 400 transformer models on 22.2 million cells and running 6,400 experiments, they find diminishing returns well before existing atlas scales (>100 million cells). This plateau effect suggests a need to focus on dataset diversity and model architectures rather than sheer scale. Together, these benchmarks illustrate diverse evaluation methodologies—from ICC and panel ratings to statistical significance tests and large-scale experiments—emphasizing both the strengths and limitations of current LLMs in biomedical tasks.

| Index | Title                                                                                                                                             | Domain                                                   | Venue                            | Team              | DOI                                   | affiliation         | paperUrl                       |
|-------|---------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------|----------------------------------|-------------------|---------------------------------------|---------------------|--------------------------------|
| 8     | Evaluating clinical AI summaries with large language models as judges                                                                             | LLM-as-judge for clinical summary evaluation             | NPJ Digital Medicine             | Majid Afshar      | 10.1038/s41746-025-02005-2            |                     | [Link](https://www.semanticscholar.org/paper/78c758993828ae1ee5f0584d813152f2ce4af84f) |
| 9     | Putting AI Chatbots to the Test: A Comparative Analysis of Large Language Models' Performance in the Context of Basal Cell Carcinoma.             | Dermato-oncology LLM performance benchmark               | Experimental dermatology         | Merete Hædersdal  | 10.1111/exd.70175                     |                     | [Link](https://www.semanticscholar.org/paper/451e3c40a041f86528800d0334d2883e427d9083) |
| 10    | Benchmarking ChatGPT and Other Large Language Models for Personalized Stage-Specific Dietary Recommendations in Chronic Kidney Disease             | CKD dietary recommendation LLM benchmark                 | Journal of Clinical Medicine     | M. Chan           | 10.3390/jcm14228033                   |                     | [Link](https://www.semanticscholar.org/paper/f96a2b7f2a4866633d0ce8b267f7ddb2b978ac50) |
| 11    | Evaluating the role of pre-training dataset size and diversity on single-cell foundation model performance                                       | Single-cell foundation model pretraining evaluation      | bioRxiv                          | Lorin Crawford    | 10.1101/2024.12.13.628448             | Microsoft Research | [Link](https://www.semanticscholar.org/paper/def7e6e5f4214315563aea3a913cc3580431af90) |

## Reviews

The Reviews section [12–15] offers comprehensive overviews of LLM agents, their technical foundations, inference optimization, and applications in biomedical fields. Peng et al. [12] examine LLM agent architectures, collaborative modes, and applications across multi-omics, drug development, chemical research, clinical diagnosis, and health management. They identify core challenges—framework interaction, data privacy, hallucinations, interpretability, knowledge timeliness, and ethical/legal risks—and propose future directions including open-source ecosystems and standardization.

Lee’s survey [13] (abstract not provided) appears to collate developments of LLMs in biology and chemistry, likely covering domain-specific modeling approaches, data modalities, and translational case studies. Ma et al. [14] focus on efficient LLM inference, categorizing optimization techniques—quantization, pruning, distillation, efficient architectures, compilation, and hardware-aware methods—within a new taxonomy. They discuss interactions between optimization and model training, fine-tuning, and serving stages, highlighting real-world applications and unresolved research issues.

Rahsepar et al. [15] review foundation and generative AI models in radiology, detailing self-attention mechanisms, multimodal data processing, and adaptation strategies such as transfer learning, prompt engineering, few-shot and zero-shot learning. They explore generative support for synthetic image creation to mitigate annotation scarcity, while cautioning against interpretability gaps, bias, privacy concerns, regulatory hurdles, high computational costs, and domain shifts. Across these surveys, there is consensus on the transformative potential of LLM agents, the necessity of efficiency and interpretability, and the importance of multidisciplinary collaboration to responsibly deploy LLM-based systems in bioinformatics and healthcare.

| Index | Title                                                                                                                           | Domain                                           | Venue                                                     | Team             | DOI                                    | affiliation | paperUrl                       |
|-------|---------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------|-----------------------------------------------------------|------------------|----------------------------------------|-------------|--------------------------------|
| 12    | The rise and potential opportunities of large language model agents in bioinformatics and biomedicine                           | Review of LLM agents in bioinformatics           | Briefings in Bioinformatics                              | Jiajie Peng      | 10.1093/bib/bbaf601                   |             | [Link](https://www.semanticscholar.org/paper/5ed2d020b02049452d6d37f46ea2f8cb67340cda) |
| 13    | A survey on large language models in biology and chemistry.                                                                     | Survey on LLMs in biology and chemistry          | Experimental & molecular medicine                        | Juyong Lee       | 10.1038/s12276-025-01583-1            |             | [Link](https://www.semanticscholar.org/paper/57ce49793ed83b65abbba9910417216e7cab21ca) |
| 14    | Survey on Efficient Large Language Models: Principles, Algorithms, Applications, and Open Issues.                               | Survey on efficient LLM inference optimization   | IEEE transactions on neural networks and learning systems | Lianbo Ma        | 10.1109/TNNLS.2025.3628671            |             | [Link](https://www.semanticscholar.org/paper/98c5653f348bd14a92a875bacea1459d7325b8cd) |
| 15    | Generative AI and Foundation Models in Radiology: Applications, Opportunities, and Potential Challenges.                       | Review of foundation models in radiology         | Radiology                                                 | A. Rahsepar      | 10.1148/radiol.242961                 |             | [Link](https://www.semanticscholar.org/paper/54fc0d09d25af18d5c24f1cb0cd6c20660ba4ae3) |

