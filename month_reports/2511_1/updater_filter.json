{
    "foundation-models": [
        {
            "year": "2025.11",
            "title": "A large language model for delirium prediction in the intensive care unit using structured electronic health records",
            "team": "Parisa Rashidi",
            "team website": "",
            "affiliation": "",
            "domain": "Delirium prediction in ICU via LLM",
            "abstract": "Delirium is an acute syndrome characterized by fluctuating attention, cognitive impairment, and severe disorganization of behavior, which has been shown to affect up to 31% of patients in the intensive care unit (ICU). Early detection can enable timely interventions and improved health outcomes. While artificial intelligence (AI) models have shown great potential for ICU delirium prediction using structured electronic health records (EHR), most studies have either not leveraged state-of-the-art AI models, been limited to single-center cohorts, or relied on small datasets for development and validation. In this study, we introduce DeLLiriuM, a novel LLM-based delirium prediction model that utilizes EHR data from the first 24 hours of ICU admission to estimate a patient\u2019s risk of developing delirium for the remainder of their ICU stay. We developed and validated DeLLiriuM using ICU admissions from 104,303 patients across 195 hospitals in three large databases: the eICU Collaborative Research Database, the Medical Information Mart for Intensive Care (MIMIC)-IV, and the University of Florida\u2019s Integrated Data Repository. Our DeLLiriuM model achieved superior performance compared to all baseline models on the external validation set, measured by the area under the receiver operating characteristic curve (AUROC) and the area under the precision-recall curve (AUPRC) metric. DeLLiriuM attained AUROC 82.4 (95% confidence interval 81.8\u201383.0) and AUPRC 11.8 (95% confidence interval 11.3\u201312.4) across 77,543 patients spanning 194 hospitals. Our approach of transforming structured EHR data into an unstructured text format, the primary data modality for LLMs, enables our DeLLiriuM model to capture clinical contextual information, resulting in improved predictive performance. To the best of our knowledge, DeLLiriuM is the first LLM-based delirium prediction tool for the ICU that utilizes structured EHR data with LLMs rather than clinical notes with LLMs or traditional structured feature representations used in AI models.",
            "venue": "Scientific Reports",
            "paperUrl": "https://www.semanticscholar.org/paper/51695af35e27cdbe56a2abefe56294c60e19e38a",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1038/s41598-025-22634-7",
            "reason_for_inclusion": "High quality: Published in Scientific Reports, a Q1 journal."
        },
        {
            "year": "2025.11",
            "title": "Artificial Intelligence-Assisted Data Extraction With a Large Language Model: A Study Within Reviews.",
            "team": "Leila C Kahwati",
            "team website": "",
            "affiliation": "",
            "domain": "AI-assisted data extraction for systematic reviews",
            "abstract": "BACKGROUND\nData extraction is a critical but error-prone and labor-intensive task in evidence synthesis. Unlike other artificial intelligence (AI) technologies, large language models (LLMs) do not require labeled training data for data extraction.\n\n\nOBJECTIVE\nTo compare an AI-assisted versus a traditional, human-only data extraction process.\n\n\nDESIGN\nStudy within reviews (SWAR) using a prospective, parallel-group comparison with blinded data adjudicators.\n\n\nSETTING\nWorkflow validation within 6 ongoing systematic reviews of interventions under real-world conditions.\n\n\nINTERVENTION\nInitial data extraction using an LLM (Claude, versions 2.1, 3.0 Opus, and 3.5 Sonnet) verified by a human reviewer.\n\n\nMEASUREMENTS\nConcordance, time on task, accuracy, sensitivity, positive predictive value, and error analysis.\n\n\nRESULTS\nThe 6 systematic reviews in the SWAR yielded 9341 data elements from 63 studies. Concordance between the 2 methods was 77.2% (95% CI, 76.3% to 78.0%). Compared with the reference standard, the AI-assisted approach had an accuracy of 91.0% (CI, 90.4% to 91.6%) and the human-only approach an accuracy of 89.0% (CI, 88.3% to 89.6%). Sensitivities were 89.4% (CI, 88.6% to 90.1%) and 86.5% (CI, 85.7% to 87.3%), respectively, with positive predictive values of 99.2% (CI, 99.0% to 99.4%) and 98.9% (CI, 98.6% to 99.1%). Incorrect data were extracted in 9.0% (CI, 8.4% to 9.6%) of AI-assisted cases and 11.0% (CI, 10.4% to 11.7%) of human-only cases, with corresponding proportions of major errors of 2.5% (CI, 2.2% to 2.8%) versus 2.7% (CI, 2.4% to 3.1%). Missed data items were the most frequent error type in both approaches. The AI-assisted method reduced data extraction time by a median of 41 minutes per study.\n\n\nLIMITATIONS\nAssessing concordance and classifying errors required subjective judgment. Consistently tracking time on task was challenging.\n\n\nCONCLUSION\nData extraction assisted by AI may offer a viable, more efficient alternative to human-only methods.\n\n\nPRIMARY FUNDING SOURCE\nAgency for Healthcare Research and Quality and RTI International.",
            "venue": "Annals of internal medicine",
            "paperUrl": "https://www.semanticscholar.org/paper/dc21f74eef0a822e4822d8530f7b31577018aca3",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.7326/ANNALS-25-00739",
            "reason_for_inclusion": "High quality: Published in Annals of Internal Medicine, a top-tier journal."
        },
        {
            "year": "2025.11",
            "title": "A multimodal whole-slide foundation model for pathology",
            "team": "Faisal Mahmood",
            "team website": "",
            "affiliation": "",
            "domain": "Multimodal pathology foundation model",
            "abstract": "The field of computational pathology has been transformed with recent advances in foundation models that encode histopathology region-of-interests (ROIs) into versatile and transferable feature representations via self-supervised learning. However, translating these advancements to address complex clinical challenges at the patient and slide level remains constrained by limited clinical data in disease-specific cohorts, especially for rare clinical conditions. We propose Transformer-based pathology Image and Text Alignment Network (TITAN), a multimodal whole-slide foundation model pretrained using 335,645 whole-slide images via visual self-supervised learning and vision-language alignment with corresponding pathology reports and 423,122 synthetic captions generated from a multimodal generative AI copilot for pathology. Without any fine-tuning or requiring clinical labels, TITAN can extract general-purpose slide representations and generate pathology reports that generalize to resource-limited clinical scenarios such as rare disease retrieval and cancer prognosis. We evaluate TITAN on diverse clinical tasks and find that it outperforms both ROI and slide foundation models across machine learning settings, including linear probing, few-shot and zero-shot classification, rare cancer retrieval, cross-modal retrieval and pathology report generation. Pretrained using 335,645 whole-slide images, a foundation model is developed to provide representations for slide- and patient-level tasks. It is capable of performing clinical tasks and generating reports even in data-scarce scenarios, such as rare cancer diagnosis and survival prediction, without requiring further fine-tuning.",
            "venue": "Nature Medicine",
            "paperUrl": "https://www.semanticscholar.org/paper/27ef41325e809dc35657540d14bcbb79cadad20f",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1038/s41591-025-03982-3",
            "reason_for_inclusion": "High quality: Published in Nature Medicine, a top-tier journal."
        },
        {
            "year": "2025.11",
            "title": "Integration of machine learning and large language models for screening and identifying key risk factors of acute kidney injury after cardiac surgery",
            "team": "Tao Liu",
            "team website": "",
            "affiliation": "",
            "domain": "AKI risk factor identification via ML & LLM",
            "abstract": "Objectives This study aimed to identify critical risk factors for acute kidney injury (AKI) following cardiac surgery. By integrating patient data from the MIMIC-IV database with large language models (LLMs) and machine learning algorithms, we ensured the clinical relevance of the selected risk factors, providing robust insights for the early identification and intervention of postoperative AKI. Methods Intensive care unit (ICU) data of patients from the MIMIC-IV database undergoing cardiac surgery were analyzed. Lasso regression and random forest algorithms were used to select significant predictive features from high-dimensional data. Model evaluation involved 10-fold cross-validation and metrics including accuracy, sensitivity, specificity, and the area under the curve. To enhance clinical relevance, LLMs-simulated expert judgment in cardiology and nephrology, which was further validated through discussions with clinical experts. Results In the cohort consisting of 4,565 patients, a total of 113 important and shared risk factors for AKI were identified, including variables such as anion gap, arterial partial pressure of oxygen (PaO2), and fraction of inspired oxygen (FiO2). Among these, 18 key variables were identified as postoperative AKI predictors via machine learning and LLMs-simulated expert validation. These included anchor age, Creatinine (serum), BUN (Blood Urea Nitrogen), Potassium (serum), Sodium (serum), Lactic Acid, Troponin-T, Furosemide (Lasix), Vancomycin (Random), Gentamicin (Trough), Albumin 5%, ART BP Mean, Cardiac Output (thermodilution), Brain Natriuretic Peptide (BNP), Absolute Count - Lymphs, Absolute Count - Monos, and Absolute Count - Neuts. The integration of LLMs with machine learning algorithms proved effective in accurately identifying clinically relevant risk factors. Conclusion The proposed risk prediction approach for postoperative AKI following cardiac surgery, based on the collaborative analysis of machine learning and large language models (LLMs), effectively identified and validated key clinical risk factors. By simulating expert clinical reasoning, the LLMs significantly enhanced the medical relevance of feature selection and improved the clinical interpretability of the model. This approach provides a solid theoretical and practical foundation for the precise early identification and clinical intervention of postoperative AKI in cardiac surgery patients.",
            "venue": "Frontiers in Medicine",
            "paperUrl": "https://www.semanticscholar.org/paper/950c512ad8fdd7345407c7c8ca0a6e0baa15e1e9",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.3389/fmed.2025.1618222",
            "reason_for_inclusion": "Included to meet target filter limit."
        }
    ],
    "ai-agents": [
        {
            "year": "2025.11",
            "title": "Multi Agent Large Language Models for Biomedical Hypothesis Generation in Drug Combination Discovery",
            "team": "Yejin Kim",
            "team website": "",
            "affiliation": "",
            "domain": "Multi-agent LLM for drug combination hypothesis",
            "abstract": "",
            "venue": "iScience",
            "paperUrl": "https://www.semanticscholar.org/paper/bc2a9fef179f39c7ede24789d91d01b291167fb6",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1016/j.isci.2025.113984",
            "reason_for_inclusion": "Included to meet target filter limit."
        },
        {
            "year": "2025.11",
            "title": "Bridging the Computational-Experimental Gap: Leveraging Large Language Model to Prioritize Alzheimer\u2019s Therapeutics Based on Comparison of Learning Models",
            "team": "Cui Tao",
            "team website": "",
            "affiliation": "",
            "domain": "Alzheimer\u2019s therapeutics prioritization with LLM",
            "abstract": "Abstract Alzheimer\u2019s Disease (AD) 1 is a progressive neurodegenerative disorder with limited therapeutic options, driving interest in drug repurposing to accelerate treatment discovery. Drug repurposing has emerged as a promising strategy to accelerate therapeutic discovery by repositioning existing drugs for new clinical indications. Recent computational repurposing approaches, including knowledge graph reasoning, transcriptomic signature analysis, and integrative literature mining, have demonstrated strong predictive capabilities 2 . However, these methods often yield divergent drug rankings, which makes it difficult to decide which candidates to advance for experimental follow-up and results in substantial gaps between computational predictions and feasible in vivo validation 2 .To bridge this computational-experimental gap, we proposed an advanced prioritization framework leveraging large language models (LLMs). Our method systematically evaluated three state-of-the-art (SOTA) and representative computational methods (TxGNN 3 , Composition-based Graph Convolutional Network (CompGCN) 4 , and a regularized logistic regression (RLR) 5 , to analyze both their predictive performance and pharmaceutical class distributions. By integrating the strengths and divergences of these models, we generated a unified, streamlined list of 90 candidate drugs for further prioritization. We then utilized an LLM-based agent to perform evidence synthesis from biomedical literature abstracts for each candidate. This process mimics expert manual curation but significantly reduces human effort and time by efficiently distilling vast textual data into actionable insights. Applying consistent and transparent selection criteria, we obtained a refined and prioritized list of drug candidates suitable for subsequent in vivo experimental validation. The robustness and clinical relevance of our framework were validated using real-world data from Alzheimer\u2019s patient cohorts, clinical trial registries, and expert pharmacological reviews. This comprehensive validation confirmed that our LLM-driven approach enhances efficiency, consistency, scalability, and generalizability. By integrating computational predictions with scalable evidence synthesis and multifaceted validation, our framework facilitated rapid and informed prioritization of repurposed drugs. Our framework can potentially accelerate the translational pathway toward viable AD therapeutics. Moreover, the versatility of our framework can also be applied to drug repurposing efforts for other diseases beyond AD.",
            "venue": "Research Square",
            "paperUrl": "https://www.semanticscholar.org/paper/684a9cd95428486f5796feb5ec659182cc57e847",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.21203/rs.3.rs-7811754/v1",
            "reason_for_inclusion": "Included to meet target filter limit."
        },
        {
            "year": "2025.11",
            "title": "BioAgents: Bridging the gap in bioinformatics analysis with multi-agent systems",
            "team": "V. Malladi",
            "team website": "",
            "affiliation": "",
            "domain": "Multi-agent bioinformatics workflow system",
            "abstract": "Developing end-to-end bioinformatics workflows is challenging, demanding deep expertise in both genomics and computational techniques. While large language models (LLMs) provide some assistance, they often lack the nuanced guidance required for complex bioinformatics tasks, and are resource-intensive. We thus propose a multi-agent system built on small language models, fine-tuned on bioinformatics data, and enhanced with retrieval augmented generation (RAG). Our system, BioAgents, enables local operation and personalization using proprietary data. We observe performance comparable to human experts on conceptual genomics tasks, and discuss future work to enhance code generation capabilities.",
            "venue": "Scientific Reports",
            "paperUrl": "https://www.semanticscholar.org/paper/3def2209d0bbd2ca5acbbb484d24ab5311fd7659",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1038/s41598-025-25919-z",
            "reason_for_inclusion": "High quality: Published in Scientific Reports, a Q1 journal."
        }
    ],
    "benchmarks": [
        {
            "year": "2025.11",
            "title": "Evaluating clinical AI summaries with large language models as judges",
            "team": "Majid Afshar",
            "team website": "",
            "affiliation": "",
            "domain": "LLM-as-judge for clinical summary evaluation",
            "abstract": "Electronic Health Records (EHRs) contain vast clinical data that are difficult for providers to synthesize. Generative AI with Large Language Models (LLMs) can summarize records to reduce cognitive burden, but ensuring accuracy requires reliable evaluation. Human review is the gold standard but is costly and slow. To address this, we introduce and validate an automated LLM-based method to assess real-world EHR multi-document summaries. Benchmarking against the validated Provider Documentation Summarization Quality Instrument (PDSQI), our LLM-as-a-Judge framework demonstrated strong inter-rater reliability with human evaluators. GPT-o3-mini achieved an intraclass correlation coefficient of 0.818 (95% CI 0.772\u20130.854), a median score difference of 0 from humans, and completed evaluations in 22 seconds. Overall, reasoning models excelled in inter-rater reliability, particularly for evaluations requiring advanced reasoning and domain expertise, outperforming non-reasoning, task-trained, and multi-agent approaches. By automating high-quality evaluations, a medical LLM-as-a-Judge provides a scalable, efficient way to identify accurate, safe AI-generated clinical summaries.",
            "venue": "NPJ Digital Medicine",
            "paperUrl": "https://www.semanticscholar.org/paper/78c758993828ae1ee5f0584d813152f2ce4af84f",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1038/s41746-025-02005-2",
            "reason_for_inclusion": "High quality: Published in NPJ Digital Medicine, a top-tier journal."
        },
        {
            "year": "2025.11",
            "title": "Putting AI Chatbots to the Test: A Comparative Analysis of Large Language Models' Performance in the Context of Basal Cell Carcinoma.",
            "team": "Merete H\u00e6dersdal",
            "team website": "",
            "affiliation": "",
            "domain": "Dermato-oncology LLM performance benchmark",
            "abstract": "Large language models (LLMs) have been explored in various dermato-oncological conditions. In this study, we aimed to compare different LLMs' potential to guide clinicians on the treatment of basal cell carcinoma (BCC). Four authors formulated 24 questions on the topic of clinical management of BCC. The blinded responses of three LLMs (Gemini, Copilot and ChatGPT 4.0) were presented to a panel of nine dermato-oncologists for assessment of (i) factual accuracy, (ii) concision, (iii) comprehensiveness and (iv) overall preference. In addition, the responses were then quantitatively compared based on lexical (i.e., vocabulary) and semantic (i.e., meaning) similarity to three additional LLMs (ChatGPT 3.5, ChatGPT 4o and Claude). ChatGPT 4.0 had the highest accuracy rate (87.5%, i.e., 21/24 responses), followed by Gemini (50%) and Copilot (25%). All models scored lower for concision and comprehensiveness, with ChatGPT 4.0 in the lead (62.5% comprehensive; 54.2% concise), followed by Gemini (33.3%; 12.5%) and Copilot (16.7%; 8.3%). The panel achieved consensus on model preference in 16 questions (ChatGPT 4.0: 54.2%; Gemini: 8.3%; Copilot: 4.2%; no consensus: 33.3%). While the lexical similarity was found to be low (x\u0304 ~0.07-0.10 across models), the semantic similarity between the LLM responses was moderate (x\u0304 ~0.60-0.70 across models). LLMs may assist clinicians in settings where expert dermato-oncological guidance is not readily available, with ChatGPT 4.0 currently outperforming both Gemini and Copilot. Since quantitative methods are unable to detect clinically relevant differences between LLMs, surveying dermatologists is necessary to identify useful models in this rapidly developing field.",
            "venue": "Experimental dermatology",
            "paperUrl": "https://www.semanticscholar.org/paper/451e3c40a041f86528800d0334d2883e427d9083",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1111/exd.70175",
            "reason_for_inclusion": "Included to meet target filter limit."
        },
        {
            "year": "2025.11",
            "title": "Benchmarking ChatGPT and Other Large Language Models for Personalized Stage-Specific Dietary Recommendations in Chronic Kidney Disease",
            "team": "M. Chan",
            "team website": "",
            "affiliation": "",
            "domain": "CKD dietary recommendation LLM benchmark",
            "abstract": "Background: Chronic kidney disease (CKD) requires strict dietary management tailored to disease stage and individual needs. Recent advances in artificial intelligence (AI) have introduced chatbot-based tools capable of generating dietary recommendations. However, their accuracy, personalization, and practical applicability in clinical nutrition remain largely unvalidated, particularly in non-Western settings. Methods: Simulated patient profiles representing each CKD stage were developed and used to prompt GPT-4 (OpenAI), Gemini (Google), and Copilot (Microsoft) with the same request for meal planning. AI-generated diets were evaluated by three physicians using a 5-point Likert scale across three criteria: personalization, consistency with guidelines, practicality, and availability. Descriptive statistics, Kruskal\u2013Wallis tests, and Dunn\u2019s post hoc tests were performed to compare model performance. Nutritional analysis of four meal plans (Initial, GPT-4, Gemini, and Copilot) was conducted using both GPT-4 estimates and manual calculations validated against clinical dietary sources. Results: Scores for personalization and consistency were significantly higher for Gemini and GPT-4 compared with Copilot, with no significant differences between Gemini and GPT-4 (p = 0.0001 and p = 0.0002, respectively). Practicality showed marginal significance, with GPT-4 slightly outperforming Gemini (p = 0.0476). Nutritional component analysis revealed discrepancies between GPT-4\u2019s internal estimations and manual values, with occasional deviations from clinical guidelines, most notably for sodium and potassium, and moderate overestimation for phosphorus. Conclusions: While AI chatbots show promise in delivering dietary guidance for CKD patients, with Gemini demonstrating the strongest performance, further development, clinical validation, and testing with real patient data are needed before AI-driven tools can be fully integrated into patient-centered CKD nutritional care.",
            "venue": "Journal of Clinical Medicine",
            "paperUrl": "https://www.semanticscholar.org/paper/f96a2b7f2a4866633d0ce8b267f7ddb2b978ac50",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.3390/jcm14228033",
            "reason_for_inclusion": "Included to meet target filter limit."
        },
        {
            "year": "2025.11",
            "title": "Evaluating the role of pre-training dataset size and diversity on single-cell foundation model performance",
            "team": "Lorin Crawford",
            "team website": "",
            "affiliation": "Microsoft Research",
            "domain": "Single-cell foundation model pretraining evaluation",
            "abstract": "The success of transformer-based foundation models on natural language and images has motivated their use in single-cell biology. Single-cell foundation models have been trained on increasingly larger transcriptomic datasets, scaling from initial studies with 1 million cells to newer atlases with over 100 million cells. This study investigates the role of pre-training dataset size and diversity on the performance of single-cell foundation models on both zero-shot and fine-tuned tasks. Using a large corpus of 22.2 million cells, we pre-train a total of 400 models, which we evaluate by conducting 6,400 experiments. Our results show that current methods tend to plateau in performance with pre-training datasets that are only a fraction of the size of current training corpora.",
            "venue": "bioRxiv",
            "paperUrl": "https://www.semanticscholar.org/paper/def7e6e5f4214315563aea3a913cc3580431af90",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1101/2024.12.13.628448",
            "reason_for_inclusion": "High quality: Preprint affiliated with Microsoft Research."
        }
    ],
    "reviews": [
        {
            "year": "2025.11",
            "title": "The rise and potential opportunities of large language model agents in bioinformatics and biomedicine",
            "team": "Jiajie Peng",
            "team website": "",
            "affiliation": "",
            "domain": "Review of LLM agents in bioinformatics",
            "abstract": "Abstract Large language model (LLM) agents have demonstrated remarkable potential in the fields of bioinformatics and biomedicine. This paper reviews the technical foundations of LLM agents, including their core architecture, key technologies, and collaborative modes. We explore the applications of LLM agents in multi-omics, drug development, chemical research, clinical diagnosis, and health management. The paper also analyzes the major challenges faced by LLM agents, such as the interaction and extension of their frameworks, data privacy and security, model hallucinations and interpretability, timeliness of knowledge updates, and ethical and legal risks. Furthermore, we discuss future directions, including paradigms for human-artificial intelligence collaboration and the development of open-source ecosystems and standardization. This paper aims to provide a comprehensive perspective and guidance on the advancement of LLM agents in bioinformatics and biomedicine.",
            "venue": "Briefings in Bioinformatics",
            "paperUrl": "https://www.semanticscholar.org/paper/5ed2d020b02049452d6d37f46ea2f8cb67340cda",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1093/bib/bbaf601",
            "reason_for_inclusion": "High quality: Published in Briefings in Bioinformatics, a Q1 journal."
        },
        {
            "year": "2025.11",
            "title": "A survey on large language models in biology and chemistry.",
            "team": "Juyong Lee",
            "team website": "",
            "affiliation": "",
            "domain": "Survey on LLMs in biology and chemistry",
            "abstract": "",
            "venue": "Experimental & molecular medicine",
            "paperUrl": "https://www.semanticscholar.org/paper/57ce49793ed83b65abbba9910417216e7cab21ca",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1038/s12276-025-01583-1",
            "reason_for_inclusion": "High quality: Published in Experimental & molecular medicine, a top-tier journal."
        },
        {
            "year": "2025.11",
            "title": "Survey on Efficient Large Language Models: Principles, Algorithms, Applications, and Open Issues.",
            "team": "Lianbo Ma",
            "team website": "",
            "affiliation": "",
            "domain": "Survey on efficient LLM inference optimization",
            "abstract": "With the rapid advancement of large language models (LLMs) in both academia and industry, their growing size and complexity have introduced significant challenges in terms of computational cost and deployment efficiency. To address these issues, a wide range of inference optimization techniques-including but not limited to model compression-have been proposed to accelerate LLM inference while preserving model performance. This survey provides a comprehensive overview of LLM inference acceleration strategies, analyzing them from multiple perspectives, including foundational principles, algorithmic techniques, real-world applications, and open research challenges. We begin by introducing core concepts underlying inference optimization and propose a new taxonomy that categorizes existing approaches, including quantization, pruning, distillation, efficient architectures, compilation, and hardware-aware methods. Following the lifecycle of LLM development and deployment, we examine how these techniques interact with model training, fine-tuning, and serving. Furthermore, we highlight key applications of efficient LLMs and discuss emerging trends and unresolved issues in the field. By synthesizing recent advances, this survey aims to provide actionable insights and practical guidance for researchers and practitioners working with scalable and efficient LLM systems.",
            "venue": "IEEE transactions on neural networks and learning systems",
            "paperUrl": "https://www.semanticscholar.org/paper/98c5653f348bd14a92a875bacea1459d7325b8cd",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1109/TNNLS.2025.3628671",
            "reason_for_inclusion": "High quality: Published in IEEE Transactions on Neural Networks and Learning Systems, a top-tier journal."
        },
        {
            "year": "2025.11",
            "title": "Generative AI and Foundation Models in Radiology: Applications, Opportunities, and Potential Challenges.",
            "team": "A. Rahsepar",
            "team website": "",
            "affiliation": "",
            "domain": "Review of foundation models in radiology",
            "abstract": "Foundation models (FMs) represent a transformative advancement in artificial intelligence (AI), with growing applications in medical imaging. These models leverage self-attention mechanisms and are capable of processing multimodal data, such as images, text, audio, and video, across multiple scales. Although FMs require large datasets for initial training, they can be adapted to specific medical imaging tasks using smaller labeled datasets through techniques such as transfer learning, fine-tuning, prompt engineering, few-shot learning, and zero-shot learning, making them especially valuable in data-scarce settings. Many FMs also incorporate generative AI capabilities that support the creation of synthetic medical images to further address annotation limitations. Current applications span various imaging modalities in radiology, where FMs have shown potential to improve diagnostic accuracy and streamline workflows. However, clinical integration remains challenging due to issues such as limited interpretability, potential bias, privacy concerns, regulatory constraints, high computational costs, and domain shifts between training data and real-world clinical environments. Addressing these barriers will require coordinated efforts among technical developers, health care providers, and regulatory bodies. This review explores the evolving role of FMs and generative AI in radiology, highlighting recent research advances, clinical applications, and the key challenges that must be addressed for responsible deployment.",
            "venue": "Radiology",
            "paperUrl": "https://www.semanticscholar.org/paper/54fc0d09d25af18d5c24f1cb0cd6c20660ba4ae3",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1148/radiol.242961",
            "reason_for_inclusion": "High quality: Published in Radiology, a top-tier journal."
        }
    ]
}