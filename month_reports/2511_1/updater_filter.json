{
    "foundation-models": [
        {
            "year": "2025.11",
            "title": "Artificial Intelligence-Assisted Data Extraction With a Large Language Model: A Study Within Reviews.",
            "team": "Leila C Kahwati",
            "team website": "",
            "affiliation": "",
            "domain": "AI-assisted data extraction for systematic reviews",
            "abstract": "BACKGROUND\nData extraction is a critical but error-prone and labor-intensive task in evidence synthesis. Unlike other artificial intelligence (AI) technologies, large language models (LLMs) do not require labeled training data for data extraction.\n\n\nOBJECTIVE\nTo compare an AI-assisted versus a traditional, human-only data extraction process.\n\n\nDESIGN\nStudy within reviews (SWAR) using a prospective, parallel-group comparison with blinded data adjudicators.\n\n\nSETTING\nWorkflow validation within 6 ongoing systematic reviews of interventions under real-world conditions.\n\n\nINTERVENTION\nInitial data extraction using an LLM (Claude, versions 2.1, 3.0 Opus, and 3.5 Sonnet) verified by a human reviewer.\n\n\nMEASUREMENTS\nConcordance, time on task, accuracy, sensitivity, positive predictive value, and error analysis.\n\n\nRESULTS\nThe 6 systematic reviews in the SWAR yielded 9341 data elements from 63 studies. Concordance between the 2 methods was 77.2% (95% CI, 76.3% to 78.0%). Compared with the reference standard, the AI-assisted approach had an accuracy of 91.0% (CI, 90.4% to 91.6%) and the human-only approach an accuracy of 89.0% (CI, 88.3% to 89.6%). Sensitivities were 89.4% (CI, 88.6% to 90.1%) and 86.5% (CI, 85.7% to 87.3%), respectively, with positive predictive values of 99.2% (CI, 99.0% to 99.4%) and 98.9% (CI, 98.6% to 99.1%). Incorrect data were extracted in 9.0% (CI, 8.4% to 9.6%) of AI-assisted cases and 11.0% (CI, 10.4% to 11.7%) of human-only cases, with corresponding proportions of major errors of 2.5% (CI, 2.2% to 2.8%) versus 2.7% (CI, 2.4% to 3.1%). Missed data items were the most frequent error type in both approaches. The AI-assisted method reduced data extraction time by a median of 41 minutes per study.\n\n\nLIMITATIONS\nAssessing concordance and classifying errors required subjective judgment. Consistently tracking time on task was challenging.\n\n\nCONCLUSION\nData extraction assisted by AI may offer a viable, more efficient alternative to human-only methods.\n\n\nPRIMARY FUNDING SOURCE\nAgency for Healthcare Research and Quality and RTI International.",
            "venue": "Annals of internal medicine",
            "paperUrl": "https://www.semanticscholar.org/paper/dc21f74eef0a822e4822d8530f7b31577018aca3",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.7326/ANNALS-25-00739",
            "reason_for_inclusion": "High quality: Published in Annals of Internal Medicine, a top-tier journal."
        },
        {
            "year": "2025.11",
            "title": "A multimodal whole-slide foundation model for pathology",
            "team": "Faisal Mahmood",
            "team website": "",
            "affiliation": "",
            "domain": "Multimodal pathology foundation model",
            "abstract": "The field of computational pathology has been transformed with recent advances in foundation models that encode histopathology region-of-interests (ROIs) into versatile and transferable feature representations via self-supervised learning. However, translating these advancements to address complex clinical challenges at the patient and slide level remains constrained by limited clinical data in disease-specific cohorts, especially for rare clinical conditions. We propose Transformer-based pathology Image and Text Alignment Network (TITAN), a multimodal whole-slide foundation model pretrained using 335,645 whole-slide images via visual self-supervised learning and vision-language alignment with corresponding pathology reports and 423,122 synthetic captions generated from a multimodal generative AI copilot for pathology. Without any fine-tuning or requiring clinical labels, TITAN can extract general-purpose slide representations and generate pathology reports that generalize to resource-limited clinical scenarios such as rare disease retrieval and cancer prognosis. We evaluate TITAN on diverse clinical tasks and find that it outperforms both ROI and slide foundation models across machine learning settings, including linear probing, few-shot and zero-shot classification, rare cancer retrieval, cross-modal retrieval and pathology report generation. Pretrained using 335,645 whole-slide images, a foundation model is developed to provide representations for slide- and patient-level tasks. It is capable of performing clinical tasks and generating reports even in data-scarce scenarios, such as rare cancer diagnosis and survival prediction, without requiring further fine-tuning.",
            "venue": "Nature Medicine",
            "paperUrl": "https://www.semanticscholar.org/paper/27ef41325e809dc35657540d14bcbb79cadad20f",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1038/s41591-025-03982-3",
            "reason_for_inclusion": "High quality: Published in Nature Medicine, a top-tier journal."
        }
    ],
    "ai-agents": [
        {
            "year": "2025.11",
            "title": "Multi Agent Large Language Models for Biomedical Hypothesis Generation in Drug Combination Discovery",
            "team": "Yejin Kim",
            "team website": "",
            "affiliation": "",
            "domain": "Multi-agent LLM for drug combination hypothesis",
            "abstract": "",
            "venue": "iScience",
            "paperUrl": "https://www.semanticscholar.org/paper/bc2a9fef179f39c7ede24789d91d01b291167fb6",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1016/j.isci.2025.113984",
            "reason_for_inclusion": "Included to meet target filter limit."
        },
        {
            "year": "2025.11",
            "title": "Bridging the Computational-Experimental Gap: Leveraging Large Language Model to Prioritize Alzheimer\u2019s Therapeutics Based on Comparison of Learning Models",
            "team": "Cui Tao",
            "team website": "",
            "affiliation": "",
            "domain": "Alzheimer\u2019s therapeutics prioritization with LLM",
            "abstract": "Abstract Alzheimer\u2019s Disease (AD) 1 is a progressive neurodegenerative disorder with limited therapeutic options, driving interest in drug repurposing to accelerate treatment discovery. Drug repurposing has emerged as a promising strategy to accelerate therapeutic discovery by repositioning existing drugs for new clinical indications. Recent computational repurposing approaches, including knowledge graph reasoning, transcriptomic signature analysis, and integrative literature mining, have demonstrated strong predictive capabilities 2 . However, these methods often yield divergent drug rankings, which makes it difficult to decide which candidates to advance for experimental follow-up and results in substantial gaps between computational predictions and feasible in vivo validation 2 .To bridge this computational-experimental gap, we proposed an advanced prioritization framework leveraging large language models (LLMs). Our method systematically evaluated three state-of-the-art (SOTA) and representative computational methods (TxGNN 3 , Composition-based Graph Convolutional Network (CompGCN) 4 , and a regularized logistic regression (RLR) 5 , to analyze both their predictive performance and pharmaceutical class distributions. By integrating the strengths and divergences of these models, we generated a unified, streamlined list of 90 candidate drugs for further prioritization. We then utilized an LLM-based agent to perform evidence synthesis from biomedical literature abstracts for each candidate. This process mimics expert manual curation but significantly reduces human effort and time by efficiently distilling vast textual data into actionable insights. Applying consistent and transparent selection criteria, we obtained a refined and prioritized list of drug candidates suitable for subsequent in vivo experimental validation. The robustness and clinical relevance of our framework were validated using real-world data from Alzheimer\u2019s patient cohorts, clinical trial registries, and expert pharmacological reviews. This comprehensive validation confirmed that our LLM-driven approach enhances efficiency, consistency, scalability, and generalizability. By integrating computational predictions with scalable evidence synthesis and multifaceted validation, our framework facilitated rapid and informed prioritization of repurposed drugs. Our framework can potentially accelerate the translational pathway toward viable AD therapeutics. Moreover, the versatility of our framework can also be applied to drug repurposing efforts for other diseases beyond AD.",
            "venue": "Research Square",
            "paperUrl": "https://www.semanticscholar.org/paper/684a9cd95428486f5796feb5ec659182cc57e847",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.21203/rs.3.rs-7811754/v1",
            "reason_for_inclusion": "Included to meet target filter limit."
        }
    ],
    "benchmarks": [
        {
            "year": "2025.11",
            "title": "Evaluating clinical AI summaries with large language models as judges",
            "team": "Majid Afshar",
            "team website": "",
            "affiliation": "",
            "domain": "LLM-as-judge for clinical summary evaluation",
            "abstract": "Electronic Health Records (EHRs) contain vast clinical data that are difficult for providers to synthesize. Generative AI with Large Language Models (LLMs) can summarize records to reduce cognitive burden, but ensuring accuracy requires reliable evaluation. Human review is the gold standard but is costly and slow. To address this, we introduce and validate an automated LLM-based method to assess real-world EHR multi-document summaries. Benchmarking against the validated Provider Documentation Summarization Quality Instrument (PDSQI), our LLM-as-a-Judge framework demonstrated strong inter-rater reliability with human evaluators. GPT-o3-mini achieved an intraclass correlation coefficient of 0.818 (95% CI 0.772\u20130.854), a median score difference of 0 from humans, and completed evaluations in 22 seconds. Overall, reasoning models excelled in inter-rater reliability, particularly for evaluations requiring advanced reasoning and domain expertise, outperforming non-reasoning, task-trained, and multi-agent approaches. By automating high-quality evaluations, a medical LLM-as-a-Judge provides a scalable, efficient way to identify accurate, safe AI-generated clinical summaries.",
            "venue": "NPJ Digital Medicine",
            "paperUrl": "https://www.semanticscholar.org/paper/78c758993828ae1ee5f0584d813152f2ce4af84f",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1038/s41746-025-02005-2",
            "reason_for_inclusion": "High quality: Published in NPJ Digital Medicine, a top-tier journal."
        },
        {
            "year": "2025.11",
            "title": "Putting AI Chatbots to the Test: A Comparative Analysis of Large Language Models' Performance in the Context of Basal Cell Carcinoma.",
            "team": "Merete H\u00e6dersdal",
            "team website": "",
            "affiliation": "",
            "domain": "Dermato-oncology LLM performance benchmark",
            "abstract": "Large language models (LLMs) have been explored in various dermato-oncological conditions. In this study, we aimed to compare different LLMs' potential to guide clinicians on the treatment of basal cell carcinoma (BCC). Four authors formulated 24 questions on the topic of clinical management of BCC. The blinded responses of three LLMs (Gemini, Copilot and ChatGPT 4.0) were presented to a panel of nine dermato-oncologists for assessment of (i) factual accuracy, (ii) concision, (iii) comprehensiveness and (iv) overall preference. In addition, the responses were then quantitatively compared based on lexical (i.e., vocabulary) and semantic (i.e., meaning) similarity to three additional LLMs (ChatGPT 3.5, ChatGPT 4o and Claude). ChatGPT 4.0 had the highest accuracy rate (87.5%, i.e., 21/24 responses), followed by Gemini (50%) and Copilot (25%). All models scored lower for concision and comprehensiveness, with ChatGPT 4.0 in the lead (62.5% comprehensive; 54.2% concise), followed by Gemini (33.3%; 12.5%) and Copilot (16.7%; 8.3%). The panel achieved consensus on model preference in 16 questions (ChatGPT 4.0: 54.2%; Gemini: 8.3%; Copilot: 4.2%; no consensus: 33.3%). While the lexical similarity was found to be low (x\u0304 ~0.07-0.10 across models), the semantic similarity between the LLM responses was moderate (x\u0304 ~0.60-0.70 across models). LLMs may assist clinicians in settings where expert dermato-oncological guidance is not readily available, with ChatGPT 4.0 currently outperforming both Gemini and Copilot. Since quantitative methods are unable to detect clinically relevant differences between LLMs, surveying dermatologists is necessary to identify useful models in this rapidly developing field.",
            "venue": "Experimental dermatology",
            "paperUrl": "https://www.semanticscholar.org/paper/451e3c40a041f86528800d0334d2883e427d9083",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1111/exd.70175",
            "reason_for_inclusion": "Included to meet target filter limit."
        },
        {
            "year": "2025.11",
            "title": "Benchmarking ChatGPT and Other Large Language Models for Personalized Stage-Specific Dietary Recommendations in Chronic Kidney Disease",
            "team": "M. Chan",
            "team website": "",
            "affiliation": "",
            "domain": "CKD dietary recommendation LLM benchmark",
            "abstract": "Background: Chronic kidney disease (CKD) requires strict dietary management tailored to disease stage and individual needs. Recent advances in artificial intelligence (AI) have introduced chatbot-based tools capable of generating dietary recommendations. However, their accuracy, personalization, and practical applicability in clinical nutrition remain largely unvalidated, particularly in non-Western settings. Methods: Simulated patient profiles representing each CKD stage were developed and used to prompt GPT-4 (OpenAI), Gemini (Google), and Copilot (Microsoft) with the same request for meal planning. AI-generated diets were evaluated by three physicians using a 5-point Likert scale across three criteria: personalization, consistency with guidelines, practicality, and availability. Descriptive statistics, Kruskal\u2013Wallis tests, and Dunn\u2019s post hoc tests were performed to compare model performance. Nutritional analysis of four meal plans (Initial, GPT-4, Gemini, and Copilot) was conducted using both GPT-4 estimates and manual calculations validated against clinical dietary sources. Results: Scores for personalization and consistency were significantly higher for Gemini and GPT-4 compared with Copilot, with no significant differences between Gemini and GPT-4 (p = 0.0001 and p = 0.0002, respectively). Practicality showed marginal significance, with GPT-4 slightly outperforming Gemini (p = 0.0476). Nutritional component analysis revealed discrepancies between GPT-4\u2019s internal estimations and manual values, with occasional deviations from clinical guidelines, most notably for sodium and potassium, and moderate overestimation for phosphorus. Conclusions: While AI chatbots show promise in delivering dietary guidance for CKD patients, with Gemini demonstrating the strongest performance, further development, clinical validation, and testing with real patient data are needed before AI-driven tools can be fully integrated into patient-centered CKD nutritional care.",
            "venue": "Journal of Clinical Medicine",
            "paperUrl": "https://www.semanticscholar.org/paper/f96a2b7f2a4866633d0ce8b267f7ddb2b978ac50",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.3390/jcm14228033",
            "reason_for_inclusion": "Included to meet target filter limit."
        },
        {
            "year": "2025.11",
            "title": "Evaluating the role of pre-training dataset size and diversity on single-cell foundation model performance",
            "team": "Lorin Crawford",
            "team website": "",
            "affiliation": "Microsoft Research",
            "domain": "Single-cell foundation model pretraining evaluation",
            "abstract": "The success of transformer-based foundation models on natural language and images has motivated their use in single-cell biology. Single-cell foundation models have been trained on increasingly larger transcriptomic datasets, scaling from initial studies with 1 million cells to newer atlases with over 100 million cells. This study investigates the role of pre-training dataset size and diversity on the performance of single-cell foundation models on both zero-shot and fine-tuned tasks. Using a large corpus of 22.2 million cells, we pre-train a total of 400 models, which we evaluate by conducting 6,400 experiments. Our results show that current methods tend to plateau in performance with pre-training datasets that are only a fraction of the size of current training corpora.",
            "venue": "bioRxiv",
            "paperUrl": "https://www.semanticscholar.org/paper/def7e6e5f4214315563aea3a913cc3580431af90",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1101/2024.12.13.628448",
            "reason_for_inclusion": "High quality: Preprint affiliated with Microsoft Research."
        }
    ],
    "reviews": [
        {
            "year": "2025.11",
            "title": "The rise and potential opportunities of large language model agents in bioinformatics and biomedicine",
            "team": "Jiajie Peng",
            "team website": "",
            "affiliation": "",
            "domain": "Review of LLM agents in bioinformatics",
            "abstract": "Abstract Large language model (LLM) agents have demonstrated remarkable potential in the fields of bioinformatics and biomedicine. This paper reviews the technical foundations of LLM agents, including their core architecture, key technologies, and collaborative modes. We explore the applications of LLM agents in multi-omics, drug development, chemical research, clinical diagnosis, and health management. The paper also analyzes the major challenges faced by LLM agents, such as the interaction and extension of their frameworks, data privacy and security, model hallucinations and interpretability, timeliness of knowledge updates, and ethical and legal risks. Furthermore, we discuss future directions, including paradigms for human-artificial intelligence collaboration and the development of open-source ecosystems and standardization. This paper aims to provide a comprehensive perspective and guidance on the advancement of LLM agents in bioinformatics and biomedicine.",
            "venue": "Briefings in Bioinformatics",
            "paperUrl": "https://www.semanticscholar.org/paper/5ed2d020b02049452d6d37f46ea2f8cb67340cda",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1093/bib/bbaf601",
            "reason_for_inclusion": "High quality: Published in Briefings in Bioinformatics, a Q1 journal."
        },
        {
            "year": "2025.11",
            "title": "A survey on large language models in biology and chemistry.",
            "team": "Juyong Lee",
            "team website": "",
            "affiliation": "",
            "domain": "Survey on LLMs in biology and chemistry",
            "abstract": "",
            "venue": "Experimental & molecular medicine",
            "paperUrl": "https://www.semanticscholar.org/paper/57ce49793ed83b65abbba9910417216e7cab21ca",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1038/s12276-025-01583-1",
            "reason_for_inclusion": "High quality: Published in Experimental & molecular medicine, a top-tier journal."
        },
        {
            "year": "2025.11",
            "title": "Survey on Efficient Large Language Models: Principles, Algorithms, Applications, and Open Issues.",
            "team": "Lianbo Ma",
            "team website": "",
            "affiliation": "",
            "domain": "Survey on efficient LLM inference optimization",
            "abstract": "With the rapid advancement of large language models (LLMs) in both academia and industry, their growing size and complexity have introduced significant challenges in terms of computational cost and deployment efficiency. To address these issues, a wide range of inference optimization techniques-including but not limited to model compression-have been proposed to accelerate LLM inference while preserving model performance. This survey provides a comprehensive overview of LLM inference acceleration strategies, analyzing them from multiple perspectives, including foundational principles, algorithmic techniques, real-world applications, and open research challenges. We begin by introducing core concepts underlying inference optimization and propose a new taxonomy that categorizes existing approaches, including quantization, pruning, distillation, efficient architectures, compilation, and hardware-aware methods. Following the lifecycle of LLM development and deployment, we examine how these techniques interact with model training, fine-tuning, and serving. Furthermore, we highlight key applications of efficient LLMs and discuss emerging trends and unresolved issues in the field. By synthesizing recent advances, this survey aims to provide actionable insights and practical guidance for researchers and practitioners working with scalable and efficient LLM systems.",
            "venue": "IEEE transactions on neural networks and learning systems",
            "paperUrl": "https://www.semanticscholar.org/paper/98c5653f348bd14a92a875bacea1459d7325b8cd",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1109/TNNLS.2025.3628671",
            "reason_for_inclusion": "High quality: Published in IEEE Transactions on Neural Networks and Learning Systems, a top-tier journal."
        },
        {
            "year": "2025.11",
            "title": "Generative AI and Foundation Models in Radiology: Applications, Opportunities, and Potential Challenges.",
            "team": "A. Rahsepar",
            "team website": "",
            "affiliation": "",
            "domain": "Review of foundation models in radiology",
            "abstract": "Foundation models (FMs) represent a transformative advancement in artificial intelligence (AI), with growing applications in medical imaging. These models leverage self-attention mechanisms and are capable of processing multimodal data, such as images, text, audio, and video, across multiple scales. Although FMs require large datasets for initial training, they can be adapted to specific medical imaging tasks using smaller labeled datasets through techniques such as transfer learning, fine-tuning, prompt engineering, few-shot learning, and zero-shot learning, making them especially valuable in data-scarce settings. Many FMs also incorporate generative AI capabilities that support the creation of synthetic medical images to further address annotation limitations. Current applications span various imaging modalities in radiology, where FMs have shown potential to improve diagnostic accuracy and streamline workflows. However, clinical integration remains challenging due to issues such as limited interpretability, potential bias, privacy concerns, regulatory constraints, high computational costs, and domain shifts between training data and real-world clinical environments. Addressing these barriers will require coordinated efforts among technical developers, health care providers, and regulatory bodies. This review explores the evolving role of FMs and generative AI in radiology, highlighting recent research advances, clinical applications, and the key challenges that must be addressed for responsible deployment.",
            "venue": "Radiology",
            "paperUrl": "https://www.semanticscholar.org/paper/54fc0d09d25af18d5c24f1cb0cd6c20660ba4ae3",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1148/radiol.242961",
            "reason_for_inclusion": "High quality: Published in Radiology, a top-tier journal."
        }
    ]
}
