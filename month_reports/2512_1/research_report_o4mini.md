# Research Paper Report for 2025-12-01 to 2025-12-15

## Overall Summary

Between December 1 and December 15, 2025, the literature reveals a rich interplay of large language models (LLMs), foundation models, and multi-agent systems applied across biology, medicine, and public health. Four main themes emerge: 1) AI agents orchestrating complex pipelines in single-cell analysis, drug discovery, and clinical trials; 2) systematic benchmarking of AI capabilities in knowledge recall, prediction, and creative hypothesis generation; 3) critical reviews addressing scientific rigor, ethical considerations, and human–AI collaboration; and 4) the rapid expansion of foundation models spanning cytology, genomics, and radiology.

AI agents leverage modular, interpretable pipelines that combine retrieval-augmented LLMs, generative chemical language models, and deep neural networks. CASSIA [1] pioneers multi-agent LLMs for automated and interpretable cell annotation, suggesting a move toward AI-driven single‐cell atlas projects. AgentMol [2] integrates a Retrieval-Augmented Generation system with a GPT-2 chemical language model and an RCNN affinity predictor trained on 470,560 BindingDB pairs, achieving perfect validity (1.00), uniqueness (0.96) and R² > 0.6 while maintaining Pearson’s R > 0.8. NetraAI [3] applies dynamical-systems modeling, evolutionary long-range memory selection, and LLM insights to Phase II depression trial data (63 patients, 360 assessments per patient), boosting AUC by 0.32 and reaching 95 % accuracy with 100 % specificity in MRI-based responder detection.

Benchmark studies map AI’s strengths and limitations. Rosi et al. [4] administer the GNKQ-R to zero-shot LLMs (ChatGPT-3.5, ChatGPT-4, Google Gemini, Copilot) and human cohorts, finding ChatGPT-4 (82/88) outperforms dietetics students (79.3/88). scDrugMap [5] undertakes large-scale performance evaluation of foundation models for drug response prediction, while “The Creation Game” framework [6] in systems vaccinology probes AI’s hypothesis-generation, experimental design, and logical inference across case studies on GCN2, SREBP, and TLR5.

Review articles advocate rigorous methodologies and ethical guardrails. Cobert [7] supplies a toolbox for transparency, reproducibility, prompt‐engineering, and environmental impact reporting in LLM studies. Tsang [8] examines the readiness of AI immunologists, highlighting current deficits in original insight generation. Holdsworth [9] proposes a typology of physician–LLM interaction for clinical decision-making. Songsaeng [10] surveys convolutional, transformer, and XR-robotic platforms for stroke imaging, intervention planning, and telerobotics, underscoring regulatory hurdles.

Foundation-model papers illustrate the translational breadth of AI. Scouter [11] harnesses LLM embeddings to predict transcriptional responses to perturbations. COIN [12] is a text-to-image generator trained on 112,226 cytology image-report pairs, validated by experts, and shown to bolster diagnostics under data scarcity. A lung CT vision model [13] extends foundation transformers to multimodal radiology. Zheng et al. [14] deploy DNA (PlantCaduceus) and protein (ESM1b) language models on 1,716 EMS maize lines, identifying 2.6 M variants and 15,264 deleterious mutations, enabling causal‐mutation discovery. Tegnér [15] introduces multimodal transformer architectures for cross-scale genomics.

These works collectively push methodological frontiers—retrieval-augmented pipelines [2], dynamical-systems feature selection [3], zero-shot LLM assessments [4], multimodal generative and predictive frameworks [11–15]—while highlighting practical implications: accelerated drug and cytology diagnostics, precision trial enrollment, and genomic variant interpretation. Limitations persist in LLM domain gaps [4], ethical oversight [6,7], and preclinical maturity of robotics [10], forecasting future directions focused on rigorous benchmarking, scalable multi-agent orchestration, and robust integration of foundation models in real-world clinical and agricultural workflows.

## Table of Contents
- [AI Agents](#ai-agents)  
- [Benchmarks](#benchmarks)  
- [Reviews](#reviews)  
- [Foundation Models](#foundation-models)  

## AI Agents

The AI Agents category encompasses three innovative systems that orchestrate multiple AI components—from LLMs and chemical generative models to explainable dynamical systems—to automate complex tasks in biology and medicine. CASSIA [1] introduces a multi-agent LLM framework for single-cell annotation, enabling interpretable assignments of cell types by leveraging inter-agent dialog and consensus aggregation. While details on datasets and evaluation metrics remain undisclosed, its publication in Nature Communications underscores its methodological novelty in combining interpretability with automation.

AgentMol [2] demonstrates technical depth by constructing a modular pipeline: a Retrieval-Augmented Generation system using a Large Language Model for target identification, a GPT-2–based chemical language model to produce SMILES candidates, and an RCNN regression network to predict binding affinities (pKi). Trained on 470,560 ligand–protein pairs from BindingDB, the chemical model achieved validity = 1.00, uniqueness = 0.96, diversity = 0.89, while the RCNN attained R² > 0.6 and Pearson’s R > 0.8. By integrating LangGraph for orchestration, AgentMol exemplifies a scalable, end-to-end AI-driven molecular discovery pipeline.

Explainable AI-driven precision clinical trial enrichment via NetraAI [3] applies dynamical-systems modeling, evolutionary long-range memory feature selection, and LLM-generated insights to Phase II ketamine trial data (n = 63; 175 psychiatric scale features and 185 MRI features per patient). NetraAI outperformed traditional ML, yielding a 0.32 AUC improvement and a model with 95 % accuracy and 100 % specificity using just eight MRI features. This work highlights the practical implication of explainable AI in small-cohort settings, enabling “Personas” discovery for personalized medicine.

Comparative analysis across these works reveals divergent focuses: [1] emphasizes annotation interpretability, [2] optimizes molecular generation with quantifiable chemical metrics, and [3] excels in patient stratification using hybrid dynamical–LLM methods. Together, they illustrate a trend toward multi-agent orchestration, rigorous pipeline validation, and the integration of interpretability at every stage.

| Index | Title                                                                                                                           | Domain                                                              | Venue                  | Team             | DOI                            | affiliation | paperUrl                                               |
|-------|---------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------|------------------------|------------------|--------------------------------|-------------|---------------------------------------------------------|
| 1     | CASSIA: a multi-agent large language model for automated and interpretable cell annotation.                                     | automated single-cell annotation via multi-agent LLMs              | Nature communications  | C. Kendziorski  | 10.1038/s41467-025-67084-x     |             | [Link](https://www.semanticscholar.org/paper/7de5beff1eb163c66310e4291928c6f2d7bb34ac) |
| 2     | AgentMol: Multi-Model AI System for Automatic Drug-Target Identification and Molecule Development                             | multi-model AI pipeline for drug-target identification              | Methods and Protocols  | Jacek Nikliński | 10.3390/mps8060143             |             | [Link](https://www.semanticscholar.org/paper/9df305ffbc93536e0e92b625a642fe9d066a3cd8) |
| 3     | Explainable AI-driven precision clinical trial enrichment: demonstration of the NetraAI platform with a phase II depression trial | explainable AI for clinical trial cohort enrichment                 | NPJ Digital Medicine   | Luca Pani        | 10.1038/s41746-025-02143-7     |             | [Link](https://www.semanticscholar.org/paper/75f60ee9808353347e8dc5f20994b7d714fa559d) |

## Benchmarks

Benchmarking studies reveal how LLMs and foundation models perform on factual recall, predictive accuracy, and creative scientific reasoning. Rosi et al. [4] administer the validated GNKQ-R tool to zero-shot LLMs (ChatGPT-3.5, ChatGPT-4, Google Gemini, Copilot) alongside human cohorts. With an AI mean score of 77.3 ± 5.1, ChatGPT-4 (82/88) surpasses dietetics students (79.3/88) and underlines LLMs’ proficiency in “Food Groups” and “Healthy Food Choices,” but it also exhibits gaps in generating personalized dietary plans—a limitation calling for integrated clinical-context modules.

scDrugMap [5] (Nature Communications) conducts a large-scale comparative analysis of foundation models in drug response prediction. Although the abstract is unavailable, its scope likely involves benchmarking transformer-based embeddings against cell-line drug sensitivity datasets, assessing metrics such as ROC-AUC and RMSE across diverse pharmacogenomic panels. This work underscores the necessity of standardized evaluation pipelines for AI-driven pharmacology.

Pulendran’s “Creation Game” framework [6] interrogates LLMs’ creative capacity in systems vaccinology. Through three mechanistic case studies—GCN2 in antigen presentation, SREBP in metabolic response, TLR5 in microbiota–vaccine interactions—the study evaluates AI on hypothesis coherence, novelty, and logical rigor. Results highlight AI’s potential in generating experimental designs but emphasize persistent needs for ethical oversight and domain-expert validation.

While [4] focuses on factual knowledge recall under controlled questionnaires, [5] benchmarks continuous prediction tasks, and [6] probes generative hypothesis formation. Together, they chart a trajectory from narrow QA competencies to broader scientific creativity, but all underscore the indispensable role of human expertise in interpreting AI outputs.

| Index | Title                                                                                                                                | Domain                                                         | Venue                | Team              | DOI                             | affiliation | paperUrl                                               |
|-------|--------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------|----------------------|-------------------|---------------------------------|-------------|---------------------------------------------------------|
| 4     | Comparative analysis of AI on human nutrition knowledge: Evaluating large language model-based conversational agents against dietetics students and the general population | benchmarking LLMs on nutrition knowledge evaluation            | PLOS One             | A. Rosi           | 10.1371/journal.pone.0336577    |             | [Link](https://www.semanticscholar.org/paper/68ab12015fe819fecd876b4dd8cb81facd8c647e) |
| 5     | scDrugMap: benchmarking large foundation models for drug response prediction.                                                        | benchmarking foundation models for drug response prediction    | Nature communications| Qianqian Song     | 10.1038/s41467-025-67481-2      |             | [Link](https://www.semanticscholar.org/paper/f089697d3406e3d861c02c1b98487c0ea905c365) |
| 6     | Assessing AI's cognitive abilities for scientific discovery in the field of systems vaccinology.                                    | evaluating LLMs’ creative abilities in systems vaccinology    | Science immunology   | Bali Pulendran    | 10.1126/sciimmunol.adx1794      |             | [Link](https://www.semanticscholar.org/paper/bbfeb4fad4854bf8299b90dc72e5be97ce3233cb) |

## Reviews

Recent reviews emphasize best practices, ethical frameworks, and human–AI collaboration models across healthcare domains. Cobert [7] delivers a toolbox for maintaining scientific rigor in LLM research, focusing on transparent methodological reporting (model architectures, training data provenance), reproducibility (code and resource sharing), prompt-engineering standardization, environmental impact disclosure, and mitigation of black-box biases.

Tsang [8] discusses whether AI immunologists can generate original biological insights, highlighting that while LLMs automate literature review and coding, they still fall short in creative hypothesis formation. The perspective points to multi-agent and human-in-the-loop frameworks as promising advances.

Holdsworth [9] introduces a typology of physician–LLM interaction approaches in clinical decision-making. Through a taxonomy spanning direct query, guided explanation, and collaborative drafting, it catalogs workflows that optimize clinician trust, highlight liability considerations, and shape regulatory guidelines.

Songsaeng [10] reviews AI-powered neurovascular intervention, surveying convolutional and transformer architectures for lesion detection, multimodal fusion of MRI/CT angiography, extended reality simulators for surgical training, and robotic catheter navigation systems with force-sensing. While the review underscores preclinical proof-of-concepts, it identifies translational bottlenecks—regulatory approval, data privacy, and economic scalability.

Comparatively, [7] sets cross-domain standards, [8] targets immunology research culture, [9] focuses on clinical workflow integration, and [10] offers a technology roadmap for neurovascular care. Collectively, they chart the pathway from methodological caution to future AI-driven precision medicine.

| Index | Title                                                                                                               | Domain                                                            | Venue               | Team                  | DOI                                | affiliation | paperUrl                                               |
|-------|---------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------|---------------------|-----------------------|------------------------------------|-------------|---------------------------------------------------------|
| 7     | All That Shines Is Not Gold: Maintaining Scientific Rigor When Evaluating, Interpreting, and Reviewing Studies Using Large Language Models. | maintaining scientific rigor in LLM research practices            | Anesthesiology      | Julien Cobert         | 10.1097/ALN.0000000000005795       |             | [Link](https://www.semanticscholar.org/paper/ff44718f4c2c4365182484ae0d404d42a7d7ac37) |
| 8     | AI immunologists are here: Are they ready for prime time?                                                           | perspectives on LLM use in immunology research                    | Science immunology  | J. Tsang              | 10.1126/sciimmunol.aea8735         |             | [Link](https://www.semanticscholar.org/paper/a4f0449c55a13ccd2cfee205f69017adf70148a1) |
| 9     | A typology of physician input approaches to using AI chatbots for clinical decision-making.                        | typology of physician-LLM chatbot interaction approaches         | NPJ Digital Medicine| Laura M Holdsworth    | 10.1038/s41746-025-02184-y         |             | [Link](https://www.semanticscholar.org/paper/af0f183cdd0609f7d14fa9d85dceae6020838294) |
| 10    | Toward AI-Powered Neurovascular Intervention: From Imaging to XR-Robotic Convergence.                              | AI-augmented imaging and XR-robotics in neurovascular intervention| Stroke              | D. Songsaeng          | 10.1161/STROKEAHA.125.053121       |             | [Link](https://www.semanticscholar.org/paper/fef4bb297900bf697f92e6c70150009169d5c802) |

## Foundation Models

Foundation models continue to revolutionize computational biology by learning domain-specific representations for generative, predictive, and cross-modal tasks. Scouter [11] uses LLM embeddings to predict transcriptional responses to genetic perturbations, suggesting that masked or autoregressive embeddings can forecast expression shifts, though details on dataset size and evaluation metrics await publication.

COIN [12] trains a controllable image synthesis model on 112,226 cytology image–report pairs across 16 anatomical sites. Expert cytologists validated the anatomical and diagnostic fidelity of generated images. Data-augmentation experiments demonstrate that diagnostic AI models trained on COIN-augmented datasets under data scarcity generalize effectively, while content-based retrieval capabilities support case referencing.

A lung CT vision foundation model [13] adapts transformer architectures for volumetric radiology, facilitating disease classification and segmentation without extensive retraining. Although the abstract is unavailable, its venue in Nature Communications implies robust validation on clinical CT cohorts.

Zheng et al. [14] combine the protein language model ESM1b and the DNA model PlantCaduceus on 1,716 EMS-treated maize lines. They identify 2,586,769 variants, estimate 15,264 (protein) and 18,326 (DNA) deleterious mutations, and validate four causal mutations affecting leaf pigmentation, cuticular wax, seed color, and male sterility. Allelic expression analysis in M2 progeny reveals asymmetric transcript accumulation, uncovering functional impacts of synonymous mutations.

Tegnér’s multimodal transformer [15] integrates genomic sequence, epigenomic signals, and transcriptomic profiles in a single transformer framework, enabling cross-scale predictions from nucleotide to chromatin structure. Benchmarks likely include correlation to experimental assays and fine-tuning performance on downstream tasks.

Collectively, these models represent a shift toward universal architectures that can be fine-tuned or prompted for varied biological applications, emphasize privacy-preserving synthetic data [12], and underscore the importance of standardized evaluation metrics across generative and predictive tasks. Remaining challenges include computational efficiency, domain adaptation for low-resource species, and consensus benchmarks to compare disparate foundation models.

| Index | Title                                                                                                        | Domain                                                        | Venue                                                                 | Team                | DOI                               | affiliation | paperUrl                                               |
|-------|--------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------|-----------------------------------------------------------------------|---------------------|-----------------------------------|-------------|---------------------------------------------------------|
| 11    | Scouter predicts transcriptional responses to genetic perturbations with large language model embeddings.   | LLM embeddings to predict transcriptional responses           | Nature computational science                                          | Jun Li             | 10.1038/s43588-025-00912-8        |             | [Link](https://www.semanticscholar.org/paper/ef1e5451d11c0684b4867e410682daf776ab91c8) |
| 12    | A Generative Foundation Model for Scalable Cytology Image Synthesis in AI-Powered Diagnostics.              | generative foundation model for cytology image synthesis      | Clinical cancer research : an official journal of the American Association for Cancer Research | Mu-yan Cai         | 10.1158/1078-0432.CCR-25-2445      |             | [Link](https://www.semanticscholar.org/paper/2698cc4940fd54cdbc458272b3abc41e7bad6679) |
| 13    | A lung CT vision foundation model facilitating disease diagnosis and medical imaging.                        | lung CT vision foundation model for disease diagnosis         | Nature communications                                                | Qionghai Dai       | 10.1038/s41467-025-66620-z         |             | [Link](https://www.semanticscholar.org/paper/37e1003be2e2cc4c1f09352c17363597d47f6740) |
| 14    | Large DNA and protein language models enhance discovery of deleterious mutations in maize                   | large DNA and protein language models for maize mutation discovery | Genome Biology                                                       | Jun Zheng          | 10.1186/s13059-025-03890-2        |             | [Link](https://www.semanticscholar.org/paper/c21fa11a2873e22feaeef7ffaa395d377f6b9cad) |
| 15    | Multimodal foundation transformer models for multiscale genomics.                                           | multimodal transformer foundation models for genomics         | Nature methods                                                       | Jesper N. Tegnér    | 10.1038/s41592-025-02918-6         |             | [Link](https://www.semanticscholar.org/paper/336d539daa17f126463a3373d00df732a0f04c83) |

