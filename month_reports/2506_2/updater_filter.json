{
    "ai-agents": [
        {
            "year": "2025.06",
            "title": "Conversational Large-Language-Model Artificial Intelligence Agent for Accelerated Synthesis of Metal-Organic Frameworks Catalysts in Olefin Hydrogenation.",
            "team": "Ge Wang",
            "team website": "",
            "affiliation": "",
            "domain": "Automated MOF synthesis optimization",
            "abstract": "Metal-organic frameworks (MOFs) attract significant attention for their structural diversity and design flexibility, making them ideal candidates for catalytic applications. However, the traditional trial-and-error approach for optimizing MOF synthesis remains inefficient. In this study, we introduce the MOFsyn agent, an AI-driven framework that harnesses large language models (LLMs) for MOF synthesis optimization. This system integrates data automatic analysis, material mechanism analysis, and experimental protocol navigation by employing retrieval-augmented generation (RAG) to refine synthetic strategies based on natural language inputs. Using Ni@UiO-66(Ce) for olefin hydrogenation as a case study, the MOFsyn agent analyzed the relationship between synthesis conditions, structural characteristics, and catalytic performance, with a particular focus on the electronic structure of nickel. Through adaptive optimization, a novel stepwise reduction strategy was proposed that outperformed conventional one-pot reduction. The optimized Ni@UiO-66(Ce)-R2T1, synthesized under MOFsyn agent's guidance, exhibited nearly twice the Ni0/Nitotal ratio compared to the best-performing sample from an initial experimental set and achieved 100% conversion and selectivity for dicyclopentadiene hydrogenation under mild conditions (70 \u00b0C, 2 MPa). These results validate the accuracy and efficiency of the MOFsyn agent. This study provides an efficient tool for intelligent material synthesis, enabling researchers without programming expertise to accelerate material development.",
            "venue": "ACS nano",
            "paperUrl": "https://www.semanticscholar.org/paper/779b1e365cfb15076632c547c5daa5d24fc71066",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1021/acsnano.5c04880",
            "reason_for_inclusion": "High quality: Published in ACS Nano, a Q1 journal."
        },
        {
            "year": "2025.06",
            "title": "Auto-TA: Towards Scalable Automated Thematic Analysis (TA) via Multi-Agent Large Language Models with Reinforcement Learning",
            "team": "Ying Ding",
            "team website": "",
            "affiliation": "",
            "domain": "Automated thematic analysis pipeline",
            "abstract": "Congenital heart disease (CHD) presents complex, lifelong challenges often underrepresented in traditional clinical metrics. While unstructured narratives offer rich insights into patient and caregiver experiences, manual thematic analysis (TA) remains labor-intensive and unscalable. We propose a fully automated large language model (LLM) pipeline that performs end-to-end TA on clinical narratives, which eliminates the need for manual coding or full transcript review. Our system employs a novel multi-agent framework, where specialized LLM agents assume roles to enhance theme quality and alignment with human analysis. To further improve thematic relevance, we optionally integrate reinforcement learning from human feedback (RLHF). This supports scalable, patient-centered analysis of large qualitative datasets and allows LLMs to be fine-tuned for specific clinical contexts.",
            "venue": "ArXiv",
            "paperUrl": "https://www.semanticscholar.org/paper/14d80550b00827175f971b0c433b9c468a507eda",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.48550/arXiv.2506.23998",
            "reason_for_inclusion": "Included to meet target filter limit."
        },
        {
            "year": "2025.06",
            "title": "Large Language Model Agent for Modular Task Execution in Drug Discovery",
            "team": "A. Farimani",
            "team website": "",
            "affiliation": "Carnegie Mellon University",
            "domain": "Automated drug discovery pipeline",
            "abstract": "We present a modular framework powered by large language models (LLMs) that automates and streamlines key tasks across the early-stage computational drug discovery pipeline. By combining LLM reasoning with domain-specific tools, the framework performs biomedical data retrieval, domain-specific question answering, molecular generation, property prediction, property-aware molecular refinement, and 3D protein\u2013ligand structure generation. In a case study targeting BCL-2 in lymphocytic leukemia, the agent autonomously retrieved relevant biomolecular information\u2014including FASTA sequences, SMILES representations, and literature\u2014and answered mechanistic questions with improved contextual accuracy over standard LLMs. It then generated chemically diverse seed molecules and predicted 67 ADMET-related properties, which guided iterative molecular refinement. Across two refinement rounds, the number of molecules with QED > 0.6 increased from 34 to 55, and those passing at least four out of five empirical drug-likeness rules rose from 29 to 52, within a pool of 194 molecules. The framework also employed Boltz-2 to generate 3D protein\u2013ligand complexes and provide rapid binding affinity estimates for candidate compounds. These results demonstrate that the approach effectively supports molecular screening, prioritization, and structure evaluation. Its modular design enables flexible integration of evolving tools and models, providing a scalable foundation for AI-assisted therapeutic discovery.",
            "venue": "bioRxiv",
            "paperUrl": "https://www.semanticscholar.org/paper/0a63c9327d443b172bb4f755bae7dece52a6aa5c",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1101/2025.07.02.662875",
            "reason_for_inclusion": "High relevance: Directly matches user's research interests."
        },
        {
            "year": "2025.06",
            "title": "AI-HOPE: an AI-driven conversational agent for enhanced clinical and genomic data integration in precision medicine research",
            "team": "E. Velazquez-Villarreal",
            "team website": "",
            "affiliation": "",
            "domain": "Conversational agent for clinical/genomic integration",
            "abstract": "Abstract Motivation The growing complexity of clinical cancer research has fueled a surge in demand for automated bioinformatics tools capable of integrating clinical and genomic data to accelerate discovery efforts. Results We present the Artificial Intelligence Agent for High-Optimization and Precision Medicine (AI-HOPE), an AI-driven system that enables domain experts to conduct integrative data analyses through natural language interactions. Powered by Large Language Models, AI-HOPE interprets user instructions, converts them into executable code, and autonomously analyzes locally stored data. It supports flexible association studies, subset comparisons, clinical prevalence assessments and survival analyses. In addition, AI-HOPE enables global variable scans to identify features significantly associated with a user-defined outcome, making a powerful and intuitive tool for advancing precision medicine research. Importantly, its closed-system design prevents clinical data leakage. To demonstrate its utility, AI-HOPE was applied to The Cancer Genome Atlas data to address two clinical questions. First, it identified significant enrichment of TP53 mutations in late-stage colorectal cancer compared to early-stage cases. Second, it uncovered a strong association between KRAS mutations and poorer progression-free survival in FOLFOX-treated patients. These findings align with established literature and demonstrate AI-HOPE's ability to generate meaningful insights independently, without prior assumptions. By removing programming barriers and simplifying complex analyses, AI-HOPE bridges the gap between data complexity and research needs. With its scalable and adaptable framework, AI-HOPE has the potential to support diverse biomedical research fields, driving innovation and efficiency in translational studies. Availability and implementation The AI-HOPE software and demonstration data is available at https://github.com/Velazquez-Villarreal-Lab/AI-HOPE.",
            "venue": "Bioinformatics",
            "paperUrl": "https://www.semanticscholar.org/paper/7bdf2e68db32b63fa02036df656ebde33a058068",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1093/bioinformatics/btaf359",
            "reason_for_inclusion": "High quality: Published in Bioinformatics, a Q1 journal."
        },
        {
            "year": "2025.06",
            "title": "Natural-Language-Interfaced Robotic Synthesis for AI-Copilot-Assisted Exploration of Inorganic Materials.",
            "team": "Cheng Wang",
            "team website": "",
            "affiliation": "",
            "domain": "Robotic synthesis with LLM copilot",
            "abstract": "The automation of chemical synthesis presents opportunities to enhance experimental reproducibility and accelerate discovery. Traditional closed-loop approaches, while effective in specific domains, are often constrained by rigid workflows and the requirement for specialized expertise. Here, we introduce a chemical robotic explorer integrated with an artificial intelligence (AI) copilot to enable a more flexible and adaptive synthesis, simplifying the process from inspiration to experimentation. This modular platform uses a large language model (LLM) to map natural language synthetic descriptions to executable unit operations, including temperature control, stirring, liquid and solid handling, filtration, etc. By integrating AI-driven literature searches, real-time experimental design, conversational human-AI interaction, and feedback-based optimization, we demonstrate the capabilities of AI in successfully synthesizing 13 compounds across four distinct classes of inorganic materials: coordination complexes, metal-organic frameworks, nanoparticles, and polyoxometalates. Notably, this approach enabled the discovery of a previously unreported family of Mn-W polyoxometalate clusters, showing the potential of AI-enhanced robotics as a generalizable and adaptable platform for material innovation.",
            "venue": "Journal of the American Chemical Society",
            "paperUrl": "https://www.semanticscholar.org/paper/7c20f04a178c601c546af3c9ac2193aed605122f",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1021/jacs.5c05916",
            "reason_for_inclusion": "High quality: Published in Journal of the American Chemical Society, a top-tier journal."
        }
    ],
    "benchmarks": [
        {
            "year": "2025.06",
            "title": "Assessing Large Language Model Utility and Limitations in Diabetes Education: A Cross-Sectional Study of Patient Interactions and Specialist Evaluations",
            "team": "Muhammad Qamar Masood",
            "team website": "",
            "affiliation": "",
            "domain": "LLM utility in diabetes education",
            "abstract": "",
            "venue": "",
            "paperUrl": "https://www.semanticscholar.org/paper/62f305b1498aa05adcb3a87a1a7506ca0c747ee4",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1101/2025.06.24.25329401",
            "reason_for_inclusion": "Included to meet target filter limit."
        },
        {
            "year": "2025.06",
            "title": "RWESummary: A Framework and Test for Choosing Large Language Models to Summarize Real-World Evidence (RWE) Studies",
            "team": "Neil M. Sanghavi",
            "team website": "",
            "affiliation": "",
            "domain": "Benchmark for RWE study summarization",
            "abstract": "Large Language Models (LLMs) have been extensively evaluated for general summarization tasks as well as medical research assistance, but they have not been specifically evaluated for the task of summarizing real-world evidence (RWE) from structured output of RWE studies. We introduce RWESummary, a proposed addition to the MedHELM framework (Bedi, Cui, Fuentes, Unell et al., 2025) to enable benchmarking of LLMs for this task. RWESummary includes one scenario and three evaluations covering major types of errors observed in summarization of medical research studies and was developed using Atropos Health proprietary data. Additionally, we use RWESummary to compare the performance of different LLMs in our internal RWE summarization tool. At the time of publication, with 13 distinct RWE studies, we found the Gemini 2.5 models performed best overall (both Flash and Pro). We suggest RWESummary as a novel and useful foundation model benchmark for real-world evidence study summarization.",
            "venue": "ArXiv",
            "paperUrl": "https://www.semanticscholar.org/paper/015df5d2d511eff0708d01a307b78e4bf2b06a51",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.48550/arXiv.2506.18819",
            "reason_for_inclusion": "Included to meet target filter limit."
        },
        {
            "year": "2025.06",
            "title": "Assessing the System-Instruction Vulnerabilities of Large Language Models to Malicious Conversion Into Health Disinformation Chatbots.",
            "team": "Ashley M Hopkins",
            "team website": "",
            "affiliation": "",
            "domain": "LLM vulnerabilities to health disinformation",
            "abstract": "Large language models (LLMs) offer substantial promise for improving health care; however, some risks warrant evaluation and discussion. This study assessed the effectiveness of safeguards in foundational LLMs against malicious instruction into health disinformation chatbots. Five foundational LLMs-OpenAI's GPT-4o, Google's Gemini 1.5 Pro, Anthropic's Claude 3.5 Sonnet, Meta's Llama 3.2-90B Vision, and xAI's Grok Beta-were evaluated via their application programming interfaces (APIs). Each API received system-level instructions to produce incorrect responses to health queries, delivered in a formal, authoritative, convincing, and scientific tone. Ten health questions were posed to each customized chatbot in duplicate. Exploratory analyses assessed the feasibility of creating a customized generative pretrained transformer (GPT) within the OpenAI GPT Store and searched to identify if any publicly accessible GPTs in the store seemed to respond with disinformation. Of the 100 health queries posed across the 5 customized LLM API chatbots, 88 (88%) responses were health disinformation. Four of the 5 chatbots (GPT-4o, Gemini 1.5 Pro, Llama 3.2-90B Vision, and Grok Beta) generated disinformation in 100% (20 of 20) of their responses, whereas Claude 3.5 Sonnet responded with disinformation in 40% (8 of 20). The disinformation included claimed vaccine-autism links, HIV being airborne, cancer-curing diets, sunscreen risks, genetically modified organism conspiracies, attention deficit-hyperactivity disorder and depression myths, garlic replacing antibiotics, and 5G causing infertility. Exploratory analyses further showed that the OpenAI GPT Store could currently be instructed to generate similar disinformation. Overall, LLM APIs and the OpenAI GPT Store were shown to be vulnerable to malicious system-level instructions to covertly create health disinformation chatbots. These findings highlight the urgent need for robust output screening safeguards to ensure public health safety in an era of rapidly evolving technologies.",
            "venue": "Annals of internal medicine",
            "paperUrl": "https://www.semanticscholar.org/paper/e6e248b199030a19fadf3e82059dc49989c2c689",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.7326/ANNALS-24-03933",
            "reason_for_inclusion": "High quality: Published in Annals of Internal Medicine, a top-tier journal."
        },
        {
            "year": "2025.06",
            "title": "Accelerating Drug Repurposing with AI: The Role of Large Language Models in Hypothesis Validation",
            "team": "Alejandro Rodr\u00edguez-Gonz\u00e1lez",
            "team website": "",
            "affiliation": "Universidad Politecnica de Madrid",
            "domain": "LLMs for drug repurposing validation",
            "abstract": "Drug repurposing accelerates drug discovery by identifying new therapeutic uses for existing drugs, but validating computational predictions remains a challenge. Large Language Models (LLMs) offer a potential solution by analyzing biomedical literature to assess drug-disease associations. This study evaluates four LLMs (GPT-4o, Claude-3, Gemini-2, and DeepSeek) using ten prompt strategies to validate repurposing hypotheses. The best-performing prompts and models were tested on 30 pathway-based cases and 10 benchmark cases. Results show that structured prompts enhance LLM accuracy, with GPT-4o and DeepSeek emerging as the most reliable models. Benchmark cases achieved significantly higher accuracy, precision, and F1-score (p < 0.001), while recall remained consistent across datasets. These findings highlight LLMs\u2019 potential in drug repurposing validation while emphasizing the need for structured prompts and human oversight.",
            "venue": "bioRxiv",
            "paperUrl": "https://www.semanticscholar.org/paper/c0c5ed5e17e82150267cabca37f3d0822cf1ecd3",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1101/2025.06.13.659527",
            "reason_for_inclusion": "High relevance: Directly matches user's research interests."
        }
    ],
    "reviews": [
        {
            "year": "2025.06",
            "title": "Evaluation Strategies for Large Language Model-Based Models in Exercise and Health Coaching: Scoping Review",
            "team": "Caihua Huang",
            "team website": "",
            "affiliation": "",
            "domain": "Evaluation strategies for LLM health coaching",
            "abstract": "Abstract Background Large language model (LLM)-based artificial intelligence (AI) coaches show promise for personalized exercise and health interventions. However, the unique demands of ensuring safety and real-time, multimodal personalized feedback have created a fragmented evaluation landscape lacking standardized frameworks. Objective This scoping review systematically maps current evaluation strategies for LLM-based AI coaches in exercise and health, identifies strengths and limitations, and proposes directions for robust, standardized validation. Methods Following PRISMA-ScR (Preferred Reporting Items for Systematic reviews and Meta-Analyses extension for Scoping Reviews) guidelines, we conducted a systematic search across 6 major databases (eg, PubMed, Web of Science) for original research on LLM-based exercise and health coaching. Studies were included if they explicitly reported on evaluation methods. We extracted and synthesized data on model types, application domains, and evaluation strategies and developed a 5-point Evaluation Rigor Score (ERS) to quantitatively assess the methodological depth of the evaluation designs. Results We included 20 studies, most using proprietary models like ChatGPT (75%). Evaluation strategies were highly heterogeneous, mixing human ratings (80%) and automated metrics (40%). Crucially, the evidence was limited by low methodological rigor: the median ERS was 2.5 out of 5, with 55% of studies classified as having low rigor. Key gaps included limited use of real-world data (40%) and inconsistent reliability reporting (45%). Conclusions The current evaluation of LLM-based health coaches is fragmented and methodologically weak. Future work must establish multidimensional validation frameworks that integrate technical benchmarks with human-centered methods to ensure safe, effective, and equitable deployment.",
            "venue": "Journal of Medical Internet Research",
            "paperUrl": "https://www.semanticscholar.org/paper/196faacf205c998095ac01963c78fd42108b8ba2",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.2196/79217",
            "reason_for_inclusion": "Included to meet target filter limit."
        },
        {
            "year": "2025.06",
            "title": "Large Language Models in Medical Chatbots: Opportunities, Challenges, and the Need to Address AI Risks",
            "team": "Kay Li",
            "team website": "",
            "affiliation": "",
            "domain": "LLMs in medical chatbots review",
            "abstract": "Large language models (LLMs) are transforming the capabilities of medical chatbots by enabling more context-aware, human-like interactions. This review presents a comprehensive analysis of their applications, technical foundations, benefits, challenges, and future directions in healthcare. LLMs are increasingly used in patient-facing roles, such as symptom checking, health information delivery, and mental health support, as well as in clinician-facing applications, including documentation, decision support, and education. However, as a study from 2024 warns, there is a need to manage \u201cextreme AI risks amid rapid progress\u201d. We examine transformer-based architectures, fine-tuning strategies, and evaluation benchmarks specific to medical domains to identify their potential to transfer and mitigate AI risks when using LLMs in medical chatbots. While LLMs offer advantages in scalability, personalization, and 24/7 accessibility, their deployment in healthcare also raises critical concerns. These include hallucinations (the generation of factually incorrect or misleading content by an AI model), algorithmic biases, privacy risks, and a lack of regulatory clarity. Ethical and legal challenges, such as accountability, explainability, and liability, remain unresolved. Importantly, this review integrates broader insights on AI safety, drawing attention to the systemic risks associated with rapid LLM deployment. As highlighted in recent policy research, including work on managing extreme AI risks, there is an urgent need for governance frameworks that extend beyond technical reliability to include societal oversight and long-term alignment. We advocate for responsible innovation and sustained collaboration among clinicians, developers, ethicists, and regulators to ensure that LLM-powered medical chatbots are deployed safely, equitably, and transparently within healthcare systems.",
            "venue": "Information",
            "paperUrl": "https://www.semanticscholar.org/paper/fc478dc7c736d60a91c7b51501f1a9ac822d347b",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.3390/info16070549",
            "reason_for_inclusion": "Included to meet target filter limit."
        }
    ],
    "foundation-models": [
        {
            "year": "2025.06",
            "title": "Unifying Biomedical Vision-Language Expertise: Towards a Generalist Foundation Model via Multi-CLIP Knowledge Distillation",
            "team": "Xiaofeng Yang",
            "team website": "",
            "affiliation": "",
            "domain": "Biomedical vision-language foundation model",
            "abstract": "CLIP models pretrained on natural images with billion-scale image-text pairs have demonstrated impressive capabilities in zero-shot classification, cross-modal retrieval, and open-ended visual answering. However, transferring this success to biomedicine is hindered by the scarcity of large-scale biomedical image-text corpora, the heterogeneity of image modalities, and fragmented data standards across institutions. These limitations hinder the development of a unified and generalizable biomedical foundation model trained from scratch. To overcome this, we introduce MMKD-CLIP, a generalist biomedical foundation model developed via Multiple Medical CLIP Knowledge Distillation. Rather than relying on billion-scale raw data, MMKD-CLIP distills knowledge from nine state-of-the-art domain-specific or generalist biomedical CLIP models, each pretrained on millions of biomedical image-text pairs. Our two-stage training pipeline first performs CLIP-style pretraining on over 2.9 million biomedical image-text pairs from 26 image modalities, followed by feature-level distillation using over 19.2 million feature pairs extracted from teacher models. We evaluate MMKD-CLIP on 58 diverse biomedical datasets, encompassing over 10.8 million biomedical images across nine image modalities. The evaluation spans six core task types: zero-shot classification, linear probing, cross-modal retrieval, visual question answering, survival prediction, and cancer diagnosis. MMKD-CLIP consistently outperforms all teacher models while demonstrating remarkable robustness and generalization across image domains and task settings. These results underscore that multi-teacher knowledge distillation is a scalable and effective paradigm for building high-performing biomedical foundation models under the practical constraints of real-world data availability.",
            "venue": "ArXiv",
            "paperUrl": "https://www.semanticscholar.org/paper/9696df55f59cf9978befe9c14a1401e793152837",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.48550/arXiv.2506.22567",
            "reason_for_inclusion": "Included to meet target filter limit."
        },
        {
            "year": "2025.06",
            "title": "Cell-GraphCompass: modeling single cells with graph structure foundation model",
            "team": "Xin Li",
            "team website": "",
            "affiliation": "",
            "domain": "Graph-based foundation model for single-cell analysis",
            "abstract": "ABSTRACT Cells in the human body are regulated by sophisticated networks of gene regulation, which allows them to fulfill their cellular destiny and function. Inspired by the advancements in large language models, there have been several attempts focusing on constructing foundation models with single-cell transcriptomic data to decipher gene regulatory networks. However, these models tend to impose a sequential structure on genes within each cell, which may omit intrinsic biological characteristics and lack the utilization of other available prior knowledge. In this paper, we introduce Cell-GraphCompass (CGCompass), the pioneering foundation model that employs graph pre-training to model genes and cells. We use three types of gene-related information as node features for constructing cell graphs and collect data from three perspectives depicting relationships between genes as edge features. We pre-trained the model with over 50 million human cells and then fine-tuned it to a broad spectrum of tasks, such as batch integration, cell type annotation, single-cell gene perturbation and in silico gene knockout predictions, achieving commendable performance. Overall, CGCompass provides a practical architecture for leveraging graph pre-training to incorporate prior knowledge in constructing a foundation model for single-cell analysis.",
            "venue": "National Science Review",
            "paperUrl": "https://www.semanticscholar.org/paper/919241b7fb72b8312861472417544f2d60ab04a2",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1093/nsr/nwaf255",
            "reason_for_inclusion": "High quality: Published in National Science Review, a top-tier journal."
        },
        {
            "year": "2025.06",
            "title": "MetaIndux-TS: Frequency-Aware AIGC Foundation Model for Industrial Time Series",
            "team": "Yuqing Wang",
            "team website": "",
            "affiliation": "",
            "domain": "Industrial time-series generative foundation model",
            "abstract": "Implementing advanced AI techniques in industrial manufacturing requires large volumes of annotated sensor data. Unfortunately, collecting such data is often impractical due to extreme environments and the manual burden of expert annotation. Recent advancements in artificial intelligence generated content (AIGC) have inspired the exploration of industrial time-series generation to mitigate data shortages. However, existing AIGC models encounter difficulties in generating industrial time series due to their complex temporal dynamics, multichannel intercolumn correlations, and diverse frequency characteristics. To address these challenges, we propose MetaIndux-TS, a frequency-informed AIGC foundation model based on diffusion model frameworks. This model is designed to generate industrial time-series data under a variety of working conditions, across different types of equipment, and with variable lengths. Specifically, MetaIndux-TS integrates dual-frequency cross-attention networks, transforming time series into the frequency domain to model multivariate dependencies and capture intricate temporal details. In addition, the contrastive synthesis layer is constructed to generate high-fidelity time series by comparing periodic and long-term trends with initial noisy sequences. Comprehensive experiments show that MetaIndux-TS outperforms state-of-the-art models (SSSD, Dit, and TabDDPM), achieving a 57.5% improvement in fidelity and 20.4% in predictive score. MetaIndux-TS exhibits zero-shot generation capabilities for samples under unseen conditions, offering the potential to address data collection challenges in extreme environments. Codes are available at: https://github.com/Dolphin-wang/MetaIndux",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
            "paperUrl": "https://www.semanticscholar.org/paper/530542ab7d376b52b90d83d0379afa489ebfa620",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1109/TNNLS.2025.3577203",
            "reason_for_inclusion": "High quality: Published in IEEE Transactions on Neural Networks and Learning Systems, a top-tier journal."
        }
    ],
    "databases": [
        {
            "year": "2025.06",
            "title": "\"What's Up, Doc?\": Analyzing How Users Seek Health Information in Large-Scale Conversational AI Datasets",
            "team": "Monica Agrawal",
            "team website": "",
            "affiliation": "",
            "domain": "HealthChat-11K conversational dataset",
            "abstract": "People are increasingly seeking healthcare information from large language models (LLMs) via interactive chatbots, yet the nature and inherent risks of these conversations remain largely unexplored. In this paper, we filter large-scale conversational AI datasets to achieve HealthChat-11K, a curated dataset of 11K real-world conversations composed of 25K user messages. We use HealthChat-11K and a clinician-driven taxonomy for how users interact with LLMs when seeking healthcare information in order to systematically study user interactions across 21 distinct health specialties. Our analysis reveals insights into the nature of how and why users seek health information, such as common interactions, instances of incomplete context, affective behaviors, and interactions (e.g., leading questions) that can induce sycophancy, underscoring the need for improvements in the healthcare support capabilities of LLMs deployed as conversational AI. Code and artifacts to retrieve our analyses and combine them into a curated dataset can be found here: https://github.com/yahskapar/HealthChat",
            "venue": "ArXiv",
            "paperUrl": "https://www.semanticscholar.org/paper/313a406ceb1c06d71cab6cd896ecf845ad195b12",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.48550/arXiv.2506.21532",
            "reason_for_inclusion": "Included to meet target filter limit."
        }
    ]
}