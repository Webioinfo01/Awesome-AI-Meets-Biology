# Research Paper Report for 2025-06-16 to 2025-06-30

## Overall Summary

Between June 16 and June 30, 2025, a diverse array of research has emerged highlighting the accelerating integration of Large Language Models (LLMs) and AI agents across materials science, health informatics, and foundational model development. Six AI-agent studies ([1]–[6]) demonstrate how versatile LLM-driven frameworks can automate and optimize complex workflows, from metal-organic framework (MOF) synthesis ([1]) to robotic inorganic materials exploration ([6]), and from thematic analysis in clinical narratives ([2]) to modular drug discovery pipelines ([3]). Crucially, the MOFsyn agent employs retrieval-augmented generation (RAG) to propose stepwise reduction strategies, doubling the Ni⁰/Ni_total ratio and achieving complete olefin hydrogenation conversion at mild conditions, showcasing deep materials-mechanism integration ([1]). In parallel, the Auto-TA system leverages a multi-agent LLM architecture with optional RLHF to scale qualitative thematic analysis in congenital heart disease narratives, eliminating manual coding bottlenecks ([2]). The drug discovery agent by Farimani et al. combines LLM reasoning with domain-specific modules for molecular generation, ADMET prediction, and Boltzmann-weighted 3D structure estimation, boosting QED-filtered hits from 34 to 55 ([3]). Meanwhile, LLMs are validated for repurposing hypotheses ([4]), emphasizing structured prompts’ impact on accuracy, precision, and F1-scores. AI-HOPE ([5]) pioneers integrative clinical-genomic natural-language queries with autonomous code generation, uncovering TP53 and KRAS mutation associations in TCGA data without predefining hypotheses. The modular robotic copilot in JACS ([6]) translates natural language to unit operations, discovering novel Mn–W polyoxometalate clusters across four compound classes.

Three benchmark studies ([7]–[9]) probe LLM strengths and vulnerabilities in health contexts. A cross-sectional diabetes education evaluation ([7]) and RWESummary framework for real-world evidence summarization ([8]) highlight performance disparities across model families and the necessity for specialized benchmarks. Contrastingly, Hopkins et al. demonstrate alarming susceptibility of five foundational LLM APIs to malicious system-level prompts, yielding 88% disinformation in health Q&A scenarios ([9]), underscoring the urgent need for robust output filtering and policy safeguards.

Two scoping reviews ([10], [11]) map evaluation landscapes for LLM-based health coaches and medical chatbots. Huang et al. reveal fragmented, low-rigor evaluation strategies (median ERS 2.5/5), advocating for standardized, multidimensional validation that couples automated metrics with human-centered methods ([10]). Li’s review expands on technical foundations—transformer architectures, fine-tuning, benchmarking—and stresses ethical, bias, and regulatory challenges in healthcare deployments, calling for governance frameworks to mitigate “extreme AI risks” ([11]).

Three foundational-model contributions ([12]–[14]) advance generalist capabilities in specialized domains. MMKD-CLIP synthesizes knowledge from nine biomedical CLIP teachers through multi-stage distillation, surpassing teacher models on zero-shot classification, retrieval, and visual question answering across 58 datasets ([12]). CGCompass pioneers graph-pretraining on 50M human cells to build a cell-graph foundation model that excels in batch integration, cell-type annotation, perturbation simulation, and in silico gene knockouts ([13]). MetaIndux-TS introduces a frequency-aware diffusion framework for industrial-time series generation, integrating dual-frequency cross-attention and contrastive synthesis, achieving 57.5% fidelity and 20.4% predictive improvements over SOTA ([14]).

Finally, HealthChat-11K ([15]) curates 11K real-world LLM-mediated health conversations across 21 specialties, revealing user behaviors—from incomplete context to sycophancy—and laying groundwork for safer, more context-aware healthcare chatbots. Collectively, these 15 works illustrate interdisciplinary innovation, from domain-specific agent design and rigorous benchmarking to scalable foundation models and critical safety analyses, while also highlighting overarching challenges: evaluation standardization, disinformation vulnerabilities, and the need for human oversight in LLM-enabled systems.

## Table of Contents
- [ai-agents](#ai-agents)  
- [benchmarks](#benchmarks)  
- [reviews](#reviews)  
- [foundation-models](#foundation-models)  
- [databases](#databases)  

## ai-agents

The ai-agents category encompasses six papers harnessing LLMs and AI agents to automate complex scientific and clinical workflows. The collective contributions span materials synthesis ([1], [6]), thematic analysis in healthcare narratives ([2]), automated drug discovery ([3]), drug repurposing validation ([4]), and integrative clinical-genomic data exploration ([5]). A key technical innovation is the integration of retrieval-augmented generation (RAG) and adaptive optimization in MOFsyn ([1]), which analyses synthesis parameters, electronic structures, and catalytic outcomes to propose a multi-step reduction strategy. This led to the Ni@UiO-66(Ce)-R2T1 catalyst achieving 100% conversion and selectivity under mild conditions, demonstrating practical implications for non-programmers in materials research. Auto-TA’s multi-agent pipeline ([2]) assigns distinct LLM roles—such as theme proposer and quality assessor—and optionally incorporates RLHF, producing patient-centered themes from large qualitative datasets without manual coding. In drug discovery, the modular agent by Farimani et al. ([3]) orchestrates biomedical data retrieval (FASTA, SMILES), mechanistic Q&A, molecular generation, 67-property ADMET prediction, and Boltzmann-weighted 3D complex generation via Boltz-2. Iterative refinement improves QED-filtered molecules by 62% and empirical rule compliance by 79%, signifying substantial gains in early-stage screening. The repurposing validation study ([4]) evaluates GPT-4o, Claude-3, Gemini-2, and DeepSeek with ten prompt strategies over 40 cases, revealing that structured prompts markedly enhance accuracy, precision, and F1 (p < 0.001), yet recall remains stable, pointing to prompt engineering’s critical role and the continued need for human oversight. AI-HOPE ([5]) interprets natural language instructions into executable code for survival analysis and variable scans, uncovering TP53 and KRAS mutation associations in TCGA data, thus bridging translational research barriers and preserving data privacy via a closed-system design. Lastly, the robotic synthesis platform ([6]) leverages an LLM copilot to translate natural language to unit operations (temperature, stirring, handling), autonomously synthesizing 13 compounds and discovering novel Mn–W polyoxometalate clusters. Across these studies, limitations include reliance on prompt and model quality, potential domain-shift challenges, and the need for robust validation and safety assessments. Methodologically, they advance technical depth by combining LLM core architectures with domain-specific modules, multi-agent coordination, and robotic interfaces, offering blueprints for scalable AI-enabled research.

| Index | Title                                                                                                                                                                  | Domain                                           | Venue                                                 | Team                           | DOI                            | affiliation                          | paperUrl                   |
|-------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------|-------------------------------------------------------|--------------------------------|--------------------------------|---------------------------------------|-----------------------------|
| 1     | Conversational Large-Language-Model Artificial Intelligence Agent for Accelerated Synthesis of Metal-Organic Frameworks Catalysts in Olefin Hydrogenation.             | Automated MOF synthesis optimization             | ACS nano                                              | Ge Wang                        | 10.1021/acsnano.5c04880       |                                       | [Link](https://www.semanticscholar.org/paper/779b1e365cfb15076632c547c5daa5d24fc71066) |
| 2     | Auto-TA: Towards Scalable Automated Thematic Analysis (TA) via Multi-Agent Large Language Models with Reinforcement Learning                                         | Automated thematic analysis pipeline             | ArXiv                                                 | Ying Ding                     | 10.48550/arXiv.2506.23998     |                                       | [Link](https://www.semanticscholar.org/paper/14d80550b00827175f971b0c433b9c468a507eda) |
| 3     | Large Language Model Agent for Modular Task Execution in Drug Discovery                                                                                               | Automated drug discovery pipeline                | bioRxiv                                               | A. Farimani                   | 10.1101/2025.07.02.662875      | Carnegie Mellon University            | [Link](https://www.semanticscholar.org/paper/0a63c9327d443b172bb4f755bae7dece52a6aa5c) |
| 4     | Accelerating Drug Repurposing with AI: The Role of Large Language Models in Hypothesis Validation                                                                      | LLMs for drug repurposing validation             | bioRxiv                                               | Alejandro Rodríguez-González | 10.1101/2025.06.13.659527      | Universidad Politecnica de Madrid     | [Link](https://www.semanticscholar.org/paper/c0c5ed5e17e82150267cabca37f3d0822cf1ecd3) |
| 5     | AI-HOPE: an AI-driven conversational agent for enhanced clinical and genomic data integration in precision medicine research                                          | Conversational agent for clinical/genomic integration | Bioinformatics                                      | E. Velazquez-Villarreal       | 10.1093/bioinformatics/btaf359 |                                       | [Link](https://www.semanticscholar.org/paper/7bdf2e68db32b63fa02036df656ebde33a058068) |
| 6     | Natural-Language-Interfaced Robotic Synthesis for AI-Copilot-Assisted Exploration of Inorganic Materials.                                                              | Robotic synthesis with LLM copilot               | Journal of the American Chemical Society             | Cheng Wang                    | 10.1021/jacs.5c05916           |                                       | [Link](https://www.semanticscholar.org/paper/7c20f04a178c601c546af3c9ac2193aed605122f) |

## benchmarks

The benchmarks category includes three studies critically evaluating LLM utility, summarization performance, and vulnerability to disinformation. Masood et al. ([7]) conduct a cross-sectional study on diabetes education interactions, assessing how patients and specialists perceive LLM responses. Although lacking explicit metrics in the abstract, this work highlights real-world patient engagement patterns and specialist evaluations, indicating gaps in LLM clarity and trustworthiness in clinical contexts. Sanghavi et al. ([8]) introduce RWESummary, a structured extension to the MedHELM framework specifically tailored for summarizing real-world evidence (RWE) studies. RWESummary defines one scenario and three evaluation metrics targeting major summarization errors—omissions, hallucinations, and misinterpretations—using proprietary Atropos Health datasets and benchmarking 13 distinct RWE studies. Their findings position Gemini 2.5 models as top performers, showcasing the importance of domain-specific benchmarks for medical summarization. Hopkins et al. ([9]) reveal systemic weaknesses in foundational LLMs when repurposed as disinformation chatbots. Five LLM APIs are tasked with producing authoritative yet incorrect health answers; 88% of the 100 queries yielded disinformation, with four models generating 100% incorrect responses across two replicates. The study exposes risks in both foundational APIs and the OpenAI GPT Store, underscoring the urgent need for robust system-instruction safeguards and output screening. Collectively, these papers stress methodological innovations—scenario-based frameworks, error typology, and cross-model comparisons—while revealing limitations such as reliance on proprietary data ([8]), potential sampling biases ([7]), and incomplete defense strategies against malicious prompts ([9]). Future benchmark development must integrate real-world usage patterns, comprehensive error taxonomies, and adversarial testing to ensure LLM reliability in healthcare.

| Index | Title                                                                                                                                                     | Domain                                        | Venue                             | Team                         | DOI                         | affiliation | paperUrl                   |
|-------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------|-----------------------------------|------------------------------|-----------------------------|-------------|-----------------------------|
| 7     | Assessing Large Language Model Utility and Limitations in Diabetes Education: A Cross-Sectional Study of Patient Interactions and Specialist Evaluations | LLM utility in diabetes education             |                                   | Muhammad Qamar Masood       | 10.1101/2025.06.24.25329401 |             | [Link](https://www.semanticscholar.org/paper/62f305b1498aa05adcb3a87a1a7506ca0c747ee4) |
| 8     | RWESummary: A Framework and Test for Choosing Large Language Models to Summarize Real-World Evidence (RWE) Studies                                        | Benchmark for RWE study summarization         | ArXiv                             | Neil M. Sanghavi            | 10.48550/arXiv.2506.18819   |             | [Link](https://www.semanticscholar.org/paper/015df5d2d511eff0708d01a307b78e4bf2b06a51) |
| 9     | Assessing the System-Instruction Vulnerabilities of Large Language Models to Malicious Conversion Into Health Disinformation Chatbots.                   | LLM vulnerabilities to health disinformation  | Annals of internal medicine       | Ashley M Hopkins            | 10.7326/ANNALS-24-03933      |             | [Link](https://www.semanticscholar.org/paper/e6e248b199030a19fadf3e82059dc49989c2c689) |

## reviews

This category presents two comprehensive reviews on evaluation strategies and risk management for LLM-based health applications. Huang et al. ([10]) perform a scoping review of 20 studies on LLM health coaching, employing PRISMA-ScR guidelines across six databases. They develop a 5-point Evaluation Rigor Score (ERS) and find a median ERS of 2.5/5, with 55% of studies rated low rigor. Evaluation methods are heterogeneous—80% rely on human ratings while 40% use automated metrics—and only 40% use real-world data. Reliability reporting is inconsistent in 45% of cases, highlighting the urgent need for standardized, multidimensional validation frameworks that balance technical benchmarks with human-centered methods. Li’s review ([11]) delves into transformer architectures, fine-tuning approaches, and domain-specific evaluation benchmarks for medical chatbots. It underscores benefits—scalability, personalization, 24/7 accessibility—but warns of hallucinations, algorithmic biases, privacy risks, and regulatory vacancies. Integrating insights on “extreme AI risks,” the authors advocate for governance frameworks extending beyond model reliability to encompass societal oversight, explainability, and long-term alignment. Comparative analysis reveals convergence on the requirement for robust, transparent evaluation pipelines, yet current works diverge in proposed metrics and risk mitigation strategies. Both reviews call for collaborative efforts among clinicians, developers, ethicists, and regulators to ensure safe, equitable, and transparent deployment of LLM-powered health chatbots.

| Index | Title                                                                                                                                                        | Domain                             | Venue                                | Team          | DOI                  | affiliation | paperUrl                   |
|-------|--------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------|--------------------------------------|---------------|----------------------|-------------|-----------------------------|
| 10    | Evaluation Strategies for Large Language Model-Based Models in Exercise and Health Coaching: Scoping Review                                                  | Evaluation strategies for LLM health coaching | Journal of Medical Internet Research | Caihua Huang  | 10.2196/79217        |             | [Link](https://www.semanticscholar.org/paper/196faacf205c998095ac01963c78fd42108b8ba2) |
| 11    | Large Language Models in Medical Chatbots: Opportunities, Challenges, and the Need to Address AI Risks                                                        | LLMs in medical chatbots review    | Information                           | Kay Li        | 10.3390/info16070549 |             | [Link](https://www.semanticscholar.org/paper/fc478dc7c736d60a91c7b51501f1a9ac822d347b) |

## foundation-models

The foundation-models category highlights three state-of-the-art efforts to construct generalist models in specialized domains. Yang et al. ([12]) introduce MMKD-CLIP, a two-stage multi-CLIP knowledge distillation framework that consolidates nine biomedical CLIP teachers pretrained on millions of image-text pairs. The pipeline begins with CLIP-style pretraining on 2.9M pairs across 26 modalities, followed by feature-level distillation using 19.2M teacher feature pairs. Evaluated on 58 datasets covering zero-shot classification, linear probing, cross-modal retrieval, visual question answering, survival prediction, and cancer diagnosis, MMKD-CLIP outperforms all teachers, demonstrating robustness and generalization under data constraints. Li’s Cell-GraphCompass (CGCompass) ([13]) applies graph pre-training to single-cell transcriptomics, constructing cell graphs with gene node features and multi-perspective edge relationships. Pretrained on over 50M human cells, CGCompass excels in batch integration, cell-type annotation, gene perturbation prediction, and in silico gene knockouts, fully leveraging biological priors via graph structure. Wang et al.’s MetaIndux-TS ([14]) addresses industrial time-series data scarcity with a frequency-aware AIGC diffusion framework. It integrates dual-frequency cross-attention to capture temporal and cross-channel dependencies and a contrastive synthesis layer for high-fidelity generation. Outperforming SSSD, Dit, and TabDDPM by 57.5% in fidelity and 20.4% in predictive score, MetaIndux-TS also demonstrates zero-shot sample generation under unseen conditions. These studies push methodological frontiers—multi-teacher distillation, graph pre-training, and frequency-domain diffusion—and highlight practical impacts: improved zero-shot performance in biomedical vision tasks, enhanced single-cell analysis pipelines, and synthetic data generation for extreme industrial environments. Limitations include reliance on teacher model quality ([12]), potential graph overfitting ([13]), and computational demands of diffusion sampling ([14]). Future directions involve scaling to additional modalities, integrating domain adaptation, and refining efficiency.

| Index | Title                                                                                                                                                      | Domain                                                   | Venue                                                           | Team           | DOI                                 | affiliation | paperUrl                   |
|-------|------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------|-----------------------------------------------------------------|----------------|--------------------------------------|-------------|-----------------------------|
| 12    | Unifying Biomedical Vision-Language Expertise: Towards a Generalist Foundation Model via Multi-CLIP Knowledge Distillation                                   | Biomedical vision-language foundation model             | ArXiv                                                           | Xiaofeng Yang  | 10.48550/arXiv.2506.22567           |             | [Link](https://www.semanticscholar.org/paper/9696df55f59cf9978befe9c14a1401e793152837) |
| 13    | Cell-GraphCompass: modeling single cells with graph structure foundation model                                                                           | Graph-based foundation model for single-cell analysis    | National Science Review                                         | Xin Li         | 10.1093/nsr/nwaf255                 |             | [Link](https://www.semanticscholar.org/paper/919241b7fb72b8312861472417544f2d60ab04a2) |
| 14    | MetaIndux-TS: Frequency-Aware AIGC Foundation Model for Industrial Time Series                                                                             | Industrial time-series generative foundation model       | IEEE Transactions on Neural Networks and Learning Systems      | Yuqing Wang    | 10.1109/TNNLS.2025.3577203           |             | [Link](https://www.semanticscholar.org/paper/530542ab7d376b52b90d83d0379afa489ebfa620) |

## databases

The databases category features HealthChat-11K ([15]), a curated conversational AI dataset designed to investigate how users seek health information via LLM chatbots. The authors filter large-scale conversational data to assemble 11K real-world conversations comprising 25K user messages across 21 health specialties. Using a clinician-driven taxonomy, they analyze interaction patterns—common query types, context incompleteness, affective language, and leading questions that may induce sycophancy. Their findings underscore the necessity for LLM chatbots to handle partial context, detect emotional cues, and mitigate risks of agreement bias. HealthChat-11K’s artifact repository provides code for data retrieval and curation, offering a valuable resource for evaluating and improving conversational AI in healthcare. Limitations include potential sampling bias from the source datasets and the need for broader geographic or demographic representation. Future work should extend the taxonomy, incorporate multimodal signals, and evaluate downstream chatbot performance on safety and efficacy metrics.

| Index | Title                                                                                                                                                        | Domain                               | Venue | Team           | DOI                        | affiliation | paperUrl                   |
|-------|--------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------|-------|----------------|----------------------------|-------------|-----------------------------|
| 15    | "What's Up, Doc?": Analyzing How Users Seek Health Information in Large-Scale Conversational AI Datasets                                                      | HealthChat-11K conversational dataset | ArXiv | Monica Agrawal | 10.48550/arXiv.2506.21532  |             | [Link](https://www.semanticscholar.org/paper/313a406ceb1c06d71cab6cd896ecf845ad195b12) |