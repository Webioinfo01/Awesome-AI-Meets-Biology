{
    "ai-agents": [
        {
            "year": "2025.10",
            "title": "Feasibility and Acceptability of a Large Language Model-Based Motivational Interviewing Agent in the Emergency Department.",
            "team": "Vanessa Pitre",
            "team website": "",
            "affiliation": "",
            "domain": "Motivational interviewing LLM agent in emergency care",
            "abstract": "",
            "venue": "Annals of emergency medicine",
            "paperUrl": "https://www.semanticscholar.org/paper/949ace206622c4aa82bdb95d722f4fe365bda11d",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1016/j.annemergmed.2025.08.021",
            "reason_for_inclusion": "Included to meet target filter limit."
        },
        {
            "year": "2025.10",
            "title": "Human knowledge-enhanced large language model agent for prediction of intestinal disease progression in patients with Crohn\u2019s disease: A multicenter retrospective study",
            "team": "Xuehua Li",
            "team website": "",
            "affiliation": "",
            "domain": "Crohn\u2019s disease progression prediction with LLM agent",
            "abstract": "",
            "venue": "Meta-Radiology",
            "paperUrl": "https://www.semanticscholar.org/paper/6b3ade2ee392a3da6a6071177044201128b35b9c",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1016/j.metrad.2025.100186",
            "reason_for_inclusion": "Included to meet target filter limit."
        },
        {
            "year": "2025.10",
            "title": "Large language model enhanced framework for systematic reviews and meta-analyses",
            "team": "Jinge Wu",
            "team website": "",
            "affiliation": "",
            "domain": "LLM-enhanced framework for systematic reviews and meta-analyses",
            "abstract": "\n \n To evaluate and synthesise current applications of large language models (LLMs) in systematic reviews and meta-analyses (SRMAs), identify key limitations and propose an enhanced theoretical framework to improve the efficiency, scalability and reliability of evidence synthesis.\n \n \n \n We conducted a narrative review of recent studies applying LLMs across key SRMA stages. A total of 21 publications were analysed for model type, task application, accuracy metrics and workflow impact. Building on this evidence base, we designed a comprehensive LLM-enhanced SRMA framework that categorises LLM roles as consultants and assistants, integrates human-in-the-loop strategies and uses retrieval-augmented generation (RAG) and agent-based architectures to address critical challenges including hallucinations, bias and workflow inefficiency.\n \n \n \n The reviewed literature demonstrated that LLMs can support various SRMA tasks with reported accuracy ranging from 61% to 99%, showing particular promise in literature screening and data extraction. Our proposed framework conceptualises modular integration of LLMs across all six SRMA stages, with LLMs serving as consultants for research question formulation and search strategy development and as assistants for task automation including abstract screening and structured data extraction. The framework incorporates RAG technology to reduce hallucinations by grounding outputs in retrieved literature and employs agent-based orchestration for complex analytical workflows. Theoretical analysis suggests potential for significant efficiency gains while maintaining methodological rigour through strategic human oversight.\n \n \n \n LLMs offer substantial theoretical potential to transform evidence synthesis by improving efficiency, scalability and consistency across SRMA workflows. The proposed LLM-enhanced framework provides a systematic, theoretically grounded approach for integrating advanced artificial intelligence capabilities into existing SRMA methodologies while preserving essential human oversight and analytical integrity. Future empirical studies are needed to validate the framework\u2019s practical effectiveness, establish implementation protocols and demonstrate real-world benefits in evidence-based medicine.\n",
            "venue": "BMJ Digital Health &amp; AI",
            "paperUrl": "https://www.semanticscholar.org/paper/7543c4368d844cf0a9320e43630cd26b502b6ad1",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1136/bmjdhai-2025-000017",
            "reason_for_inclusion": "Included to meet target filter limit."
        },
        {
            "year": "2025.10",
            "title": "Large Language Model Agent for Managing Patients With Suspected Hypertension.",
            "team": "Jun Wang",
            "team website": "",
            "affiliation": "",
            "domain": "Hypertension management LLM agent",
            "abstract": "BACKGROUND\nThe effectiveness of Large Language Model agent frameworks for hypertension screening and personalized health management has not been fully studied. This study aimed to develop and evaluate a Large Language Model-based Agent, called the Cascade Framework, and assess its effectiveness in hypertension education and clinical decision support.\n\n\nMETHODS\nThe Cascade Framework was developed utilizing the Dify platform, and its performance was tested via a robust 2-phase evaluation protocol from August 2024 to June 2025. The first phase involved systematic performance benchmarking of 6 configurations: 3 foundational Large Language Models (Chat Generative Pretrained Transformer [ChatGPT]-4o, ChatGPT-4oMini, and DeepSeek-V3) and their respective Cascade-enhanced versions. The second phase included an external validation in a cohort of patients with suspected hypertension.\n\n\nRESULTS\nCascade integration yielded significant performance improvements across all models. For ChatGPT-4o, educational outcomes improved (Accuracy: 3.87\u21924.10, P=0.02; Comprehensiveness: 4.07\u21924.32, P=0.16; Credibility: 3.79\u21924.03, P<0.001; Understandability: 3.90\u21923.96, P=0.005; Emotional Support: 3.87\u21924.01, P<0.001). Blood pressure classification accuracy rose from 62.5% to 87.0% (P<0.001) and risk factor stratification from 60.4% to 98.6% (P<0.001). Clinical decision-making improved, with accuracy of 72.0% to 92.5%. A similar trend of performance improvement was observed in the external validation cohort, where the 4o-Cascade model achieved increases in blood pressure classification accuracy (58.9%\u219295.3%), risk stratification accuracy (71.0%\u219290.7%), and clinical decision appropriateness (66.4%\u219292.5%), all with P<0.001 and surpassing the performance of the 3 physicians.\n\n\nCONCLUSIONS\nCascade Framework can improve the management of hypertension. Its extensible architecture allows integration with existing clinical workflows while providing transparent reasoning pathways.",
            "venue": "Hypertension",
            "paperUrl": "https://www.semanticscholar.org/paper/52f1b628bd6246106058a359604bcfebad30ecc6",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1161/HYPERTENSIONAHA.125.25305",
            "reason_for_inclusion": "Included to meet target filter limit."
        },
        {
            "year": "2025.10",
            "title": "Dialogues with large language models reduce conspiracy beliefs even when the AI is perceived as human",
            "team": "Gordon Pennycook",
            "team website": "",
            "affiliation": "",
            "domain": "LLM dialogues for reducing conspiracy beliefs",
            "abstract": "Abstract Although conspiracy beliefs are often viewed as resistant to correction, recent evidence shows that personalized, fact-based dialogues with a large language model (LLM) can reduce them. Is this effect driven by the debunking facts and evidence, or does it rely on the messenger being an AI? In other words, would the same message be equally effective if delivered by a human? To answer this question, we conducted a preregistered experiment (N = 955) in which participants reported either a conspiracy belief or a nonconspiratorial but epistemically unwarranted belief and interacted with a LLM that argued against that belief using facts and evidence. We randomized whether the debunking LLM was characterized as an AI tool or a human expert and whether the model used human-like conversational tone. The conversations significantly reduced participants' confidence in both conspiracies and epistemically unwarranted beliefs, with no significant differences across conditions. Thus, AI persuasion is not reliant on the messenger being an AI model: it succeeds by generating compelling messages.",
            "venue": "PNAS Nexus",
            "paperUrl": "https://www.semanticscholar.org/paper/7d909a8b3c49ecf6962b80d686d790b5dbe257b4",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1093/pnasnexus/pgaf325",
            "reason_for_inclusion": "High quality: Published in PNAS Nexus, a top-tier journal."
        }
    ],
    "reviews": [
        {
            "year": "2025.10",
            "title": "Large Language Model Agents for Biomedicine: A Comprehensive Review of Methods, Evaluations, Challenges, and Future Directions",
            "team": "Ravi Sankar",
            "team website": "",
            "affiliation": "",
            "domain": "Comprehensive review of biomedical LLM agents",
            "abstract": "Large language model (LLM)-based agents are rapidly emerging as transformative tools across biomedical research and clinical applications. By integrating reasoning, planning, memory, and tool use capabilities, these agents go beyond static language models to operate autonomously or collaboratively within complex healthcare settings. This review provides a comprehensive survey of biomedical LLM agents, spanning their core system architectures, enabling methodologies, and real-world use cases such as clinical decision making, biomedical research automation, and patient simulation. We further examine emerging benchmarks designed to evaluate agent performance under dynamic, interactive, and multimodal conditions. In addition, we systematically analyze key challenges, including hallucinations, interpretability, tool reliability, data bias, and regulatory gaps, and discuss corresponding mitigation strategies. Finally, we outline future directions in areas such as continual learning, federated adaptation, robust multi-agent coordination, and human AI collaboration. This review aims to establish a foundational understanding of biomedical LLM agents and provide a forward-looking roadmap for building trustworthy, reliable, and clinically deployable intelligent systems.",
            "venue": "Information",
            "paperUrl": "https://www.semanticscholar.org/paper/aa3da305318799a02ab371318bd01678187ef080",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.3390/info16100894",
            "reason_for_inclusion": "Included to meet target filter limit."
        },
        {
            "year": "2025.10",
            "title": "Large language models to reduce antimicrobial resistance: ChatGPT, Claude and Gemini comparison",
            "team": "L. Villani",
            "team website": "",
            "affiliation": "",
            "domain": "Survey of user trust in healthcare LLMs in Saudi Arabia",
            "abstract": "Abstract Introduction Antimicrobial resistance (AMR) is a major public health challenge. Artificial Intelligence (AI), particularly Large Language Models (LLMs), offers a promising opportunity to deliver accurate and appropriate health information and education. However, the public health implications of their widespread use remain largely unassessed by scientific experts. This study evaluates the effectiveness of leading LLMs in providing information on infection control and antibiotic use. Methods ChatGPT 3.5, ChatGPT 4.0, Claude 2.0, and Gemini 1.0 were adequately prompted in both Italian and English. Their textual output underwent a Computational Text Analysis to assess readability, lexical diversity, and sentiment. In addition, 3 experts rated the output via an adapted DISCERN instrument built to assess AMR impact, persuasiveness, and the overall quality and appropriateness of the content. Results A total of 864 scores were obtained from ChatGPT 3.5, ChatGPT 4.0 and Claude, each evaluated both in English and in Italian. In contrast, only 270 scores were obtained from Gemini in English, as it self-interrupted, reporting the questioning as inappropriate for a chatbot. A general performance gradient was observed from Gemini to ChatGPT 3.5. ChatGPT 4.0 demonstrated the highest lexical diversity and sentiment scores, while Gemini presented the best readability and overall rating. English-based prompts consistently overperformed Italian-based ones. The impact on AMR received low scores across all LLMs. Conclusions The study identified Gemini as the best-performing model in terms of content quality, accessibility, and contextual awareness. While LLMs are promising tools, they are not intended to replace professional medical assessment. Instead, their responsible integration is necessary to ensure safe and effective public health applications. Further studies are warranted to expand the evidence base regarding the assessments of medical content generated by LLMs. Key messages \u2022\u2002Large Language Models (LLMs) are promising in delivering accurate and appropriate health information. However, the public health implications remain largely unassessed by scientific experts. \u2022\u2002A general performance gradient was observed from Gemini to ChatGPT 3.5 regarding readability, lexical diversity, sentiment scores and overall rating. The rated impact on AMR was low across all LLMs.",
            "venue": "The European Journal of Public Health",
            "paperUrl": "https://www.semanticscholar.org/paper/014a5845cf9a0c2cc2ed6d0c800a67a540094b80",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1093/eurpub/ckaf161.316",
            "reason_for_inclusion": "Included to meet target filter limit."
        },
        {
            "year": "2025.10",
            "title": "LLM4Cell: A Survey of Large Language and Agentic Models for Single-Cell Biology",
            "team": "Liqing Zhang",
            "team website": "",
            "affiliation": "",
            "domain": "Survey of LLM and agentic models for single-cell biology",
            "abstract": "Large language models (LLMs) and emerging agentic frameworks are beginning to transform single-cell biology by enabling natural-language reasoning, generative annotation, and multimodal data integration. However, progress remains fragmented across data modalities, architectures, and evaluation standards. LLM4Cell presents the first unified survey of 58 foundation and agentic models developed for single-cell research, spanning RNA, ATAC, multi-omic, and spatial modalities. We categorize these methods into five families-foundation, text-bridge, spatial, multimodal, epigenomic, and agentic-and map them to eight key analytical tasks including annotation, trajectory and perturbation modeling, and drug-response prediction. Drawing on over 40 public datasets, we analyze benchmark suitability, data diversity, and ethical or scalability constraints, and evaluate models across 10 domain dimensions covering biological grounding, multi-omics alignment, fairness, privacy, and explainability. By linking datasets, models, and evaluation domains, LLM4Cell provides the first integrated view of language-driven single-cell intelligence and outlines open challenges in interpretability, standardization, and trustworthy model development.",
            "venue": "ArXiv",
            "paperUrl": "https://www.semanticscholar.org/paper/fb2c169f42adb99f8f3c37e24e2eba98df65e952",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.48550/arXiv.2510.07793",
            "reason_for_inclusion": "High relevance: Directly matches user's research interests."
        }
    ],
    "foundation-models": [
        {
            "year": "2025.10",
            "title": "#3218 Predicting fluid overload hospitalizations in hemodialysis patients using a foundational model and large language model techniques",
            "team": "F. Bellocchio",
            "team website": "",
            "affiliation": "",
            "domain": "Fluid overload hospitalization prediction in hemodialysis with foundational LLM",
            "abstract": "\n \n \n Developing a foundational model for machine learning (ML) and data visualization in hemodialysis (HD) represents a critical step toward enhancing predictive analytics and patient care in nephrology. The idea is to create a general model capable of representing patient data in a more informative manner by encoding temporal and inter-variable dependencies. This study introduces an innovative approach that applies the Life2Vec model to pre-train dense and informative embeddings representing the full health history of patients, followed by multiple downstream predictions assessing the risk of occurrence for a variety of endpoints. In this specification our approach was aimed at predicting the likelihood of fluid overload hospitalizations (FOH) within one week of dialysis treatment. The framework serves as a foundation for both ML applications and qualitative analyses. This study aims to develop a model that maximizes the informational value of patient data while providing a holistic representation of health trajectories.\n \n \n \n The methodology consists of three main phases. Patient data, including dialysis treatments, laboratory results, and demographic characteristics, is pre-processed and arranged into a sequence of data that represent the full history of each patient, similarly to how large language models process text. Three-month history documents are created for each patient. Continuous input data is categorized into percentiles, while categorical data is used as-is. The sentences of these documents represent the sequence of the categorized input variables, with each variable value functioning as a token. The Life2Vec architecture is then pretrained on these documents to generate precise and informative embeddings. Training involves two tasks: masked language modeling and sequence order prediction. These tasks enable the model to learn token interactions and temporal sequence order. Temporal progression and patient age are incorporated to contextualize the embeddings, enhancing their informativeness. Finally, the trained model is used to generate the embeddings. Each token is mapped in 12-dimensional space, representing the value of the original variable as well as its interactions with the other variables.\n The encoded data obtained through token mapping is then employed to predict hospitalizations due to fluid overload. An XGBoost classifier is developed using both raw (as benchmark) and embedding-encoded data. The classification task specifically targets predicting FOH hospitalizations within one week.\n Patients are divided into training, testing, and validation datasets, and a descriptive table summarizes their demographic and clinical characteristics, such as age, sex, race, vascular access type, and comorbidities. This ensures a comprehensive understanding of data distribution and its impact on model development. As the weekly FOH prevalence is low (0.8%), the classifier's performance is evaluated using the area under the precision-recall curve (AUC-PR), it measures the area under the Precision-Recall curve, evaluating a classification model's performance, especially with imbalanced classes. Values near 1 indicate high effectiveness.\n \n \n \n Fluid overload hospitalizations occurred at a rate of 8 per 1,000 treatment sessions (0.008 per session, 95% CI: 0.003\u20130.014). The AUC-PR score for the downstream task of hospitalization prediction was significantly higher for embeddings-encoded data, with a score of 0.083 compared to 0.053 of a machine learning model trained on raw data (P\u00a0<\u00a00.001), that is a 10-times discrimination gain compared to the null model.\n \n \n \n We present a framework to densely encode sequences of medical events and use them for multiple downstream predictions. In this implementation, our model outperforms traditional machine learning methods trained on raw data in predicting fluid overload hospitalization. This method will help streamline and improve prediction of rare events for which insufficient outcome data are available for traditional machine learning exercise. Medical history embeddings are also suitable for qualitative analysis of medical events.\n",
            "venue": "Nephrology Dialysis Transplantation",
            "paperUrl": "https://www.semanticscholar.org/paper/1162518e157df984005781befad5137fe322e387",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1093/ndt/gfaf116.0786",
            "reason_for_inclusion": "Included to meet target filter limit."
        },
        {
            "year": "2025.10",
            "title": "AI-powered three-category Helicobacter pylori diagnosis via magnetic controlled capsule endoscopy: a multicenter validation of a vision-language model",
            "team": "Hongmei Jiao",
            "team website": "",
            "affiliation": "",
            "domain": "Vision-language model for H. pylori diagnosis via capsule endoscopy",
            "abstract": "Introduction Accurate classification of Helicobacter pylori (H. pylori) infection status is critical for gastric cancer risk stratification. Current methods based on traditional convolutional neural networks (CNNs) are limited by their reliance on fragmented single-image analysis and operator-dependent selection variability, impairing diagnostic reliability. Methods To overcome these limitations, we developed MC-CLIP, a vision-language foundation model for the fully automated, three-categorical diagnosis of H. pylori infection using magnetically controlled capsule endoscopy (MCCE). The model was first pretrained on a large-scale dataset of 2,427,475 MCCE image-text pairs derived from 123,543 examinations. It was subsequently fine-tuned on 40,695 expertly annotated images from 864 patients. MC-CLIP autonomously selects 30 representative images per case for end-to-end classification. Its performance was rigorously evaluated on multicenter internal (n = 220) and external (n = 208) validation cohorts. Results On the internal and external validation cohorts, MC-CLIP achieved overall accuracies of 89.6% (95% CI: 85.5\u201393.6%) and 86.6% (80.8\u201390.3%), respectively. The model demonstrated particularly high sensitivity in detecting H. pylori infection: 91.4% for current infection and 83.7% for past infection. This performance significantly surpassed that of both senior endoscopists (84.3% and 71.4%, respectively) and junior endoscopists (74.3% for current infection). MC-CLIP also maintained high specificity (>90% across all categories) and excelled at identifying subtle mucosal changes following eradication therapy, thereby reducing the misclassification of past infections as non-infections. Discussion By integrating multimodal image-text data and performing end-to-end analysis, MC-CLIP effectively addresses the fundamental limitations of CNN-based approaches. The model shows strong potential for enhancing the accuracy and reliability of MCCE-based gastric cancer screening programs.",
            "venue": "Frontiers in Microbiology",
            "paperUrl": "https://www.semanticscholar.org/paper/d33e009f7804910b78436dbf1ef9c33b0b4735a0",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.3389/fmicb.2025.1687021",
            "reason_for_inclusion": "Included to meet target filter limit."
        }
    ],
    "benchmarks": [
        {
            "year": "2025.10",
            "title": "There are significant differences among artificial intelligence large language models when answering scientific questions",
            "team": "E. Barraj\u00f3n-Catal\u00e1n",
            "team website": "",
            "affiliation": "",
            "domain": "Comparative evaluation of LLMs on scientific question answering",
            "abstract": "Introduction This study investigates the efficacy of large language models (LLMs) for generating accurate scientific responses through a comparative evaluation of five prominent free models: Claude 3.5 Sonnet, Gemini, ChatGPT 4o, Mistral Large 2, and Llama 3.1 70B. Methods Sixteen expert scientific reviewers assessed these models in terms of depth, accuracy, relevance, and clarity. Results Claude 3.5 Sonnet emerged as the highest scoring model, followed by Gemini, with notable variability among the other models. Additionally, retrieval-augmented generation (RAG) techniques were applied to improve LLM performance, and prompts were refined to improve answers. The results indicate that although LLMs such as Claude 3.5 Sonnet have potential for scientific tasks, other models may require more development or additional prompt engineering to reach comparable accuracy. Reviewers\u2019 perceptions of artificial intelligence (AI) utility and trustworthiness showed a positive shift after evaluation. However, ethical concerns, particularly with respect to transparency and disclosure, remained consistent. Discussion The study highlights the need for structured frameworks for evaluating LLMs and ethical considerations essential for responsible AI integration in scientific research. These findings should be interpreted with caution, as the limited sample size and domain-specific focus of the exam questions restrict the generalizability of the results.",
            "venue": "Frontiers in Artificial Intelligence",
            "paperUrl": "https://www.semanticscholar.org/paper/fb8c61c3c19c42a2ea24c9d24d8fe154a1c2ad5c",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.3389/frai.2025.1664303",
            "reason_for_inclusion": "Included to meet target filter limit."
        },
        {
            "year": "2025.10",
            "title": "Variation in Large Language Model Recommendations in Challenging Inpatient Management Scenarios.",
            "team": "Eric Bressman",
            "team website": "",
            "affiliation": "",
            "domain": "Variability of LLM clinical recommendations in inpatient scenarios",
            "abstract": "",
            "venue": "Journal of general internal medicine",
            "paperUrl": "https://www.semanticscholar.org/paper/8111d51e12be7239b1bdceb41a8a7d49364ce36f",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1007/s11606-025-09888-7",
            "reason_for_inclusion": "Included to meet target filter limit."
        },
        {
            "year": "2025.10",
            "title": "How does AI compare to the experts in a Delphi setting: simulating medical consensus with large language models.",
            "team": "A. Han",
            "team website": "",
            "affiliation": "",
            "domain": "Simulating Delphi consensus with LLMs in medical decision-making",
            "abstract": "BACKGROUND\nSeveral attempts have been made to enhance decision-making capabilities of large language models (LLMs) through debate and collaboration, simulating human-like deliberative processes. However, limited research exists on whether the collective intelligence of LLMs can reproduce consensus decisions of human experts. We investigated consensus-building processes and outcomes among LLMs using a modified Delphi method, comparing results to a human expert Delphi study.\n\n\nMETHODS\nWe conducted a three-round Delphi study involving eight LLMs, evaluating 135 medical statements from the International Federation for the Surgery of Obesity and Metabolic Disorders 2024 Delphi study. LLMs independently assessed statements in Round 1, refined their opinions based on feedback integration in Round 2, and engaged in pairwise debate in Round 3. Consensus was defined as \u226570% agreement. Concordance was defined as identical outcomes between LLMs and human experts, either both reaching or both failing to reach consensus.\n\n\nRESULTS\nLLMs achieved a higher overall consensus rate than human experts (93.3% vs. 81.5%, P\u00a0 =\u00a00.002). Initial independent evaluations yielded consensus on 117 statements (86.7%), with five additional statements reaching consensus after feedback integration and four more following structured debates. Concordance between LLM and human expert consensus outcomes was observed in 78.5% of statements overall, and in 91.8% of statements where human experts had achieved consensus. The consensus rates between LLMs and human experts demonstrated a strong positive correlation (Spearman's rho\u00a0=\u00a00.73, P < 0.001). Substantial variation was observed among individual LLMs in their likelihood of changing decisions in response to peer feedback during Round 2 (0-44.4%). Similarly, considerable differences existed between LLMs in their ability to persuade others (0-63.6%) or their susceptibility to persuasion (0-80.0%) during Round 3.\n\n\nCONCLUSION\nLLM-based Delphi methods demonstrated high clinical consensus closely aligned with human expert decisions. LLMs effectively simulated structured human-like deliberative reasoning, though they tended to adopt more guideline-driven and conservative positions. However, the use of commercial LLM platforms limited control over model parameters that may affect reproducibility. While the current study suggests that LLMs hold promise as complementary tools in medical consensus-building processes, further research addressing parameter optimization is warranted.",
            "venue": "International journal of surgery",
            "paperUrl": "https://www.semanticscholar.org/paper/78f0fe0f64e3c240cb970a1a09aa3246dcf7b17d",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1097/JS9.0000000000003631",
            "reason_for_inclusion": "Included to meet target filter limit."
        }
    ],
    "databases": [
        {
            "year": "2025.10",
            "title": "Towards Community-Based Evaluation of AI in Neurology: Development of a Headache Diagnosis Dataset for Large Language Models.",
            "team": "Dorian Zwanzig",
            "team website": "",
            "affiliation": "",
            "domain": "Headache diagnosis dataset for community-based LLM evaluation in neurology",
            "abstract": "",
            "venue": "Studies in health technology and informatics",
            "paperUrl": "https://www.semanticscholar.org/paper/16e555468e627ef481d4d175fd23ccf75b2267c6",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.3233/SHTI251535",
            "reason_for_inclusion": "Included to meet target filter limit."
        },
        {
            "year": "2025.10",
            "title": "Development of a large-scale grounded vision language dataset for chest CT analysis",
            "team": "Yanfeng Wang",
            "team website": "",
            "affiliation": "",
            "domain": "RadGenome-Chest CT: large-scale grounded vision-language dataset for chest CT",
            "abstract": "Developing generalist foundation model has recently attracted tremendous attention in the field of AI for Medicine, which requires open-source medical image datasets that incorporate diverse supervision signals across various imaging modalities. In this paper, we introduce RadGenome-Chest CT, a comprehensive, large-scale, region-guided 3D chest CT interpretation dataset based on CT-RATE. Specifically, we leverage the latest powerful universal segmentation model and large language models, to extend the original datasets from the following aspects: organ-level segmentation masks covering 197 categories, which provide intermediate reasoning visual clues for interpretation; 665K multigranularity grounded reports, where each sentence of the report is linked to the corresponding anatomical region of CT volume with a segmentation mask; 1.2M grounded VQA pairs, where questions and answers are all linked with reference segmentation masks, enabling models to associate visual evidence with textual explanations. We believe that RadGenome-Chest CT can significantly advance the development of multimodal medical foundation models, by training to generate texts based on given segmentation regions, which is unattainable with previous relevant datasets.",
            "venue": "Scientific Data",
            "paperUrl": "https://www.semanticscholar.org/paper/3912db2f306b51446460ca728fc328fdf05e41f5",
            "codeUrl": "",
            "githubStars": "",
            "doi": "10.1038/s41597-025-05922-9",
            "reason_for_inclusion": "High quality: Published in Scientific Data, a Nature portfolio journal."
        }
    ]
}