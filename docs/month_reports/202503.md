# Research Paper Report for 2025-03-01 to 2025-03-31

## Overall Summary

The collection of nineteen papers surveyed from March 2025 reflects the rapid proliferation of large language model (LLM)–driven multi-agent systems, methodological advances in multi-modal data integration, and emerging benchmarks in both synthetic and biological domains. A dominant theme is the use of LLMs as orchestrators of task-specific “agents” that collaborate to automate complex scientific workflows. For instance, PharmAgents [1] and DrugAgent [10] target drug discovery by decomposing the pipeline into specialized agents for target identification, lead optimization, and in silico toxicity analysis. MultiMol [5] extends this paradigm for multi-objective molecular optimization, contrasting a data-driven worker agent with a literature-guided research agent and achieving an 82.3% success rate versus 27.5% for previous approaches. AstroAgents [2] applies an eight-agent LLM system to mass spectrometry data, generating and critiquing novel astrobiological hypotheses, with 36% plausibility and 66% novelty. DORA [3] generalizes agent-based automation to the entire research lifecycle—hypothesis generation, experimental design, data analysis, and report drafting—by employing plug-and-play generalist and domain-specific LLMs.

Beyond LLM-centric pipelines, the agent-based simulation platform BioDynaMo and its distributed counterpart TeraAgent [4] deliver modular abstractions and up to three orders of magnitude speedups, enabling simulations of 500 billion agents across 84 096 cores. Evolutionary Ecology of Words [6] leverages LLMs within a spatial agent-based ecological model to uncover emergent species dynamics, demonstrating both gradual and punctuated equilibria. IAN [7] integrates high-throughput omics and pathway data through a multi-agent AI system, grounding LLM summaries on KEGG, Reactome, and STRING databases to minimize hallucinations and provide interpretability in biological discovery.

Review articles [11–16] survey LLM prompting in biomanufacturing, bioinformatics, genome annotation, and neuromodulation, highlighting challenges such as data scarcity, cross-omics fusion, and the need for human-AI collaboration. The benchmarks paper [17] examines model-brain alignment in audio-language processing, revealing that Qwen2-Audio’s surprisal metrics predict human N400 responses, while Ultravox 0.5 fails to capture paralinguistic context. The foundation model work MADRIGAL [18] introduces a transformer bottleneck for multimodal drug combination prediction across 953 clinical outcomes, excelling in virtual screening and polypharmacy management. Finally, the RxRx3-core database [19] provides a high-content microscopy benchmark for drug-target interaction modeling, supporting ICLR-level development of robust computer vision architectures in chemical perturbation studies.

Collectively, these works demonstrate methodological innovations—transformer bottlenecks, paired-VAE and reinforcement learning frameworks, grid-based neighbor searches, delta encoding for communication efficiency—and advance practical applications from autonomous pharma ecosystems to personalized neuromodulation. Limitations persist in empirical validation at scale (e.g., in silico predictions need wet-lab confirmation), in generalization beyond curated tasks, and in bounding LLM hallucinations. Future directions point to stronger human-AI feedback loops, hybrid symbolic-neural methods, and unified multimodal benchmarks.

## Table of Contents

- [AI Agents](#ai-agents)  
- [Reviews](#reviews)  
- [Benchmarks](#benchmarks)  
- [Foundation Models](#foundation-models)  
- [Databases](#databases)  

## AI Agents

This category encompasses ten works [1–10] that leverage multi-agent LLM architectures to automate tasks across drug discovery, astrobiology, research automation, molecular optimization, ecological modeling, omics data integration, personalized treatment, and proteomics. PharmAgents [1] constructs a virtual pharmaceutical ecosystem of explainable LLM agents that simulate the drug pipeline—target discovery, lead identification, affinity optimization, toxicity screening—while supporting self-evolution through structured knowledge exchange. AstroAgents [2] organizes eight specialized agents (data analyst, planner, domain scientists, accumulator, literature reviewer, critic) to process mass spectrometry from meteorites and soils; evaluation by an astrobiologist shows 36% of 100+ generated hypotheses are plausible and 66% novel. DORA [3] uses hierarchical generalist and domain-specific LLMs in reinforcement learning–driven plug-and-play templates for automated literature review, experiment design, and report drafting, greatly shortening manuscript preparation by focusing human effort on discovery. BioDynaMo and TeraAgent [4] introduce an extreme-scale, modular agent-based simulation platform: first defining abstractions and validating use cases in neuroscience, epidemiology, oncology; then optimizing shared-memory parallelism with grid neighbor searches and latency reduction; finally scaling distributed simulations of 500 billion agents on 84 096 cores via serialization and delta encoding. MultiMol [5] combines a fine-tuned data-driven worker LLM with a literature-guided research agent to perform six real-world multi-objective molecular tasks, achieving 82.3% success versus 27.5% for baselines; demonstrated on XAC selectivity and Saquinavir bioavailability. Evolutionary Ecology of Words [6] embeds LLM-generated words in spatial ABMs, enabling word mutations and competitions that yield emergent, ecologically specialized “species” across habitats. IAN [7] is an R package integrating KEGG, WikiPathways, Reactome, GO, ChEA, and STRING for enrichment analysis, then uses LLM agents to produce grounded multi-dataset summaries, avoiding hallucinations and facilitating omics-driven discovery. TxAgent [8] proposes an AI agent for therapeutic reasoning across diverse biomedical tools (abstract not available). DrBioRight 2.0 [9] develops an LLM-powered chatbot for large-scale cancer proteomics analysis (abstract not available). DrugAgent [10] automates AI-aided drug discovery programming via LLM multi-agent collaboration (abstract not available). While [1,5,10] concentrate on molecular design, [2,6] extend to hypothesis generation and evolutionary modeling; [3,4] offer general research and simulation platforms; [7] and [9] target biological data interpretation. Common limitations include dependency on curated data, limited wet-lab validation, and potential agent coordination overhead.

| Index | Title                                                                                                                        | Domain                                 | Venue                                                                                           | Team                     | DOI                               | affiliation                                                                                           | paperUrl                                                                                  |
|-------|------------------------------------------------------------------------------------------------------------------------------|----------------------------------------|-------------------------------------------------------------------------------------------------|--------------------------|-----------------------------------|-------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------|
| 1     | PharmAgents: Building a Virtual Pharma with Large Language Model Agents                                                      | Drug Discovery                         | ArXiv                                                                                           | Yanyan Lan               | 10.48550/arXiv.2503.22164         |                                                                                                       | [Link](https://www.semanticscholar.org/paper/46e921cb6fba3423e48fa894afa9b0b47bf934d6)    |
| 2     | AstroAgents: A Multi-Agent AI for Hypothesis Generation from Mass Spectrometry Data                                          | Astrobiology & Mass Spectrometry AI    | ArXiv                                                                                           | Amirali Aghazadeh        | 10.48550/arXiv.2503.23170         |                                                                                                       | [Link](https://www.semanticscholar.org/paper/a967904a87ba4c1d6359fbd7342a074d63e2fe63)    |
| 3     | DORA AI Scientist: Multi-agent Virtual Research Team for Scientific Exploration Discovery and Automated Report Generation    | Scientific Research Automation         | bioRxiv                                                                                         | Alex Zhavoronkov         | 10.1101/2025.03.06.641840         | Insilico Medicine                                                                                    | [Link](https://www.semanticscholar.org/paper/b913baa47245a4a76054bb12e04d61c3c88fb532)    |
| 4     | Design and Analysis of an Extreme-Scale, High-Performance, and Modular Agent-Based Simulation Platform                       | Agent-Based Simulation                 | ArXiv                                                                                           | Lukas Breitwieser        | 10.3929/ethz-b-000726143         |                                                                                                       | [Link](https://www.semanticscholar.org/paper/ab1f61ae171dfbe425e60bc942997c1bdba29c24)    |
| 5     | Collaborative Expert LLMs Guided Multi-Objective Molecular Optimization                                                     | Molecular Optimization                 | ArXiv                                                                                           | Haishuai Wang            | 10.48550/arXiv.2503.03503         |                                                                                                       | [Link](https://www.semanticscholar.org/paper/391ca60f6cd9b7ccc0ff8bc5fc72c6543c770fd9)    |
| 6     | Evolutionary Ecology of Words                                                                                                 | Evolutionary Linguistics Modeling      | 2025 IEEE Symposium on ALIFE-CIS                                                               | Takaya Arita             | 10.1109/ALIFE-CIS64968.2025.10979831 |                                                                                                       | [Link](https://www.semanticscholar.org/paper/84dd4691093d0df1e91f81d2c4df522cf69b288f)    |
| 7     | IAN: An Intelligent System for Omics Data Analysis and Discovery                                                             | Omics Data Integration                 | bioRxiv                                                                                         | Rachel R. Caspi          | 10.1101/2025.03.06.640921         | Laboratory of Immunology, National Eye Institute, NIH, Bethesda 20892, USA                          | [Link](https://www.semanticscholar.org/paper/1ecbcfe40fe475b6fec3186ced39162afdb33175)    |
| 8     | TxAgent: An AI Agent for Therapeutic Reasoning Across a Universe of Tools                                                    | Personalized Treatment Recommendations | arXiv                                                                                           | Marinka Zitnik           |                                   |                                                                                                       | [Link](https://arxiv.org/abs/2503.10970)                                                |
| 9     | DrBioRight 2.0: an LLM-powered bioinformatics chatbot for large-scale cancer functional proteomics analysis                 | Proteomics                             | Nature Communication                                                                           |                          |                                   |                                                                                                       | [Link](https://www.nature.com/articles/s41467-025-57430-4)                               |
| 10    | DrugAgent: Automating AI-aided Drug Discovery Programming through LLM Multi-Agent Collaboration                             | Drug Discovery                         | arXiv                                                                                           |                          |                                   |                                                                                                       | [Link](https://arxiv.org/abs/2411.15692)                                                |

## Reviews

This section reviews six survey and conceptual studies [11–16] addressing the integration of LLMs into biomanufacturing, bioinformatics, genomics, and neurotechnology. Tang [11] in **Trends in Biotechnology** provides a high-level overview of LLM applications for knowledge synthesis in biomanufacturing pipelines, highlighting potential for real-time design of microbial strains, process optimization, and digital twins, while cautioning about data curation and explainability challenges. Li’s survey on prompt-based biological sequence analysis [12] systematically examines prompt engineering for DNA/RNA promoter prediction, protein structure modeling, and drug-target affinity prediction using limited labeled data; it underscores the need for multimodal fusion (e.g., combining sequence and structural information) and computational resource constraints. Zhou’s review on genome decoding [13] (Frontiers of Digital Education) discusses applications of LLMs in annotating non-coding regions and variant interpretation, though lacking a formal abstract. Li’s bioinformatics survey [14] covers genomic sequence modeling, RNA secondary structure prediction, protein function inference, and single-cell transcriptomics, and identifies cross-omics integration and scalability as key future directions. Suchecki’s HAICoGA framework [15] proposes a human-AI collaborative model for genome annotation that merges automated predictions with expert validation, detailing a modular LLM-supported curation loop and open research questions around user interfaces and provenance. Wen’s dual-loop neuromodulation system [16] integrates implanted responsive neurostimulation devices with wearable LLM systems to monitor PTSD triggers and naturalistic behaviors; it describes real-time sensory data analysis, closed-loop theta oscillation detection, and context-aware audiovisual interventions, signifying a convergence of BCI and multimodal LLMs. Collectively, these reviews reveal common methodological concerns—data scarcity, prompt reliability, human-in-the-loop validation—and propose architectures that balance automated efficiency with expert oversight, pointing toward hybrid AI-human workflows in biotechnology and neuroscience.

| Index | Title                                                                                               | Domain                                        | Venue                                | Team                   | DOI                              | affiliation | paperUrl                                                                                       |
|-------|-----------------------------------------------------------------------------------------------------|-----------------------------------------------|--------------------------------------|------------------------|----------------------------------|-------------|------------------------------------------------------------------------------------------------|
| 11    | Large language model for knowledge synthesis and AI-enhanced biomanufacturing.                     | Biomanufacturing                              | Trends in Biotechnology              | Yinjie J. Tang         | 10.1016/j.tibtech.2025.02.008    |             | [Link](https://www.semanticscholar.org/paper/12610a7b39efe0ec0a1cd52c242a712ea719fb4c)        |
| 12    | Biological Sequence with Language Model Prompting: A Survey                                         | Sequence Analysis                             | ArXiv                                | Yu Li                  | 10.48550/arXiv.2503.04135        |             | [Link](https://www.semanticscholar.org/paper/9ad1cc1a90c204bc066861e41c7d3c4125ad6340)        |
| 13    | AI-Empowered Genome Decoding: Applications of Large Language Models in Genomics                     | Genomics                                      | Frontiers of Digital Education       | Yu Zhou                | 10.1007/s44366-025-0051-1        |             | [Link](https://www.semanticscholar.org/paper/fb2f674c38dfd2fb692d08588fa1f99758bce28b)        |
| 14    | Large Language Models in Bioinformatics: A Survey                                                   | Bioinformatics                                | ArXiv                                | Yu Li                  | 10.48550/arXiv.2503.04490        |             | [Link](https://www.semanticscholar.org/paper/aef0d2c0359d97c688f89c13732ec21a351ead8d)        |
| 15    | A Conceptual Framework for Human-AI Collaborative Genome Annotation                                  | Genome Annotation                             | ArXiv                                | Radoslaw Suchecki      | 10.48550/arXiv.2503.23691        |             | [Link](https://www.semanticscholar.org/paper/b5bbf10cc21c28b6a7b626f76869aa8f30506f78)        |
| 16    | When neural implant meets multimodal LLM: A dual-loop system for neuromodulation and naturalistic neuralbehavioral research | Brain-Computer Interface & Neuromodulation    | ArXiv                                | Cynthia Xin Wen        | 10.48550/arXiv.2503.12334        |             | [Link](https://www.semanticscholar.org/paper/1412f530b1758f7e9b8f4888c7a6e52d6502cfa5)        |

## Benchmarks

Distinct social-linguistic processing between humans and large audio-language models [17] investigates cognitive alignment using EEG and two LALMs: Qwen2-Audio and Ultravox 0.5. Employing surprisal and entropy metrics, the study quantifies model responses to speaker-content incongruencies in social stereotype violations (e.g., “a man who gets manicures”) and biological impossibilities (e.g., “a man who is pregnant”). Qwen2-Audio’s surprisal correlates significantly with human N400 amplitudes, indicating sensitivity to social context, whereas Ultravox 0.5 shows limited paralinguistic processing. Crucially, neither model replicates the human distinction between N400 effects (social violations) and P600 effects (biological violations). This reveals that current LALMs partially mirror human linguistic integration but lack nuanced treatment of taboo and biological knowledge. Methodologically, the paper pioneers direct model-brain alignment using surprisal metrics and EEG, highlighting both the promise and limitations of multimodal cognitive benchmarks. Future work should explore training LALMs with more varied paralinguistic signals, incorporate attention-based metrics, and extend to real-time speech recognition contexts.

| Index | Title                                                                                                              | Domain                                          | Venue | Team                   | DOI                           | affiliation | paperUrl                                                                                            |
|-------|--------------------------------------------------------------------------------------------------------------------|-------------------------------------------------|-------|------------------------|-------------------------------|-------------|-----------------------------------------------------------------------------------------------------|
| 17    | Distinct social-linguistic processing between humans and large audio-language models: Evidence from model-brain alignment | Cognitive Neuroscience & Audio-Language Modeling | ArXiv | Zhenguang G. Cai       | 10.48550/arXiv.2503.19586     |             | [Link](https://www.semanticscholar.org/paper/47e39f3d5f62dbf4079be48a77f540baea3f689d)             |

## Foundation Models

Multimodal AI predicts clinical outcomes of drug combinations from preclinical data (MADRIGAL) [18] introduces a transformer-bottleneck architecture to fuse structural, pathway, cell-viability, and transcriptomic modalities across 21 842 compounds and 953 clinical endpoints. MADRIGAL handles missing modalities via masked training and inference, outperforming single-modality baselines and state-of-the-art models in adverse interaction prediction. It excels in virtual anticancer screening and polypharmacy management for type II diabetes and MASH, correctly identifying resmetirom’s favorable safety profile. Integration with an LLM interface enables natural language queries for clinical outcome descriptions, further refining risk assessment. Technical contributions include a modality-agnostic transformer bottleneck, robust handling of missing data, and seamless LLM integration for interpretability. Limitations involve potential biases from preclinical datasets, challenges in translating predictions to patient heterogeneity, and computational overhead of large transformer modules. Future research should explore real-time patient data integration, continuous learning from electronic health records, and on-device inference for decentralized clinical decision support.

| Index | Title                                                                                                                | Domain                          | Venue | Team               | DOI                           | affiliation | paperUrl                                                                                         |
|-------|----------------------------------------------------------------------------------------------------------------------|---------------------------------|-------|--------------------|-------------------------------|-------------|------------------------------------------------------------------------------------------------|
| 18    | Multimodal AI predicts clinical outcomes of drug combinations from preclinical data                                 | Clinical Pharmacology Modeling  | ArXiv | Marinka Zitnik     | 10.48550/arXiv.2503.02781     |             | [Link](https://www.semanticscholar.org/paper/342b7a61048fb4025a64078633f530fd3f74dbc7)         |

## Databases

RxRx3-core: Benchmarking drug-target interactions in High-Content Microscopy [19] provides a meticulously curated dataset of cell-image perturbation profiles under chemical treatments, designed to evaluate AI models for drug-target interaction prediction. Released at ICLR 2025, the dataset comprises millions of microscopy images annotated with compound identities and concentrations, enabling supervised and self-supervised learning experiments. Although no abstract is provided, the resource is vital for training convolutional and transformer-based vision models in the context of pharmacological perturbation. By offering standardized splits, metadata on cell lines, and perturbation dosages, RxRx3-core addresses reproducibility and generalization challenges in high-content screening. Future extensions may include multimodal integration with transcriptomic readouts, cell-type diversity, and time-series imaging to capture dynamic drug responses.

| Index | Title                                                                                                 | Domain                        | Venue      | Team                    | DOI | affiliation | paperUrl                                                    |
|-------|-------------------------------------------------------------------------------------------------------|-------------------------------|------------|-------------------------|-----|-------------|-------------------------------------------------------------|
| 19    | RxRx3-core: Benchmarking drug-target interactions in High-Content Microscopy                          | Microscopy Perturbation Data  | ICLR 2025 | Recursion (Imran S. Haque) |     |             | [Link](https://arxiv.org/abs/2503.20158v2)                   |

