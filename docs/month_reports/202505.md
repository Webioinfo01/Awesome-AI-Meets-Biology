# Research Paper Report for 2025-05-01 to 2025-05-30

## Overall Summary

Between May 1 and May 30, 2025, a diverse set of 22 papers [1–22] illustrate rapid progress in AI-driven biomedical research, spanning agentic systems, foundation models, systematic reviews, benchmarking efforts, and AI-ready databases. In the **AI Agents** domain [1–7], we observe a shift from single-purpose tools toward generalist and self-evolving frameworks. For instance, CellTypeAgent [1] integrates large language models (LLMs) with external databases to verify annotations across 303 cell types, achieving superior accuracy while mitigating hallucinations. ChatMolData [2] pioneers multimodal LLM-agent design, combining GPT-4/GPT-3.5 with a cycle of reasoning and action to process 128 molecular tasks at >90% accuracy. PlantGPT [3] leverages a Chroma retrieval-augmented generation (RAG) pipeline and fine-tuned Llama3-8B on Arabidopsis phenotype and gene function data, outperforming generic LLMs. DrugPilot [4] introduces a parametric inference architecture and interactive memory pool to standardize heterogeneous drug data, reporting task completion rates up to 98% in complex multi-stage pipelines. Meanwhile, BRAD [5] emphasizes transparency and provenance for automated biomarker discovery, Alita [6] explores minimal predefinition self-evolving agents, and BioOmni [7] presents a multi-modal generalist biomedical research assistant.

In **Foundation Models** [8–14], innovations include sciLaMA’s paired-VAE that fuses static gene embeddings from multimodal LLMs with scRNA-seq tabular data for interpretable cell and gene representations [8]; LlamaAffinity’s integration of Llama3 backbone with OAS antibody sequences to achieve an AUC-ROC of 0.9936 [9] in binding affinity prediction; GRAPE’s heterogeneous graph neural network that models gene biotype distinctions using LLM- and DNA-derived features [10]; and a visual-omics foundation model [11] that aligns histopathology images with spatial transcriptomics. Security and reasoning are addressed by GeneBreaker’s pathogenicity-guided jailbreak attacks [12] and BioReason’s incentivized multimodal DNA‐LLM reasoning [13]. At scale, CellFM [14] is pre-trained on transcriptomes from 100 million human cells, offering a foundational resource for single-cell analysis.

Systematic **Reviews** [15–16] identify overarching challenges: computational cost, data quality, model interpretability, and domain adaptation. Luo et al. [15] survey foundation models in computational microscopy, highlighting gains in resolution and real-time analysis alongside annotation bottlenecks. A second survey [16] catalogs LLM applications in bioinformatics, mapping current capabilities and gaps.

**Benchmarking** efforts [17–20] critically evaluate LLM performance: prompt engineering for PPI extraction achieves up to 90.3% F1-score using Gemini 1.5 Pro [17]; CellVerse [18] assesses LLMs on hierarchical single-cell tasks, revealing generalist models’ nascent understanding but overall underperformance; GPT-4o metadata extraction in neuroimaging attains human-comparable accuracy (0.91–0.97) [19]; and scDrugMap [20] offers a platform comparing eight single-cell foundation models and two LLMs for drug response prediction, with pooled-data F1 scores up to 0.971.

Lastly, **Databases** [21–22] like scCompass integrate and standardize 105 million single-cell profiles across 13 species for AI-readiness [21], while AlphaLasso provides an interactive server identifying >14 million lasso motifs in biopolymers, accelerating topology-based analyses [22].

Collectively, these works reveal emerging trends: retrieval-augmented and parametric agentic reasoning, multimodal integration, graph-based representations, and large-scale pre-training. Technical innovations range from paired-VAE frameworks and memory pools to interactive web servers and benchmark suites. While significant accuracy and efficiency gains are demonstrated, persistent limitations include model hallucinations, provenance traceability, domain generalizability, and the need for high-quality annotations. Future research must emphasize interpretability, robust evaluation, and modular, transparent architectures to fully realize the potential of AI in biomedical science.

## Table of Contents

- [ai-agents](#ai-agents)  
- [foundation-models](#foundation-models)  
- [reviews](#reviews)  
- [benchmarks](#benchmarks)  
- [databases](#databases)  

## ai-agents

The **ai-agents** category [1–7] illustrates a rapid evolution of LLM-driven systems tailored to specific scientific tasks and broader generalist frameworks. CellTypeAgent [1] pioneers trustworthy annotation by integrating LLM predictions with database verification across nine datasets covering 303 cell types. ChatMolData [2] extends LLM agents into multimodal molecular data processing, harnessing GPT-4/GPT-3.5 alongside robust retrieval, structuring, prediction, and visualization tools. It achieves over 90% accuracy on 128 distinct tasks, showcasing the power of a reasoning-action cycle. PlantGPT [3] adopts a retrieval-augmented generation (RAG) approach, compiling abstracts from 60,000+ articles and fine-tuning Llama3-8B on 13,993 phenotypes and 23,323 gene functions, significantly outperforming generalist LLM baselines. DrugPilot [4] addresses parametric reasoning in drug discovery via an interactive memory pool that standardizes multi-modal data into parametric representations, yielding task completion rates up to 98.0% in simple tasks and outperforming agents like ReAct and LoT on a custom drug instruct dataset.

Beyond task-specific agents, BRAD [5] emphasizes reproducibility by maintaining transparent protocols and data provenance in biomarker discovery workflows. Alita [6] proposes a self-evolving generalist agent framework with minimal predefinitions, enabling scalable multi-chain planning (MCP). BioOmni [7] further explores general-purpose multi-modal research, automating end-to-end biomedical workflows. Collectively, these works demonstrate methodological advances such as retrieval-augmented LLM integration [1,3], parametric inference architectures [4], modular agent design for transparency [5], and self-evolving planning mechanisms [6]. Major limitations include residual hallucinations, dependence on high-quality external databases, and challenges in up-to-date domain knowledge integration. Practical implications span accelerated single-cell annotation, streamlined molecular data pipelines, improved plant genomics queries, and automated drug discovery planning.

| Index | Title                                                                 | Domain                                                                 | Venue                       | Date     | Team            | DOI                          | paperUrl                                                                                                                                                                             |
|-------|-----------------------------------------------------------------------|-----------------------------------------------------------------------|-----------------------------|----------|-----------------|------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 1     | CellTypeAgent: Trustworthy cell type annotation with Large Language Models | LLM agent for cell type annotation in single-cell data               | ArXiv                       | 2025.05  | Yunjian Li      | 10.48550/arXiv.2505.08844    | [CellTypeAgent: Trustworthy cell type annotation with Large Language Models](https://www.semanticscholar.org/paper/40e125e1b882b278d14d7f86617a4381084fa69a)                        |
| 2     | ChatMolData: A Multimodal Agent for Automatic Molecular Data Processing | Multimodal LLM-agent for automatic molecular data processing          | Advanced Intelligent Systems | 2025.05  | Xiaohui Yu      | 10.1002/aisy.202401089        | [ChatMolData: A Multimodal Agent for Automatic Molecular Data Processing](https://www.semanticscholar.org/paper/1fd6efbd486e07872b17bf8b08ab68c5a60716c0)                        |
| 3     | PlantGPT: An Arabidopsis-Based Intelligent Agent that Answers Questions about Plant Functional Genomics. | PlantGPT: LLM agent for plant functional genomics question answering | Advanced science             | 2025.05  | Qinlong Zhu     | 10.1002/advs.202503926        | [PlantGPT: An Arabidopsis-Based Intelligent Agent that Answers Questions about Plant Functional Genomics.](https://www.semanticscholar.org/paper/b0b726a254ede59afe4ab6635fb23c916ceacb94) |
| 4     | DrugPilot: LLM-based Parameterized Reasoning Agent for Drug Discovery  | LLM-based agent for parameterized reasoning in drug discovery         | ArXiv                        | 2025.05  | Wenbin Hu       | 10.48550/arXiv.2505.13940     | [DrugPilot: LLM-based Parameterized Reasoning Agent for Drug Discovery](https://www.semanticscholar.org/paper/a51b28d8fcf17077a038fe775117865731f9c98d)                              |
| 5     | Automatic biomarker discovery and enrichment with BRAD               | LLM agent for automatic biomarker discovery and enrichment (BRAD)     | Bioinformatics              | 2025.05  | I. Rajapakse    | 10.1093/bioinformatics/btaf159 | [Automatic biomarker discovery and enrichment with BRAD](https://www.semanticscholar.org/paper/3ac8cac9583281d5eebca6a17e00af015eb8b286)                                              |
| 6     | Alita: Generalist Agent Enabling Scalable Agentic Reasoning with Minimal Predefinition and Maximal Self-Evolution | General Scientific Research(self-evolution and create MCP)            | bioRxiv                      | 2025.05  | Mengdi Wang     |                              | [Alita: Generalist Agent Enabling Scalable Agentic Reasoning with Minimal Predefinition and Maximal Self-Evolution](https://arxiv.org/abs/2505.20286)                                 |
| 7     | BioOmni: A General-Purpose AI Agent for Automated Biomedical Research | General Biomedical Research (Multi-modal)                            | bioRxiv                      | 2025.05  | Marinka Zitnik  |                              | [BioOmni: A General-Purpose AI Agent for Automated Biomedical Research](https://www.biorxiv.org/content/10.1101/2025.05.30.656746v1)                                                |

## foundation-models

The **foundation-models** group [8–14] presents advanced deep learning frameworks and pre-training strategies for single-cell and molecular biology. sciLaMA [8] introduces a paired-VAE architecture that fuses static gene embeddings from multimodal LLMs with scRNA-seq tabular data, enabling context-aware cell and gene representations. Evaluated on batch correction, clustering, and marker identification, it matches or surpasses state-of-the-art generative models while retaining computational efficiency. LlamaAffinity [9] repurposes the open-source Llama3 backbone for antibody–antigen binding prediction, training on OAS sequence data to achieve 0.9640 accuracy, 0.9936 AUC-ROC, and a training time reduction by a factor of five compared to prior models. GRAPE [10] constructs a heterogeneous graph neural network (HGNN) by initializing gene nodes with combined LLM-derived descriptive embeddings and DNA sequence features, modeling biotype-specific roles and dynamically refining regulatory networks. This approach yields SOTA performance on genetic perturbation benchmarks. A visual-omics model [11] bridges histopathology and spatial transcriptomics via a transformer-based image encoder aligned with gene expression profiles, facilitating joint spatial–molecular inference. GeneBreaker [12] explores security by crafting pathogenicity-guided jailbreak attacks against DNA language models, while BioReason [13] integrates multimodal stimuli into DNA-LLM frameworks to incentivize biological reasoning. At the largest scale, CellFM [14] pre-trains on 100 million human cell transcriptomes, providing generic embeddings for downstream single-cell tasks. Methodological innovations span paired-VAE integration, graph-based representation, cross-modal alignment, and security testing. Challenges include computational cost of high-dimensional pre-training, interpretability of deep embeddings, and robustness of models under adversarial or out-of-distribution scenarios.

| Index | Title                                                                                             | Domain                                                                            | Venue               | Date     | Team             | DOI                                | paperUrl                                                                                                                                                                 |
|-------|---------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------|---------------------|----------|------------------|-------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 8     | sciLaMA: A Single-Cell Representation Learning Framework to Leverage Prior Knowledge from Large Language Models | Single-cell representation learning leveraging LLM prior knowledge               | bioRxiv             | 2025.05  | G. Quon          | 10.1101/2025.01.28.635153          | [sciLaMA: A Single-Cell Representation Learning Framework to Leverage Prior Knowledge from Large Language Models](https://www.semanticscholar.org/paper/9aab32b1ffef407d9918bcf6f7b87c930d45c3f7) |
| 9     | LlamaAffinity: A Predictive Antibody–Antigen Binding Model Integrating Antibody Sequences with Llama3 Backbone Architecture | Antibody–antigen binding affinity prediction using LLM-based models                | bioRxiv             | 2025.05  | J. Chen          | 10.1101/2025.05.28.653051           | [LlamaAffinity: A Predictive Antibody–Antigen Binding Model Integrating Antibody Sequences with Llama3 Backbone Architecture](https://www.semanticscholar.org/paper/2478b5379054be98bb2b0f325bc474ff806f1ea6) |
| 10    | GRAPE: Heterogeneous Graph Representation Learning for Genetic Perturbation with Coding and Non-Coding Biotype | Graph representation learning for genetic perturbation integrating LLM and DNA features | ArXiv               | 2025.05  | Stan Z. Li       | 10.48550/arXiv.2505.03853           | [GRAPE: Heterogeneous Graph Representation Learning for Genetic Perturbation with Coding and Non-Coding Biotype](https://www.semanticscholar.org/paper/e8cd781a1d5b928a0ff4d5eb2c3d42fdefdd6239) |
| 11    | A visual–omics foundation model to bridge histopathology with spatial transcriptomics            | histopathology and spatial single cell RNA                                        | Nature Methods      | 2025.05  | Guangyu Wang     |                                     | [A visual–omics foundation model to bridge histopathology with spatial transcriptomics](https://www.nature.com/articles/s41592-025-02707-1)                                                                       |
| 12    | GeneBreaker: Jailbreak Attacks against DNA Language Models with Pathogenicity Guidance           | Biosafety for DNA                                                                   | arXiv               | 2025.05  | Mengdi Wang      |                                     | [GeneBreaker: Jailbreak Attacks against DNA Language Models with Pathogenicity Guidance](https://arxiv.org/abs/2505.23839)                                                                                       |
| 13    | BioReason: Incentivizing Multimodal Biological Reasoning within a DNA-LLM Model                  | DNA                                                                                 | arXiv               | 2025.05  | Bo Wang          |                                     | [BioReason: Incentivizing Multimodal Biological Reasoning within a DNA-LLM Model](https://arxiv.org/abs/2505.23579)                                                                                              |
| 14    | CellFM: a large-scale foundation model pre-trained on transcriptomics of 100 million human cells | single cell RNA                                                                     | Nature Communications | 2025.05 |                  |                                     | [CellFM: a large-scale foundation model pre-trained on transcriptomics of 100 million human cells](https://www.nature.com/articles/s41467-025-59926-5)                                                           |

## reviews

The **reviews** section [15–16] provides critical syntheses of foundation models applied to biomedical imaging and bioinformatics. Luo et al. [15] conduct a systematic review of computational microscopy, highlighting how foundation models trained on large, heterogeneous image datasets overcome traditional limitations such as low resolution and slow processing. They underscore practical implications for real-time analysis of dynamic processes and precision diagnostics, while noting challenges in dataset curation, annotation quality, and cross-domain generalizability. The authors call for standardized benchmarks and explainable AI metrics to validate model outputs in clinical contexts. In contrast, the survey on LLMs in bioinformatics [16] maps the breadth of text-based applications—from gene annotation to literature mining—identifying methodological trends like retrieval-augmented fine-tuning and integration with biological databases. It outlines gaps in large-scale annotated corpora for specialized tasks and emphasizes the need for domain-specific tokenization, evaluation suites, and interpretability frameworks. Both reviews converge on the necessity of community-driven resources, transparency in data provenance, and modular architectures to balance model capacity with real-world applicability.

| Index | Title                                                                                                   | Domain                                           | Venue                        | Date     | Team         | DOI                          | paperUrl                                                                                                                              |
|-------|---------------------------------------------------------------------------------------------------------|--------------------------------------------------|------------------------------|----------|--------------|------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|
| 15    | Empowering Biomedical Research with Foundation Models in Computational Microscopy: A Systematic Review | Foundation models in computational microscopy     | Advanced Intelligent Systems | 2025.05  | Rong Luo     | 10.1002/aisy.202500154       | [Empowering Biomedical Research with Foundation Models in Computational Microscopy: A Systematic Review](https://www.semanticscholar.org/paper/c1a1785a284ae67ac3fcdfa4fc096a589762e3e2) |
| 16    | Large Language Models in Bioinformatics: A Survey                                                      | Bioinformatics foundation models                  | arXiv                        | 2025.05  |              |                              | [Large Language Models in Bioinformatics: A Survey](https://arxiv.org/abs/2503.04490)                                                    |

## benchmarks

The **benchmarks** category [17–20] rigorously evaluates LLM and foundation model capabilities across critical biomedical tasks. Lin et al. [17] explore prompt engineering for protein–protein interaction extraction using GPT-3.5, GPT-4, and Gemini, achieving up to 90.3% F1-score on LLL datasets and mitigating positive bias via refined prompts. CellVerse [18] establishes a language-centric, multi-omics QA benchmark for single-cell analysis across cell annotation, drug response, and perturbation tasks, revealing that no current LLM outperforms random guessing on drug response and underscoring large performance gaps. Turner et al. [19] demonstrate GPT-4o’s metadata extraction in neuroimaging, attaining 0.91–0.97 accuracy without fine-tuning, marking parity with human annotators across gold-standard fields. Qianqian Song’s scDrugMap [20] presents an evaluation framework comparing eight single-cell foundation models and two LLMs on drug response prediction using 326,751 cells from 36 datasets. Under pooled-data evaluation, scFoundation achieves mean F1 of 0.971 with layer freezing and 0.947 with LoRA fine-tuning; in cross-data settings, UCE leads at 0.774, while scGPT excels at zero-shot (0.858). Collectively, these benchmarks expose strengths and deficits—particularly in generalizability, prompt sensitivity, and domain adaptation—offering standardized metrics and datasets to guide future model development.

| Index | Title                                                                                                                        | Domain                                                                                   | Venue             | Date     | Team                | DOI                                 | paperUrl                                                                                                                                                                       |
|-------|------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------|-------------------|----------|---------------------|-------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 17    | The influence of prompt engineering on large language models for protein–protein interaction identification in biomedical literature | Prompt engineering of LLMs for protein-protein interaction extraction                    | Scientific Reports | 2025.05  | Yi-Hsuan Lin        | 10.1038/s41598-025-99290-4          | [The influence of prompt engineering on large language models for protein–protein interaction identification in biomedical literature](https://www.semanticscholar.org/paper/8738c78f5a2c2d94dcfc34ada047ed7e51dfbe8a) |
| 18    | CellVerse: Do Large Language Models Really Understand Cell Biology?                                                         | LLMs for language-driven single-cell multi-omics analysis (CellVerse)                    | ArXiv             | 2025.05  | P. Heng             | 10.48550/arXiv.2505.07865           | [CellVerse: Do Large Language Models Really Understand Cell Biology?](https://www.semanticscholar.org/paper/4dd1c628a26c7b13c98e975c7ae8b5c16fbc320e)                                    |
| 19    | Large Language Models Can Extract Metadata for Annotation of Human Neuroimaging Publications                               | LLM extraction and annotation of neuroimaging metadata                                   | bioRxiv           | 2025.05  | Jessica A Turner    | 10.1101/2025.05.13.653828           | [Large Language Models Can Extract Metadata for Annotation of Human Neuroimaging Publications](https://www.semanticscholar.org/paper/59087a91539883877289ceb562272f1cc13e9545)           |
| 20    | scDrugMap: Benchmarking Large Foundation Models for Drug Response Prediction                                                 | Benchmarking foundation models for single-cell drug response prediction                  | ArXiv             | 2025.05  | Qianqian Song       | 10.48550/arXiv.2505.05612           | [scDrugMap: Benchmarking Large Foundation Models for Drug Response Prediction](https://www.semanticscholar.org/paper/2dde8c82b5b59d46224f68e760ce5965a847e9c2)                         |

## databases

The **databases** section [21–22] focuses on large-scale, AI-ready repositories that accelerate model development and biological discovery. scCompass [21] aggregates and standardizes scRNA-seq data from 105 million cells across 13 species, offering consistent pre-processing, curated stable and organ-specific expression gene sets (SEGs/OSGs), and ready-to-use checkpoints for single-cell foundation models. Its web interface and visualization tools simplify data access for AI training and downstream analysis. AlphaLasso [22] provides an interactive web server that identifies topologically defined lasso motifs—cysteine, amide, ester, thioester, and user-specified closures—in >14 million AlphaFold protein models. Advanced visualization features support structure smoothing, topology maps, similarity searches, and metadata annotation, empowering large-scale topology analysis in biology and drug design. Both resources exemplify high-quality data curation, scalable interfaces, and AI-friendly formats, addressing key challenges in heterogeneity and reproducibility. Future extensions will likely incorporate additional modalities (e.g., spatial transcriptomics) and automated update pipelines to maintain currency with expanding public datasets.

| Index | Title                                                                                                                  | Domain                                                            | Venue                   | Date     | Team                  | DOI                            | paperUrl                                                                                                                                                                      |
|-------|------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------|-------------------------|----------|-----------------------|--------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 21    | scCompass: An Integrated Multi-Species scRNA-seq Database for AI-Ready                                                | Multi-species scRNA-seq database for AI-ready applications        | Advanced Science        | 2025.05  | Yuanchun Zhou         | 10.1002/advs.202500870          | [scCompass: An Integrated Multi-Species scRNA-seq Database for AI-Ready](https://www.semanticscholar.org/paper/f5af8e3f7acd968b7baeb788f5b33cf7d868616e)                             |
| 22    | AlphaLasso—a web server to identify loop and lasso motifs in 3D structure of biopolymers                              | Web server for lasso motifs in biopolymer 3D structures (AlphaLasso) | Nucleic Acids Research  | 2025.05  | Joanna I. Sulkowska   | 10.1093/nar/gkaf375             | [AlphaLasso—a web server to identify loop and lasso motifs in 3D structure of biopolymers](https://www.semanticscholar.org/paper/b2f16f79e6dc815d09b2892db7d3ca1d8a766478) |
