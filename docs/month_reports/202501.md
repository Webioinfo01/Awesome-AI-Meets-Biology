# Research Paper Report for 2025-01-01 to 2025-01-31

## Overall Summary  
During January 2025, the landscape of artificial intelligence in biomedicine and life sciences was marked by four key research themes: rigorous LLM benchmarking, expansive surveys of AI methods, novel foundation model architectures, and the rise of AI-driven multi-agent systems. The benchmarks category ([1]–[4]) introduced domain-specific evaluation frameworks highlighting critical gaps in current LLMs’ ability to handle complex tasks. CARDBiomedBench [1] proposed a 68,000 Q/A pair dataset for neurodegenerative disease research, revealing accuracy rates as low as 25% and safety issues up to 31% in top models. Alibakhshi et al. [2] assessed preschool science content generation, finding Claude outperformed GPT-4 in biological topics but struggling with abstract chemistry. Wilson [3] used Little Alchemy 2 to show that LLMs “think too fast,” trading depth for speed, whereas Phyla [4] introduced a hybrid state-space transformer with a tree-based objective, outperforming standard PLMs by up to 13% in evolutionary reasoning.

The review papers ([5]–[10]) provided comprehensive overviews of AI methods across gene prioritization, computational protein science, foundation models in bioinformatics, antimicrobial peptide (AMP) design, biochemical research frontiers, and biofilm-related antimicrobial resistance. GPT-4 excelled in ranking causal genes [5] though biases remained; pLMs were categorized by learned knowledge types [6]; four FM modalities (language, vision, graph, multimodal) were benchmarked [7]; deep learning surged in AMP discovery [8]; AI’s role in enzyme engineering and metabolic pathway studies was mapped out [9]; and hybrid ML approaches were proposed to tackle AMR in cancer biofilms [10].

Foundation model research ([11]–[19]) advanced sequence-to-function design and dataset curation. Multi-likelihood optimization in antimicrobial peptide generation [11] and AMP-SEMiner [12] expanded discovery to over 1.6 million candidates, validating 18 active peptides. Multiomic LLMs [13] like Evo demonstrated cross-layer predictions on genomes. Dataset distillation via MeSH-guided question generation [14] enabled Llama3-70B to surpass GPT-4 on biomedical QA tasks. Large-scale models for transcription [15], long DNA sequences [16], adapter-based conditional protein generation [17], evolutionary simulation over 500 million years [18], and morphological response modeling [19] underscore the depth of architectural innovations and translational potential.

Finally, AI agents ([20]–[23]) explored domain-specific automation. Direct vs. soft prompting for cancer model entity extraction [20] showed that soft prompts can match GPT-4 with smaller LLaMA3 variants. BioMaster [21] and BioAgents [23] demonstrated multi-agent orchestration for multi-omics workflows, while InstructCell [22] provided a multimodal copilot for single-cell RNA analysis.

Across categories, methodological advancements—from hybrid state-space transformers and knowledge distillation to multi-agent orchestration—highlight a convergence toward domain-tailored AI solutions. Key limitations include safety and accuracy gaps in LLM reasoning [1], bias in gene ranking [5], and the need for higher-quality annotated datasets [14]. Future directions emphasize expanding benchmarks to new domains, integrating multimodal data, improving model interpretability, and scaling multi-agent systems to fully automate complex bioinformatics pipelines.

## Table of Contents  
- [Benchmarks](#benchmarks)  
- [Reviews](#reviews)  
- [Foundation Models](#foundation-models)  
- [AI Agents](#ai-agents)  

## Benchmarks  
The benchmarks category ([1]–[4]) underscores the critical need for domain-specific evaluation frameworks to assess LLM performance in specialized scientific contexts. CARDBiomedBench [1] addresses neurodegenerative disease research with a 68,000 Q/A pair benchmark, evaluating seven LLMs across ten biological categories and nine reasoning skills. It introduced novel metrics—Response Quality Rate and Safety Rate—revealing substantial limitations in accuracy (25%–37%) and safety (31%–76%). Bridging early science education [2] evaluated GPT-4, Claude, Gemini, and Llama via 30 nursery teachers, finding Claude superior in biology but noting all models struggled with abstract chemistry. This highlights the challenge of tailoring LLM outputs to cognitive development levels. Wilson et al. [3] studied exploration in open-ended tasks using Little Alchemy 2, demonstrating that traditional LLMs prioritize uncertainty-driven strategies with limited iterative reasoning, while DeepSeek’s SAE-enabled representational analysis found empowerment values processed too late, leading to premature decisions. Zitnik’s Phyla [4] introduced a transformer with a tree-based objective for evolutionary reasoning, training on 3,000 phylogenies. It improved tree reconstruction accuracy by 13% and taxonomic clustering by 10%, revealing that conventional sequence models cannot capture phylogenetic structures without explicit supervision.

| Index | Title                                                                                                               | Domain                                                        | Venue                                                                          | Team            | DOI                          | affiliation                                                                                                                                                                       | paperUrl                                             |
|-------|---------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------|--------------------------------------------------------------------------------|-----------------|------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------|
| 1     | CARDBiomedBench: A Benchmark for Evaluating Large Language Model Performance in Biomedical Research                  | LLM evaluation in neurodegenerative disease research          | bioRxiv                                                                        | F. Faghri       | 10.1101/2025.01.15.633272    | Center for Alzheimer's and Related Dementias, National Institute on Aging, National Institutes of Health, Bethesda, MD, 20892, USA; DataTecnica, Washington                         | [Link](https://www.semanticscholar.org/paper/8b9312c65220e7ef9c5edfc5b3b9f8bb259e1444) |
| 2     | Bridging the Early Science Gap with Artificial Intelligence: Evaluating Large Language Models as Tools for Early Childhood Science Education | LLM-generated science education content for preschoolers      | Proceedings of the Extended Abstracts of the CHI Conference on Human Factors… | A. Alibakhshi   | 10.1145/3706599.3721261      |                                                                                                                                                                                    | [Link](https://www.semanticscholar.org/paper/28d2c5d1bc02efab5a9ddf2508e0a68ec2501bf9) |
| 3     | Large Language Models Think Too Fast To Explore Effectively                                                         | Exploration capabilities of LLMs in open-ended tasks         | ArXiv                                                                          | Robert C. Wilson | 10.48550/arXiv.2501.18009   |                                                                                                                                                                                    | [Link](https://www.semanticscholar.org/paper/7d4202a3e3801d6304e0e2463c49d033b37da77d) |
| 4     | Sequence Modeling Is Not Evolutionary Reasoning                                                                      | Evolutionary reasoning benchmark for protein LLMs             | bioRxiv                                                                        | M. Zitnik       | 10.1101/2025.01.17.633626    | Harvard Medical School                                                                                                                                                             | [Link](https://www.semanticscholar.org/paper/f62b34cfa37bc9066affe185da5349f4c9ef9fb0) |

## Reviews  
The review articles ([5]–[10]) synthesize advances and challenges in applying AI to genomics, proteomics, bioinformatics, and antimicrobial research. Hu et al. [5] benchmarked LLMs for gene prioritization using multi-agent and HPO classification approaches, demonstrating GPT-4’s superior accuracy but exposing biases towards well-studied genes and input-order sensitivity. Li’s overview [6] categorized pLMs by learned knowledge—sequence patterns, structural details, and external scientific language—highlighting their applications in structure prediction, function annotation, antibody and enzyme design, and drug discovery. Wang [7] reviewed foundation models in bioinformatics across genomics, transcriptomics, proteomics, drug discovery, and single-cell analysis, classifying them into language, vision, graph, and multimodal FMs. de la Fuente-Nunez [8] examined AI methods for AMPs, noting a shift from classical ML to DL, and emphasized the untapped potential of LLMs, GNNs, and structure-guided design. Junaid [9] provided a panoramic view of AI in biochemistry, from AlphaFold-enabled structural predictions to AI-driven metabolic pathway studies, while identifying data quality and interpretability as key hurdles. Banerjee [10] focused on AI tools for AMR in cancer biofilms, reviewing 76 articles that employed KNN, HMM, SVM, RF, and DNN models. It underscored the importance of hybrid approaches and knowledge graphs for discovering novel AMPs and highlighted the lack of a universally optimal AI method for AMP prediction.

| Index | Title                                                                                                                | Domain                                                  | Venue                               | Team                           | DOI                        | affiliation | paperUrl                                             |
|-------|----------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------|-------------------------------------|--------------------------------|----------------------------|-------------|------------------------------------------------------|
| 5     | Survey and Improvement Strategies for Gene Prioritization with Large Language Models                                | LLM-based gene prioritization                           | ArXiv                               | Xia Hu                         | 10.48550/arXiv.2501.18794 |             | [Link](https://www.semanticscholar.org/paper/7ca21dec02c418f010d6b95de3bde7bb6ac69c66) |
| 6     | Computational Protein Science in the Era of Large Language Models (LLMs)                                             | Computational protein science with LLMs                 | ArXiv                               | Qing Li                        | 10.48550/arXiv.2501.10282 |             | [Link](https://www.semanticscholar.org/paper/0322d6eef1567b8b6ce3998d03f955082f9a87b6) |
| 7     | Foundation models in bioinformatics                                                                                  | Bioinformatics foundation models overview                | National Science Review             | Jianxin Wang                  | 10.1093/nsr/nwaf028        |             | [Link](https://www.semanticscholar.org/paper/3f21f2fadc60b7f627b5154a9944ec22d28c2678) |
| 8     | AI Methods for Antimicrobial Peptides: Progress and Challenges                                                       | AI methods for antimicrobial peptide design             | Microbial Biotechnology             | C. de la Fuente-Nunez         | 10.1111/1751-7915.70072   |             | [Link](https://www.semanticscholar.org/paper/9c310642e1a07abeab7c6a8216ec60328feb74a2) |
| 9     | Artificial intelligence driven innovations in biochemistry: A review of emerging research frontiers                  | AI innovations in biochemistry                           | Biomolecules and Biomedicine        | M. A. Lateef Junaid           | 10.17305/bb.2024.11537    |             | [Link](https://www.semanticscholar.org/paper/fa06241dfa98d42974532dd314b87d14ad5dc33f) |
| 10    | Artificial Intelligence Tools Addressing Challenges of Cancer Progression Due to Antimicrobial Resistance in Biofilms | AI tools for antimicrobial resistance in cancer biofilms | Artificial Intelligence Evolution   | Abhijit G. Banerjee          | 10.37256/aie.6120255553   |             | [Link](https://www.semanticscholar.org/paper/a86fd4458cbbe7f1ea491d93c1b5ec31da8d272d) |

## Foundation Models  
This section ([11]–[19]) highlights architectural innovations and large-scale pretraining strategies in protein, antimicrobial, transcriptional, and multiomic domains. De la Fuente-Nunez [11] proposed optimizing generative protein models with dual likelihoods in sequence space and PLM latent space, outperforming GANs, VAEs, and baseline GPT on AMP and MDH tasks. Zheng et al. [12] introduced AMP-SEMiner, combining PLMs, structural clustering, and evolutionary analysis to discover 1,670,600 AMP candidates from MAGs, validating 18 highly active peptides. Topol [13] reviewed “language of life” foundation models (LLLMs) like Evo, pretrained on 300B nucleotides across prokaryotic genomes, capable of predicting variant impacts on DNA, RNA, and protein function. Xiao [14] developed a MeSH-guided dataset distillation framework that automatically generates AI-ready biomedical QA data, enabling Llama3-70B to outperform GPT-4 on domain tasks. A transcriptional regulation FM published in Nature [15] models ATAC-seq data across human cell types. AIRI’s GENA-LM [16] supports long DNA sequences in Nucleic Acids Research. Salesforce’s ProCALM [17] uses adapters for conditional protein generation. EvolutionaryScale [18] simulated 500M years of protein evolution with ESM, revealing deep evolutionary constraints. Theis et al. [19] employed generative modeling for predicting cellular morphological responses to perturbations, linking latent space sampling to phenotypic outcomes. Collectively, these works demonstrate the efficacy of multi-likelihood training, MeSH-guided distillation, adapter modules, and multiomic pretraining in pushing the frontier of foundation models for biomolecular discovery.

| Index | Title                                                                                                           | Domain                                                           | Venue                          | Team                                          | DOI                        | affiliation               | paperUrl                                             |
|-------|-----------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------|--------------------------------|-----------------------------------------------|----------------------------|---------------------------|------------------------------------------------------|
| 11    | Improving functional protein generation via foundation model-derived latent space likelihood optimization      | Generative protein design via PLM latent space optimization      | bioRxiv                        | C. de la Fuente-Nunez                         | 10.1101/2025.01.07.631724 | University of Pennsylvania | [Link](https://www.semanticscholar.org/paper/98be9134fce5ba8889ea6de518b814c9482ecd8e) |
| 12    | Unveiling the Evolution of Antimicrobial Peptides in Gut Microbes via Foundation Model-Powered Framework        | Antimicrobial peptide discovery from gut microbes                | bioRxiv                        | Jinfang Zheng                                 | 10.1101/2025.01.13.632881 | Zhejiang Lab              | [Link](https://www.semanticscholar.org/paper/2fc9811c567186a15873eb8600df36b27f40c088) |
| 13    | Learning the language of life with AI.                                                                          | Multiomic foundation models for biomolecule prediction           | Science                        | E. Topol                                      | 10.1126/science.adv4414   |                           | [Link](https://www.semanticscholar.org/paper/864031c9cc2195153f6fc08fed9d67131ddee8c4) |
| 14    | Knowledge Hierarchy Guided Biological-Medical Dataset Distillation for Domain LLM Training                     | Biomedical dataset distillation for LLM training                | ArXiv                          | Meng Xiao                                     | 10.48550/arXiv.2501.15108 |                           | [Link](https://www.semanticscholar.org/paper/8d6ab0e57aa1f7d20e680f3306a9e098a9b3a286) |
| 15    | A foundation model of transcription across human cell types                                                     | transcriptional regulation(ATAC-seq)                             | Nature                         |                                               |                            |                           | [Link](https://www.nature.com/articles/s41586-024-08391-z)        |
| 16    | GENA-LM: a new DNA language model for long sequences                                                             | DNA                                                              | Nucleic Acids Research         |                                               |                            |                           | [Link](https://academic.oup.com/nar/article/53/2/gkae1310/7954523) |
| 17    | Function-Guided Conditional Generation Using Protein Language Models with Adapters                              | Protein                                                           | arXiv                          | Salesforce Research (Ali Madani)               |                            |                           | [Link](https://arxiv.org/abs/2410.03634)                         |
| 18    | Simulating 500 million years of evolution with a language model                                                 | Protein                                                           | Science                        | EvolutionaryScale (Alexander Rives)           |                            |                           | [Link](https://www.science.org/doi/10.1126/science.ads0018)         |
| 19    | Predicting cell morphological responses to perturbations using generative modeling                              | phenotype morphological responses to perturbation               | Nature Communications          | Fabian J. Theis, Mohammad Lotfollahi          |                            |                           | [Link](https://www.nature.com/articles/s41467-024-55707-8)         |

## AI Agents  
The AI agent papers ([20]–[23]) explore automated workflows and domain-specific copilot systems. Savova et al. [20] compared direct prompting vs. soft prompting for extracting patient-derived cancer model entities, showing that soft prompts trained on LLaMA3 models can rival GPT-4’s performance. BioMaster [21] implements a multi-agent system for automated bioinformatics workflows, orchestrating tasks across genomics and proteomics pipelines with agent-level specialization. InstructCell [22] introduces a multimodal copilot for single-cell RNA analysis, combining instruction following with image and sequence data to streamline exploratory analysis. BioAgents [23] from Microsoft Research presents a democratized platform for multi-omics analysis with agent-driven task allocation, enabling scalable end-to-end bioinformatics pipelines. Together, these studies reveal the potential of prompt engineering, agent orchestration, and multimodal instruction following to accelerate domain-specific scientific workflows.

| Index | Title                                                                                                                | Domain                                                           | Venue    | Team                                   | DOI                        | affiliation                                               | paperUrl                                             |
|-------|----------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------|----------|----------------------------------------|----------------------------|-----------------------------------------------------------|------------------------------------------------------|
| 20    | Extracting Knowledge from Scientific Texts on Patient-Derived Cancer Models Using Large Language Models: Algorithm Development and Validation | LLM-based entity extraction for patient-derived cancer models    | bioRxiv  | G. Savova                              | 10.1101/2025.01.28.634527 | Boston Children's Hospital, Harvard Medical School       | [Link](https://www.semanticscholar.org/paper/4ea31174978b289183898b4cafd1432d9ddd0349) |
| 21    | BioMaster: Multi-agent System for Automated Bioinformatics Analysis Workflow                                         | Multi-omics Pipelines                                            | bioRxiv  |                                        |                            |                                                           | [Link](https://www.biorxiv.org/content/10.1101/2025.01.23.634608v1) |
| 22    | InstructCell: A Multi-Modal AI Copilot for Single-Cell Analysis with Instruction Following                          | single cell RNA                                                  | arXiv    |                                        |                            |                                                           | [Link](https://arxiv.org/abs/2501.08187)               |
| 23    | BioAgents: Democratizing Bioinformatics Analysis with Multi-Agent Systems                                           | Multi-omics Pipelines                                            | arXiv    | Microsoft Research (Venkat S. Malladi) |                            |                                                           | [Link](https://arxiv.org/abs/2501.06314)               |