# Research Paper Report for 2024-12-01 to 2024-12-31

## Overall Summary

Between December 1 and December 31, 2024, the collected works demonstrate a robust convergence of artificial intelligence methodologies, domain-specific adaptations, and data-intensive frameworks across computational biology, drug discovery, materials science, and single-cell analysis. A central theme is the integration of large language models (LLMs) and specialized pre-trained models, exemplified by ProtChat [1], which leverages GPT-4 alongside protein language models (ESM, MASSA) to fully automate protein property prediction and protein–drug interaction analyses without human scripting. Similarly, SCREADER [2] combines LLaMA-2 with gene expression prompts to improve cross-species scRNA-seq annotation, advancing interoperability and annotation accuracy. These AI-agent frameworks often deploy multi-agent orchestration (e.g., SciAgents [3] uses ontological knowledge graphs plus in-situ learning subagents) or computer vision backbones (e.g., YOLOv5 in the schistosomula screening assay [4]) to tackle traditional bottlenecks in experimental throughput.

Foundation models have been repurposed for non-linguistic tasks, as with L2G [6], where a neural architecture search and three-stage training pipeline adapts pre-trained LLMs for genomics, outperforming genomic-specific FMs on enhancer activity prediction. Porter 6 [8] ensembles CBRNN predictors with ESM-2 embeddings to reach 86.60% three-state and 76.43% eight-state secondary structure accuracy, marking a 3% improvement over predecessors. ProtDAT [9] introduces a novel multi-modal cross-attention framework uniting textual descriptions and sequence data to enhance de novo protein sequence generation, achieving improvements of 6% in pLDDT and 0.26 in TM-score on Swiss-Prot benchmarks.

Review articles [10–12] collectively chart the transformative role of LLMs in molecular biology, tumor microenvironment multi-omics, and the evolution of protein structure prediction—from homology and threading to AI-driven approaches like AlphaFold and emerging PLMs. These contributions highlight both the promise and ethical challenges of LLM deployment in drug design pipelines.

Large-scale resources and databases further underscore the push toward comprehensive AI-driven discovery. M3-20M [13] offers 20 million multi-modal molecules (SMILES, graphs, 3D structures, properties, GPT-3.5–generated text) to boost generative and predictive tasks, while BioMedGraphica [14] unifies over 3.1 million biomedical entities in a text-attributed knowledge graph supporting zero-shot discoveries. AI-enhanced search platforms [15] augment NIAGADS with specialized LLMs trained on API specifications to streamline Alzheimer’s genomic data access.

Finally, benchmarking efforts [16,17] provide critical evaluation frameworks: a gene property benchmark assessing text-based, protein-, DNA-, and expression-based models across hundreds of tasks, and PerturBench offering a scRNA perturbation analysis standard. Together, these works illustrate a field rapidly moving toward autonomous, scalable, and interpretable AI tools, while revealing ongoing challenges in domain adaptation, data labeling, model explainability, and integration of multi-modal data streams.

## Table of Contents

- [AI Agents](#ai-agents)  
- [Foundation Models](#foundation-models)  
- [Reviews](#reviews)  
- [Databases](#databases)  
- [Benchmarks](#benchmarks)  

## AI Agents

The AI Agents category showcases multi-agent systems and specialized models that automate core biological tasks without manual intervention. ProtChat [1] integrates GPT-4 with PLLMs (ESM, MASSA) to create an end-to-end protein analysis pipeline, automating property prediction and protein–drug interaction workflows directly from user instructions. SCREADER [2] introduces a hybrid LLM-representation model approach, initializing gene embeddings with LLaMA-2 functional descriptions and using single-cell expression prompts for cross-species annotation and visualization, demonstrating improved annotation accuracy on challenging human and mouse developmental cells. SciAgents [3] advances materials discovery by coupling large-scale ontological knowledge graphs, multiple LLMs, and in-situ learning subagents; this “swarm intelligence” autonomously generates hypotheses and reveals novel biocomposite designs. In a different modality, the schistosomula screening assay [4] applies YOLOv5 object detection on 4,390 annotated images to achieve a mAP of 0.966 for healthy vs damaged larvae, outperforming human counters in speed and accuracy. Finally, CASSIA [5] presents a reference-free, interpretable multi-agent LLM framework for single-cell RNA-seq cell annotation, highlighting the growing trend of embedding reasoning capabilities directly within LLMs for biological data interpretation. Collectively, these works leverage GPT-4, LLaMA-2, and vision backbones within modular agent architectures, striking a balance between automation, interpretability, and domain-specific performance. However, limitations include scalability to larger datasets, dependence on pre-trained model availability, and incomplete evaluation of edge cases, particularly in CASSIA’s unreported metrics.

| Index | Title                                                                                                            | Domain                                        | Venue                                                       | Team             | DOI                         | affiliation | paperUrl                                                                                 |
|-------|------------------------------------------------------------------------------------------------------------------|-----------------------------------------------|-------------------------------------------------------------|------------------|-----------------------------|-------------|------------------------------------------------------------------------------------------|
| 1     | ProtChat: An AI Multi-Agent for Automated Protein Analysis Leveraging GPT-4 and Protein Language Model          | Automated protein analysis AI agent           | Journal of chemical information and modeling                | Yunpeng Cai      | 10.1021/acs.jcim.4c01345    |             | [Link](https://www.semanticscholar.org/paper/c89cfdd40cf24c40b09304cbbf85024933def10c)    |
| 2     | SCREADER: Prompting Large Language Models to Interpret scRNA-seq Data                                           | scRNA-seq interpretation via LLM prompting    | 2024 IEEE International Conference on Data Mining Workshops | Meng Xiao        | 10.1109/ICDMW65004.2024.00092 |             | [Link](https://www.semanticscholar.org/paper/aba63c2f387349a58bbc9a89d83e2018bc436819)    |
| 3     | SciAgents: Automating Scientific Discovery Through Bioinspired Multi-Agent Intelligent Graph Reasoning         | Multi-agent AI for automated materials discovery | Advanced Materials (Deerfield Beach, Fla.)                | Markus J. Buehler | 10.1002/adma.202413523      |             | [Link](https://www.semanticscholar.org/paper/dbbcdb281ed6aa646af0402172cf8a7cfda85d5c)    |
| 4     | Development and Application of an In Vitro Drug Screening Assay for Schistosoma mansoni Schistosomula Using YOLOv5 | AI-powered drug screening assay for schistosomula | Biomedicines                                              | Antonio Muro     | 10.3390/biomedicines12122894 |             | [Link](https://www.semanticscholar.org/paper/0cac159098bb2365ca8547979f6f81018165a0ca)    |
| 5     | CASSIA: a multi-agent large language model for reference free, interpretable, and automated cell annotation of single-cell RNA-sequencing data | Single Cell annotation                        | bioRxiv                                                     |                  |                             |             | [Link](https://www.biorxiv.org/content/10.1101/2024.12.04.626476v2)                       |

## Foundation Models

The Foundation Models section focuses on adapting and extending large pre-trained architectures to specialized biological tasks. L2G [6] exploits the “cross-modal transfer” capacity of LLMs, using neural architecture search and a three-stage training pipeline to tailor a language model for genomics tasks without extensive DNA pre-training; it surpasses several dedicated genomic FMs on benchmark tasks, including enhancer activity prediction by identifying key transcription factor motifs. Filizola et al. [7] develop deep transfer learning models targeting Class A GPCRs for large-scale drug screening, although details on architectures, datasets, and performance metrics are not reported. Porter 6 [8] ensembles CBRNNs with ESM-2 embeddings, achieving 86.60% Q3 and 76.43% Q8 accuracies on independent test sets, marking a 3% Q3 improvement over its predecessor and matching state-of-the-art methods. ProtDAT [9] presents a unified multi-modal framework that aligns protein sequences and free-form textual descriptions via cross-attention layers; on 20,000 Swiss-Prot text–sequence pairs, it yields 6% higher pLDDT, a 0.26 gain in TM-score, and a 1.2 Å reduction in RMSD. Across these works, methodological advances include neural architecture search, ensemble architectures, and multi-modal attention, reflecting a shift toward versatile, resource-efficient pipelines. Nonetheless, concerns remain regarding computational demands, reliance on high-quality training corpora, and generalizability across diverse biological domains.

| Index | Title                                                                                                        | Domain                                           | Venue                                  | Team             | DOI                            | affiliation                       | paperUrl                                                                                   |
|-------|--------------------------------------------------------------------------------------------------------------|--------------------------------------------------|----------------------------------------|------------------|--------------------------------|-----------------------------------|--------------------------------------------------------------------------------------------|
| 6     | L2G: Repurposing Language Models for Genomics Tasks                                                          | Repurposing LLMs for genomics tasks              | bioRxiv                                | Ameet Talwalkar  | 10.1101/2024.12.09.627422       | Carnegie Mellon University        | [Link](https://www.semanticscholar.org/paper/238d842ee1679545e00d212a342f271a05473eaf)     |
| 7     | Fine-Tuned Deep Transfer Learning Models for Large Screenings of Safer Drugs Targeting Class A GPCRs          | Deep transfer learning for GPCR drug screening   | bioRxiv                                | M. Filizola      | 10.1101/2024.12.07.627102       | Icahn School of Medicine at Mount Sinai | [Link](https://www.semanticscholar.org/paper/adfd39e5060b00b74a38d7b7243f60af2934e966)     |
| 8     | Porter 6: Protein Secondary Structure Prediction by Leveraging Pre-Trained Language Models (PLMs)           | Protein secondary structure prediction with PLMs | International Journal of Molecular Sciences | G. Pollastri     | 10.3390/ijms26010130            |                                   | [Link](https://www.semanticscholar.org/paper/4eb9efc6ea3f7bf7935c374940af4c3866a5a368)     |
| 9     | ProtDAT: A Unified Framework for Protein Sequence Design from Any Protein Text Description                  | Multi-modal protein sequence design from text    | ArXiv                                  | Hong-Bin Shen    | 10.48550/arXiv.2412.04069       |                                   | [Link](https://www.semanticscholar.org/paper/1fc064d589849dfefce0d28fe64b9b83600a0cb6)     |

## Reviews

This collection of reviews synthesizes the impact of AI and LLMs on molecular biology, multi-omics in tumor microenvironments, and protein structure prediction. Liu et al. [10] survey the deployment of LLMs within pharmaceutical research, detailing neural network architectures, natural language interfaces, and applications in novel drug discovery while critically evaluating ethical considerations such as data privacy and model bias. Sun [11] delves into AI strategies for tumor metabolism and tumor microenvironment (TME) analyses, exploring how multi-omics—especially metabolomics—alongside cytokine interaction models can inform predictive biomarker development. Chen [12] traces the evolution of protein structure prediction from homology modeling and threading to AI-based frameworks like AlphaFold, spotlighting challenges in predicting intrinsically disordered protein regions and mutation-induced conformational changes; the review also highlights emerging PLM approaches that address these limitations. Collectively, these reviews emphasize interdisciplinary connections—LLMs in drug pipelines, graph and generative models in TME studies, and deep learning in structural biology—and identify methodological gaps in interpretability, dynamic region modeling, and robust cross-modal integration. They underscore the need for standardized benchmarks, transparent reporting of evaluation metrics, and frameworks that reconcile model performance with ethical and practical constraints.

| Index | Title                                                                                                        | Domain                                                        | Venue                             | Team        | DOI                           | affiliation | paperUrl                                                                                  |
|-------|--------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------|-----------------------------------|-------------|-------------------------------|-------------|-------------------------------------------------------------------------------------------|
| 10    | Large language models facilitating modern molecular biology and novel drug development                      | Review of LLMs in molecular biology and drug development      | Frontiers in Pharmacology         | Fei Liu     | 10.3389/fphar.2024.1458739     |             | [Link](https://www.semanticscholar.org/paper/d7157f73a59c3a619549217e6d27884bebedf419)     |
| 11    | From multi-omics to predictive biomarker: AI in tumor microenvironment                                      | Review of AI in tumor microenvironment multi-omics            | Frontiers in Immunology           | Yingli Sun  | 10.3389/fimmu.2024.1514977     |             | [Link](https://www.semanticscholar.org/paper/6ae86bc78b187c0ca15b977156b8a2bd41d91be9)     |
| 12    | Advancements and Applications of Protein Structure Prediction Algorithms                                     | Review of protein structure prediction methods                | Theoretical and Natural Science   | Ye Chen     | 10.54254/2753-8818/2024.la18791 |             | [Link](https://www.semanticscholar.org/paper/6567678e2b6869ead0953704efdb926294daeebb)     |

## Databases

Database resources published in December 2024 emphasize multi-modal datasets, unified knowledge graphs, and AI-enhanced access to genomic repositories. M3-20M [13] compiles over 20 million molecules with SMILES strings, 2D/3D structures, physicochemical properties, and GPT-3.5–generated textual annotations; extensive experiments using GLM4, GPT-3.5, GPT-4, and Llama3-8b show substantial improvements in molecular diversity, validity, and property prediction accuracy. BioMedGraphica [14] constructs a text-attributed knowledge graph (TAKG) with 3.13 million entities and 56.8 million relations sourced from 43 databases, harmonizing 29 edge types; its GUI supports zero- and few-shot relation prediction and multi-omic signaling graph generation, paving the way for AI4PHM applications. The AI-enhanced NIAGADS platform [15] integrates generative LLMs trained on domain documentation and OpenAPI specs to power conversational agents and planner-driven search tools, facilitating programmatic and natural-language queries across Alzheimer’s genomics knowledgebases. These platforms demonstrate methodological innovation in data harmonization, multi-modal integration, and LLM-based user interfaces. Challenges include ensuring data quality across heterogeneous sources, scaling graph inference for very large knowledgebases, and measuring the reliability of AI-driven search recommendations.

| Index | Title                                                                                                                                        | Domain                                               | Venue                                                           | Team          | DOI                         | affiliation                                            | paperUrl                                                                                      |
|-------|----------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------|-----------------------------------------------------------------|---------------|-----------------------------|--------------------------------------------------------|-----------------------------------------------------------------------------------------------|
| 13    | M3-20M: A Large-Scale Multi-Modal Molecule Dataset for AI-driven Drug Design and Discovery                                                   | Multi-modal molecule dataset for drug design         | Journal of bioinformatics and computational biology             | Shuigeng Zhou | 10.48550/arXiv.2412.06847   |                                                        | [Link](https://www.semanticscholar.org/paper/aa3a1bbd4f6ec6934ce6a3850991747f563ab3b0)       |
| 14    | BioMedGraphica: An All-in-One Platform for Biomedical Prior Knowledge and Omic Signaling Graph Generation                                      | Biomedical knowledge graph platform                  | bioRxiv                                                         | Fuhai Li      | 10.1101/2024.12.05.627020   | Washington University in St. Louis                    | [Link](https://www.semanticscholar.org/paper/212898d49afdf60a12eeda9935bc54d748da3089)        |
| 15    | Basic Science and Pathogenesis.                                                                                                              | AI-enhanced search for Alzheimer's genomic database   | Alzheimer's & Dementia: the journal of the Alzheimer's Association | Li-San Wang   | 10.1002/alz.092685          |                                                        | [Link](https://www.semanticscholar.org/paper/28d30ecee02ba1039bd83c77d3131c7c744dad26)        |

## Benchmarks

Benchmark studies provide standardized evaluation protocols for biological and text-based models. Shimoni et al. [16] introduce an architecture-agnostic gene property benchmark, converting model embeddings into task-specific predictors across genomic properties, regulatory functions, localization, biological processes, and protein properties. Their findings reveal that text-based and protein language models excel in genomic and regulatory function tasks, while expression-based models lead in localization, offering guidance for model selection in therapeutic discovery. PerturBench [17], presented at NeurIPS 2024, benchmarks machine learning approaches in cellular perturbation analysis on scRNA-seq data; although details on tasks and metrics are pending, its open-source repository aims to foster reproducibility and comparison across methods. Together, these benchmarks underscore the importance of consistent task definitions, transparent code availability, and cross-model comparability to drive methodological progress in AI-driven biological research.

| Index | Title                                                                                                        | Domain                                        | Venue           | Team  | DOI                           | affiliation | paperUrl                                                                                     |
|-------|--------------------------------------------------------------------------------------------------------------|-----------------------------------------------|-----------------|-------|-------------------------------|-------------|------------------------------------------------------------------------------------------------|
| 16    | Does your model understand genes? A benchmark of gene properties for biological and text models              | Benchmark of gene property prediction models | ArXiv           | Y. Shimoni | 10.48550/arXiv.2412.04075     |             | [Link](https://www.semanticscholar.org/paper/7eb6e6bce59f3e724ae3365b1348627dfd73c409)       |
| 17    | PerturBench: Benchmarking Machine Learning Models for Cellular Perturbation Analysis                        | Perturbation scRNA                            | NeurIPS 2024    |       |                               |             | [Link](https://neurips.cc/virtual/2024/102911)                                                |

